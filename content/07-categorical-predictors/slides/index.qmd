---
title: "The linear model and comparing multiple groups"
subtitle: "Power analysis through simulation in R"
author: "Niklas Johannes"
format: 
  revealjs:
    theme: ../../slidetheme.scss
    slide-number: true
    chalkboard: 
      buttons: false
    footer: "[Home](https://niklasjohannes.github.io/power-workshop/)"
execute:
  echo: true
---

# Takeaways

- Understand the logic behind the data generating process
- See how the linear model is our data generating process
- Apply this to a setting with multiple categories in a predictor

# Lots of foreshadowing

- Data generating process
- Simulations are that process

![](teaser.jpg)

## What's that data generating process?

- The process, in the real world, that you believe created the data
- We don't know the process, but we can observe the outcomes
- When we try to explain the outcome, we make assumptions about the data generating process
- We collect data from samples and want to know whether our assumptions fit (and generalize)

## What's the process?

You measure height in a sample (the observable consequences). What's the data generating process?

```{r}
#| echo: false

library(tidyverse)
m <- 174
sd <- 7

ggplot(
  data.frame(x = c(m-3*sd, m+3*sd)),
  aes(x)) +
    stat_function(
      fun = dnorm,
      n = 10001,
      args = list(mean = m, sd = sd)
    ) + 
  scale_y_continuous(breaks = NULL) + 
  ylab("") + xlab("Height") +
  cowplot::theme_cowplot()
```

## We have a (simple) process

$Normal(\mu, \sigma)$

```{r}
#| echo: false

ggplot(
  data.frame(x = c(m-3*sd, m+3*sd)),
  aes(x)) +
    stat_function(
      fun = dnorm,
      n = 10001,
      args = list(mean = m, sd = sd)
    ) + 
  scale_y_continuous(breaks = NULL) + 
  ylab("") + xlab("Height") +
  scale_x_continuous(
    breaks = c(m-sd, m, m+sd),
    labels = c(expression(-sigma), expression(mu), expression(+sigma))
  ) +
  geom_vline(xintercept = m-sd) +
  geom_vline(xintercept = m) +
  geom_vline(xintercept = m+sd) +
  cowplot::theme_cowplot()
```

## Look familiar?

$Normal(174, 7)$ aka `rnorm(174, 7)`

:::: {.columns}

:::: {.column}

```{r}
#| echo: false

ggplot(
  data.frame(x = c(m-3*sd, m+3*sd)),
  aes(x)) +
    stat_function(
      fun = dnorm,
      n = 10001,
      args = list(mean = m, sd = sd)
    ) + 
  scale_y_continuous(breaks = NULL) + 
  ylab("") + xlab("Height") +
  scale_x_continuous(
    breaks = c(m-sd, m, m+sd),
    labels = c(sd, m, sd)
  ) +
  geom_vline(xintercept = m-sd) +
  geom_vline(xintercept = m) +
  geom_vline(xintercept = m+sd) +
  cowplot::theme_cowplot()
```

::::

:::: {.column}

```{r}
plot(density(rnorm(1e4, 174, 7)))
```


::::

::::

## All are processes

- `rnorm`: You say that the underlying data generating process is a normal distribution
- `rbinom`: You say that the underlying data generating process is a binomial distribution
- You get the idea

**Important: Those are assumptions you make explicit and they can be wrong.**

## Remember our t-tests?

You are explicitly claiming that the observable outcome (the data) have been generated by this true underlying process. The process is a normal distribution with a true effect size ($\mu$) and a true standard deviation ($\sigma$) for two data generating processes: $Normal(100, 15)$ and $Normal(105, 15)$ 

```{r}
control <- rnorm(100, 100, 15)
treatment <- rnorm(100, 105, 15)

t.test(control, treatment)
```

## That's why simulation is cool

- You're being explicit about your model of the world
- You translate that model (data generating process) into a concrete formula (code) to create data (the consequences of the data generation process)
- You check how changes to your model influence the data, which inform the conclusions you draw from the data about your model (aka statistical inference)
- No more hiding your model assumptions behind standard software

## This all sounds complicated

Here's the good news: In (much of) the (social) sciences we rely on a common data generating process: the linear model.

t-test, regression, logistic regression, machine learning are all variations of the linear model.

$y = \beta_0 + \beta_1x$

## All stolen from here

![](linear.jpg)

[Source](https://lindeloev.github.io/tests-as-linear/#1_the_simplicity_underlying_common_tests)

## Back to our control and treatment

$y = \beta_0 + \beta_1x$: What's $x$ here?

```{r}
control <- rnorm(15, 0, 1)
treatment <- rnorm(15, 1, 1)
```

## Group membership

Our group is $x$.

```{r}
d <- 
  data.frame(
    group = rep(c("control", "treatment"), each = 15),
    y = c(control, treatment)
  )

d$x <- ifelse(d$group=="control", 0, 1)
```

## Finding our inputs again {.scrollable}

::: {.panel-tabset}

### Data

```{r}
#| echo: false

knitr::kable(d)
```


### Boxplot

```{r}
#| echo: false

boxplot(d$y ~ d$x)
```

:::

## What happens when $x$ is zero?

```{=tex}
\begin{align}
& y = \beta_0 + \beta_1x\\
& y = \beta_0 + \beta_1 \times 0\\
& y = \beta_0
\end{align}
```

In other words, when we predict values for the control (aka $x=0$) group, our best predictor for outcome scores becomes $\beta_0$ (also known as the intercept): the mean for the control group.

## What happens when $x$ is one?

```{=tex}
\begin{align}
& y = \beta_0 + \beta_1x\\
& y = \beta_0 + \beta_1 \times 1\\
& y = \beta_0 + \beta_1\\
& y = mean(control) + ?
\end{align}
```

We already know that the intercept ($\beta_0$) is the mean of the control group. How would we now go from that mean to predicting scores of the treatment group? What do we need to add to the mean of the control group to get the mean of the treatment group?

## Pictures, please

```{r}
#| echo: false

library(ggbeeswarm)

d$x <- as.factor(d$x)
d_sum <- d %>% group_by(x) %>% summarise(m = mean(y))

ggplot(d, aes(x = x, y = y)) +
  geom_beeswarm() + 
  geom_segment(x = 0.8, xend = 1.2, y = d_sum$m[d_sum$x==0], yend = d_sum$m[d_sum$x==0], size = 1, color = "black") +
  geom_segment(x = 1.8, xend = 2.2, y = d_sum$m[d_sum$x==1], yend = d_sum$m[d_sum$x==1], size = 1, color = "steelblue") +
  geom_segment(x = 1, xend = 2, y = d_sum$m[d_sum$x==0], yend = d_sum$m[d_sum$x==1], size = 1, color = "firebrick4") +
  annotate("text", x = 0.8, y = d_sum$m[d_sum$x==0] + 0.2, color = "black", label = paste("beta[0] ==", "Intercept"), parse = TRUE) +
  annotate("text", x = 1.5, y = d_sum$m[d_sum$x==0] + 1, color = "firebrick4", label = paste("beta[1] ==", "Slope"), parse = TRUE) +
  annotate("text", x = 2.3, y = d_sum$m[d_sum$x==1] + 0.2, color = "steelblue", label = paste("beta[0] + beta[1] ==", "Group~2~mean"), parse = TRUE) +
  ylab("") + scale_y_continuous(breaks = NULL) + cowplot::theme_cowplot()

```

## Let's check that

:::: {.columns}

:::: {.column}
Mean of the control group (our intercept or $\beta_0$):

```{r}
mean(d$y[d$x==0])
```

Difference (our slope or $\beta_1$):

```{r}
mean(d$y[d$x==1]) - mean(d$y[d$x==0])
```

Mean of the treatment group ($\beta_0 + \beta_1$):

```{r}
mean(d$y[d$x==1])
```
::::

:::: {.column}

Linear model:

```{r}
model <- lm(y ~ x, data = d)

summary(model)
```

::::

::::

## So they're really the same?

:::: {.columns}

:::: {.column}

```{r}
t.test(control, treatment, var.equal = TRUE)
```
::::

:::: {.column}

```{r}
summary(lm(y ~ x, d))
```

::::

::::

## Same logic for a one-sample t-test

There's no $x$ here, so all we're left with is the intercept, which we test against our H~0~. A single number (aka the mean) predicts $y$.

```{=tex}
\begin{align}
& y = \beta_0
\end{align}
```

## Pictures, please

```{r}
#| echo: false

ggplot(d %>% filter(x == 1), aes(x = x, y = y)) +
  geom_point() + 
  geom_segment(x = 0.8, xend = 1.2, y = d_sum$m[d_sum$x==1], yend = d_sum$m[d_sum$x==1], size = 1, color = "black") +
  annotate("text", x = 0.8, y = d_sum$m[d_sum$x==1] + 0.1, color = "black", label = paste("beta[0] ==", "Intercept"), parse = TRUE) +
  ylab("") + scale_y_continuous(breaks = NULL) + cowplot::theme_cowplot()
```

## Let's check that once more

:::: {.columns}

:::: {.column}

Mean of the treatment group (our intercept or $\beta_0$):

```{r}
mean(d$y[d$x==1])
```

::::

:::: {.column}

Linear model:

```{r}
model <- lm(y ~ 1, data = d[d$x==1,])

summary(model)
```

::::

::::

## So they're really the same?

:::: {.columns}

:::: {.column}

```{r}
t.test(treatment)
```
::::

:::: {.column}

```{r}
summary(lm(y ~ 1, d[d$x==1,]))
```

::::

::::

## Same logic for a paired-samples t-test

Remember: Paired samples t-test is just testing the difference in scores against H~0~. This way, it turns into a one-sample t-test, so all we're left with is, once again, the intercept, which we test against our H~0~. A single number (aka the mean) predicts $y$ (technically $y_{difference}$).

```{=tex}
\begin{align}
& y_{treatment} - y_{control} = \beta_0
\end{align}
```

## Pictures, please

```{r}
#| echo: false

ggplot(d %>% group_by(x) %>% mutate(id = row_number()), aes(x=x, y=y)) +
  geom_point(aes(group = id)) +
  geom_line(aes(group = id)) + 
  ylab("") + scale_y_continuous(breaks = NULL) + cowplot::theme_cowplot()
```

## Pictures, please

```{r}
#| echo: false

ggplot(data.frame(x = "Difference", y = treatment-control), aes(x = x, y = y)) +
  geom_point() + 
  geom_segment(x = 0.8, xend = 1.2, y = d_sum$m[d_sum$x==1], yend = d_sum$m[d_sum$x==1], size = 1, color = "black") +
  annotate("text", x = 0.8, y = d_sum$m[d_sum$x==1] + 0.2, color = "black", label = paste("beta[0] ==", "Intercept"), parse = TRUE) +
  ylab("") + xlab("") + scale_y_continuous(breaks = NULL) + cowplot::theme_cowplot()
```


## Let's check that once more

:::: {.columns}

:::: {.column}

Mean of the difference (our intercept or $\beta_0$):

```{r}
mean(treatment-control)
```

::::

:::: {.column}

Linear model:

```{r}
model <- lm(treatment-control ~ 1)

summary(model)
```

::::

::::

## So they're really the same?

:::: {.columns}

:::: {.column}

```{r}
t.test(treatment-control)
```
::::

:::: {.column}

```{r}
summary(lm(treatment-control ~ 1))
```

::::

::::

## What about ANOVA then?

Same as before: We predict scores with the group membership (aka the mean in each group). Doesn't matter whether we predict it from two (t-test) or more groups (ANOVA). Now we just have an indicator for membership for each group: dummy coding.

| groups  | $x_1$ | $x_2$ |
|---------|-------|-------|
| control | 0     | 0     |
| low     | 1     | 0     |
| high    | 0     | 1     |

```{=tex}
\begin{align}
& y = \beta_0 + \beta_1x_1 + \beta_2x_2
\end{align}
```

## Getting the score for control

| groups  | $x_1$ | $x_2$ |
|---------|-------|-------|
| control | 0     | 0     |
| low     | 1     | 0     |
| high    | 0     | 1     |

```{=tex}
\begin{align}
& y = \beta_0 + \beta_1x_1 + \beta_2x_2\\
& y = \beta_0 + \beta_1 \times 0 + \beta_2 \times 0\\
& y = \beta_0
\end{align}
```

## Getting the score for low

| groups  | $x_1$ | $x_2$ |
|---------|-------|-------|
| control | 0     | 0     |
| low     | 1     | 0     |
| high    | 0     | 1     |

```{=tex}
\begin{align}
& y = \beta_0 + \beta_1x_1 + \beta_2x_2\\
& y = \beta_0 + \beta_1 \times 1 + \beta_2 \times 0\\
& y = \beta_0 + \beta_1
\end{align}
```

## Getting the score for high

| groups  | $x_1$ | $x_2$ |
|---------|-------|-------|
| control | 0     | 0     |
| low     | 1     | 0     |
| high    | 0     | 1     |

```{=tex}
\begin{align}
& y = \beta_0 + \beta_1x_1 + \beta_2x_2\\
& y = \beta_0 + \beta_1 \times 0 + \beta_2 \times 1\\
& y = \beta_0 + \beta_2
\end{align}
```

## What are $\beta_1$ and $\beta_2$ then?

What do we need to go from the control mean to the low mean? What do we need to go from control mean to high mean? Same as with the t-test: the difference between those means.

## Pictures, please

```{r}
#| echo: false
set.seed(42)

n <- 15
d <- data.frame(
  x = as.factor(rep(0:2, n)),
  y = rnorm(3*n, c(100, 110, 120), 15)
)

d_sum <- d %>% group_by(x) %>% summarise(m = mean(y))
# ugly code, I know, but no time to write it do it dynamically. manual will have to do.
ggplot(d, aes(x = x, y = y)) +
  geom_beeswarm() + 
  geom_segment(x = 0.8, xend = 3.2, y = d_sum$m[d_sum$x==0], yend = d_sum$m[d_sum$x==0], size = 1, color = "black") +
  geom_segment(x = 1.8, xend = 2.2, y = d_sum$m[d_sum$x==1], yend = d_sum$m[d_sum$x==1], size = 1, color = "steelblue") +
  geom_segment(x = 2.8, xend = 3.2, y = d_sum$m[d_sum$x==2], yend = d_sum$m[d_sum$x==2], size = 1, color = "steelblue") +
  geom_segment(x = 1, xend = 2, y = d_sum$m[d_sum$x==0], yend = d_sum$m[d_sum$x==1], size = 1, color = "firebrick4") +
  geom_segment(x = 2, xend = 3, y = d_sum$m[d_sum$x==0], yend = d_sum$m[d_sum$x==2], size = 1, color = "firebrick4") +
  annotate("text", x = 0.8, y = d_sum$m[d_sum$x==0] + 3, color = "black", label = paste("beta[0] ==", "Intercept"), parse = TRUE) +
  annotate("text", x = 1.6, y = d_sum$m[d_sum$x==0] + 4.5, color = "firebrick4", label = paste("beta[1] ==", "Slope"), parse = TRUE) +
  annotate("text", x = 2.6, y = d_sum$m[d_sum$x==1] - 6, color = "firebrick4", label = paste("beta[2] ==", "Slope"), parse = TRUE) +
  annotate("text", x = 1.75, y = d_sum$m[d_sum$x==1] + 3, color = "steelblue", label = paste("beta[0] + beta[1] ==", "low~mean"), parse = TRUE) +
  annotate("text", x = 2.73, y = d_sum$m[d_sum$x==2] + 3, color = "steelblue", label = paste("beta[0] + beta[2] ==", "high~mean"), parse = TRUE) +
  ylab("") + scale_y_continuous(breaks = NULL) + cowplot::theme_cowplot()
```

## Just an extension

If we know how to simulate a t-test, we know how to simulate an ANOVA (because both are just linear models): Imitate the data generating process.

```{=tex}
\begin{align}
& y = \beta_0 + \beta_1x_1 + \beta_2x_2
\end{align}
```

```{r}
n <- 5

control <- rnorm(n, 100, 15)
low <- rnorm(n, 120, 15)
high <- rnorm(n, 140, 15)

d <- data.frame(
  y = c(control, low, high),
  condition = rep(c("control", "low", "high"), each = n)
)

d$condition <- as.factor(d$condition)
```

## Add our dummy codes

```{r}
d$group_low <- rep(c(0, 1, 0), each = n)
d$group_high <- rep(c(0, 0, 1), each = n)
```

```{r}
#| echo: false

d
```

## Compare to model

:::: {.columns}

:::: {.column}

```{r}
#| echo: false

d %>%
  mutate(
    condition = fct_relevel(condition, "control", "low")
  ) %>% 
  group_by(condition) %>% 
  summarize(mean = mean(y))
```

::::

:::: {.column}
```{r}
model <- lm(y ~ group_low + group_high, data = d)

summary(model)
```

::::

::::

## Compare to ANOVA

:::: {.columns}

:::: {.column}

```{r}
our_anova <- aov(y ~ condition, data = d)

summary(our_anova)
```

::::

:::: {.column}
```{r}
our_regression <- lm(y ~ group_low + group_high, data = d)

summary(our_regression)
```

::::

::::

## No need to use dummies

The `lm` call will automatically dummy code factors.

```{r}
summary(lm(y ~ condition, data = d))
```

## In a power simulation

```{r}
n <- 40
m1 <- 100
m2 <- 103
m3 <- 105
sd <- 8
draws <- 1e4

pvalues <- NULL

for (i in 1:n) {

  group1 <- rnorm(n, m1, sd)
  group2 <- rnorm(n, m2, sd)
  group3 <- rnorm(n, m3, sd)

  dat <- data.frame(
    scores = c(group1, group2, group3),
    condition = as.factor(rep(c("group1", "group2", "group3"), each = n))
  )

  m <- summary(lm(scores ~ condition, data = dat))
  
  pvalues[i] <- broom::glance(m)$p.value
}

sum(pvalues < 0.05) / length(pvalues)
```

## Getting that p-value

You can access the p-value by storing the summary in an object (as a list) and accessing its component. For the `lm` summary, that's a bit less straightforward. See <https://stackoverflow.com/questions/5587676/pull-out-p-values-and-r-squared-from-a-linear-regression>.

## About that effect size

Let's just swap the low and high conditions.

```{r}
d2 <- d

levels(d2$condition) <- c("control", "low", "high")
```

:::: {.columns}

:::: {.column}

```{r}
#| echo: false
d <- d %>% mutate(condition = fct_relevel(condition, "control", "low"))
boxplot(d$y ~ d$condition)
```

::::

:::: {.column}

```{r}
#| echo: false
boxplot(d2$y ~ d2$condition)
```

::::

::::

## Now what are the model statistics?

:::: {.columns}

:::: {.column}

```{r}
summary(lm(y ~ condition, d))
```

::::

:::: {.column}

```{r}
summary(lm(y ~ condition, d2))
```

::::

::::

## What about the effect size?

```{r}
effectsize::eta_squared(lm(y ~ condition, d))
effectsize::eta_squared(lm(y ~ condition, d2))
```

## How would I go for power now?

- Back to standardized vs. unstandardized
- Best to "draw" first
- Determine contrast of interest

## What about correlated measures?

Remember drawing two scores from the same unit?

$$
\begin{bmatrix} 
var  & cov \\ 
cov & var \\ 
\end{bmatrix}
$$

We just need to extend that to the number of measures:

$$
\begin{bmatrix} 
var & cov & cov\\ 
cov & var & cov\\
cov & cov & var
\end{bmatrix}
$$

## What we need to know

- The standard deviation for each measure ($\sigma_{x_1}$)
- The correlation between each measure ($r_{x_1x_2}$)

$$
\begin{bmatrix} 
SD_{x_1} & r_{x_1x_2} & r_{x_1x_3}\\
r_{x_2x_1} & SD_{x_2} & r_{x_2x_3}\\
r_{x_3x_1} & r_{x_3x_2} & SD_{x_3}
\end{bmatrix}
$$

## Same as before

Define our parameters and get variance-covariance matrix.

```{r}
means <- c(control = 100, low = 103, high = 105)
sd <- 8
correlation <- 0.4
covariance <- correlation * sd * sd

our_matrix <- matrix(
  c(
    sd**2, covariance, covariance,
    covariance, sd**2, covariance,
    covariance, covariance, sd**2
  ),
  ncol = 3
)

our_matrix
```

## Then draw from multivariate normal

```{r}
library(MASS)

set.seed(42)

d <- 
  mvrnorm(
    200,
    means,
    our_matrix
  )
d <- as.data.frame(d)
d$id <- factor(1:200)
head(d)
```

## Let's check

Can we recover our numbers of means = `c(100, 103, 105)`, SD of 8, and correlation of 0.4?

```{r}
mean(d$control); mean(d$low); mean(d$high)
sd(d$control); sd(d$low); sd(d$high)
cor(d$control, d$low); cor(d$control, d$high); cor(d$low, d$high);
```

## We can't use `lm` this time

The linear model has several assumptions, one of which is that observations are independent. They clearly aren't (we specified a correlation between them after all). So we need to go for models that take this dependence into account.

For most cases, data need to be in the long format:

```{r}
library(tidyr)

d2 <- 
  pivot_longer(
    d,
    cols = -id,
    names_to = "condition",
    values_to = "score"
  )
```

## Long format

:::: {.columns}

:::: {.column}

```{r}
#| echo: false

knitr::kable(head(d))
```

::::

:::: {.column}

```{r}
#| echo: false

knitr::kable(head(d2))
```

::::

::::

## Let's run the RM ANOVA

```{r}
model <- aov(score ~ condition + Error(id), d2)
summary(model)
```

## Just a (hierarchical) linear model

:::: {.columns}

:::: {.column}

```{r}
summary(model)
```

::::

:::: {.column}
```{r}
library(lme4)

model2 <- lmer(score ~ condition + (1 | id), d2)
anova(model2)
```

::::

::::

Check [here](https://m-clark.github.io/docs/mixedModels/anovamixed.html#repeated_measures_anova) for more background info.

## This is the max

The workshop won't go further than (interactions) with repeated measures. More resources on simulating more complicated designs at the final wrap up.

# Takeaways

- Understand the logic behind the data generating process
- See how the linear model is our data generating process
- Apply this to a setting with multiple categories in a predictor

# Let's get simulating