[
  {
    "objectID": "content/01-intro/01-slides.html",
    "href": "content/01-intro/01-slides.html",
    "title": "Slides",
    "section": "",
    "text": "If you want to see the slides in full screen, you can click here. To download them as PDFs, hit the ‘e’ button when you got the presentation open and then Ctrl+P print to PDF. This will work best in Chrome (in Firefox, it didn’t print the headings for me)."
  },
  {
    "objectID": "content/01-intro/slides/index.html#why-this-workshop",
    "href": "content/01-intro/slides/index.html#why-this-workshop",
    "title": "Welcome",
    "section": "Why this workshop",
    "text": "Why this workshop\n\nDesigning an informative study is a key skill\nA study is rarely informative if it can’t detect what you’re after\nNeglecting power means not knowing what our results mean"
  },
  {
    "objectID": "content/01-intro/slides/index.html#okay-so-i-use-gpower",
    "href": "content/01-intro/slides/index.html#okay-so-i-use-gpower",
    "title": "Welcome",
    "section": "Okay, so I use GPower?",
    "text": "Okay, so I use GPower?"
  },
  {
    "objectID": "content/01-intro/slides/index.html#gpower-is-great",
    "href": "content/01-intro/slides/index.html#gpower-is-great",
    "title": "Welcome",
    "section": "GPower is great",
    "text": "GPower is great\n\nGPower works great!\nRuns the risk of treating power just as a hoop to jump through\nSimulating data instead forces us to be explicit about many more features than GPower asks for"
  },
  {
    "objectID": "content/01-intro/slides/index.html#so-why-are-we-here",
    "href": "content/01-intro/slides/index.html#so-why-are-we-here",
    "title": "Welcome",
    "section": "So why are we here?",
    "text": "So why are we here?\nThe goal of the workshop is for you to\n\nhave an understanding of the philosophy behind using data to test claims,\nget an intuition of how data generation processes work,\nlearn the technical skills to turn these processes into data, and\nuse these skills to simulate power for an informative study."
  },
  {
    "objectID": "content/01-intro/slides/index.html#so-what-will-you-learn",
    "href": "content/01-intro/slides/index.html#so-what-will-you-learn",
    "title": "Welcome",
    "section": "So what will you learn",
    "text": "So what will you learn\nA bit of everything. It’ll be a weird mix:\n\nPhilosophy of science\nMeta-science\nStatistics\nR"
  },
  {
    "objectID": "content/01-intro/slides/index.html#what-i-expect-from-you",
    "href": "content/01-intro/slides/index.html#what-i-expect-from-you",
    "title": "Welcome",
    "section": "What I expect from you",
    "text": "What I expect from you\n\nA vague memory of your stats courses\nSome familiarity with R (and RStudio)\nTolerance for confusion\nEnthusiasm for 2 days (or at least resilience)"
  },
  {
    "objectID": "content/01-intro/slides/index.html#what-you-can-expect-from-me",
    "href": "content/01-intro/slides/index.html#what-you-can-expect-from-me",
    "title": "Welcome",
    "section": "What you can expect from me",
    "text": "What you can expect from me\n\nComprehensive overview\nLots of exercises to apply what you learned\nFlexibility in schedule\nEnthusiasm for 2 days"
  },
  {
    "objectID": "content/01-intro/slides/index.html#principle",
    "href": "content/01-intro/slides/index.html#principle",
    "title": "Welcome",
    "section": "Principle",
    "text": "Principle\nWe’ll follow the same routine over and over: Learn, do, recall.\n\nTheory (I talk)\nExercises (You apply)\nQuiz (You recall)"
  },
  {
    "objectID": "content/01-intro/slides/index.html#day-1",
    "href": "content/01-intro/slides/index.html#day-1",
    "title": "Welcome",
    "section": "Day 1",
    "text": "Day 1\n\n\n\nWhat?\nWhen?\n\n\n\n\n9:00-9:45\nIntro (now)\n\n\n10:00-10:45\nWhat’s power?\n\n\n11:00-11:45\nSimulations in R\n\n\n12:00-13:00\nExercise 1\n\n\n14:00-14:45\nEffect sizes\n\n\n15:00-15:45\nExercise 2\n\n\n16:00-16:45\nAlpha, beta, sensitivity\n\n\n17:00-17:45\nExercise 3\n\n\n17:45-18:00\nRecap"
  },
  {
    "objectID": "content/01-intro/slides/index.html#day-2",
    "href": "content/01-intro/slides/index.html#day-2",
    "title": "Welcome",
    "section": "Day 2",
    "text": "Day 2\n\n\n\nWhat?\nWhen?\n\n\n\n\n9:00-9:45\nRecap\n\n\n10:00-10:45\nCategorical predictors\n\n\n11:00-11:45\nExercise 4\n\n\n12:00-13:00\nInteractions\n\n\n14:00-14:45\nExercise 5\n\n\n15:00-15:45\nContinuous predictors\n\n\n16:00-16:45\nExercise 6\n\n\n17:00-17:45\nBuffer\n\n\n17:45-18:00\nRecap"
  },
  {
    "objectID": "content/01-intro/slides/index.html#whats-power",
    "href": "content/01-intro/slides/index.html#whats-power",
    "title": "Welcome",
    "section": "What’s power",
    "text": "What’s power\n\nUnderstanding of the logic behind NHST\nIntuition about what power is\nSee why power, perhaps, potentially isn’t just a hoop to jump through"
  },
  {
    "objectID": "content/01-intro/slides/index.html#simulations-in-r",
    "href": "content/01-intro/slides/index.html#simulations-in-r",
    "title": "Welcome",
    "section": "Simulations in R",
    "text": "Simulations in R\n\nUnderstand why simulations are useful\nLogic of Monte Carlo Simulations\nBasic tools"
  },
  {
    "objectID": "content/01-intro/slides/index.html#effect-sizes",
    "href": "content/01-intro/slides/index.html#effect-sizes",
    "title": "Welcome",
    "section": "Effect sizes",
    "text": "Effect sizes\n\nUnderstand the importance of effect sizes\nHow to formulate a smallest effect size of interest\nKnow when you don’t have enough information"
  },
  {
    "objectID": "content/01-intro/slides/index.html#alpha-beta-sensitivity",
    "href": "content/01-intro/slides/index.html#alpha-beta-sensitivity",
    "title": "Welcome",
    "section": "Alpha, beta, sensitivity",
    "text": "Alpha, beta, sensitivity\n\nQuestion the default of \\(\\alpha\\) = 0.05 and power = 80%\nUnderstand how terribly complex designing an informative study is\nKnow where to turn when you don’t have enough information"
  },
  {
    "objectID": "content/01-intro/slides/index.html#categorical-predictors",
    "href": "content/01-intro/slides/index.html#categorical-predictors",
    "title": "Welcome",
    "section": "Categorical predictors",
    "text": "Categorical predictors\n\nUnderstand the logic behind the data generating process\nSee how the linear model is our data generating process\nApply this to a setting with multiple categories in a predictor"
  },
  {
    "objectID": "content/01-intro/slides/index.html#interactions",
    "href": "content/01-intro/slides/index.html#interactions",
    "title": "Welcome",
    "section": "Interactions",
    "text": "Interactions\n\nUnderstand what an interaction is from the perspective of the linear model\nMake yourself think in more detail about the form of interactions\nBe able to translate that detail to generating data"
  },
  {
    "objectID": "content/01-intro/slides/index.html#continuous-predictors",
    "href": "content/01-intro/slides/index.html#continuous-predictors",
    "title": "Welcome",
    "section": "Continuous predictors",
    "text": "Continuous predictors\n\nUnderstand that continuous predictors are just another case of the linear model\nExtend this understanding to continuous (by categorical) interactions\nBe able to translate that extension to generating data"
  },
  {
    "objectID": "content/01-intro/slides/index.html#youre-guinea-pigs",
    "href": "content/01-intro/slides/index.html#youre-guinea-pigs",
    "title": "Welcome",
    "section": "You’re guinea pigs",
    "text": "You’re guinea pigs\n\nFirst time I’m giving this workshop, so timing might be way off\nThat’s why these slides are so full: I wrote the entire thing so you can go and revisit\nAlways interrupt!"
  },
  {
    "objectID": "content/01-intro/slides/index.html#materials",
    "href": "content/01-intro/slides/index.html#materials",
    "title": "Welcome",
    "section": "Materials",
    "text": "Materials\n\nEverything is up on: https://github.com/niklasjohannes/power-workshop\nJust download everything as a zip file (Code -> Zip)\nRendered to follow along here: https://niklasjohannes.github.io/power-workshop/\nThere, you’ll also find instructions on how to download R, RStudio, and GPower\nFor discussions we use Discord:"
  },
  {
    "objectID": "content/01-intro/slides/index.html#stealing-stuff",
    "href": "content/01-intro/slides/index.html#stealing-stuff",
    "title": "Welcome",
    "section": "Stealing stuff",
    "text": "Stealing stuff\nI cite all my sources, but relied heavily on the following:\n\nJulian Quandt’s power simulation tutorials\nCourses by Daniel Lakens (textbook)\nTutorial by Ariel Muldoon\nBen Staton on Monte Carlo methods\nR for Data Science"
  },
  {
    "objectID": "content/01-intro/slides/index.html#on-r-code-and-efficiency",
    "href": "content/01-intro/slides/index.html#on-r-code-and-efficiency",
    "title": "Welcome",
    "section": "On R code and efficiency",
    "text": "On R code and efficiency\n\nI focused on base R and making things simple rather than fast\nThis workshop is about getting the principles, rarely about coding\nThere are (much) better ways to implement"
  },
  {
    "objectID": "content/02-whats-power/02-slides.html",
    "href": "content/02-whats-power/02-slides.html",
    "title": "Slides",
    "section": "",
    "text": "Below you can find the slides for session on power:\n\n\n\nIf you want to see the slides in full screen, you can click here. To download them as PDFs, hit the ‘e’ button when you got the presentation open and then Ctrl+P print to PDF. This will work best in Chrome (in Firefox, it didn’t print the headings for me)."
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#where-it-all-started",
    "href": "content/02-whats-power/slides/index.html#where-it-all-started",
    "title": "What’s power?",
    "section": "Where it all started",
    "text": "Where it all started\n\n\nSir Ronald Fisher (1890-1962)\n\n\n\nProbability value = p-value\nCentral question: How likely is it to observe such data if there were nothing?"
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#the-original-p-value",
    "href": "content/02-whats-power/slides/index.html#the-original-p-value",
    "title": "What’s power?",
    "section": "The original p-value",
    "text": "The original p-value\n\n\n\n\n\n\nTea or milk first\nHow many cups would you want to be convinced?\nCorrectly identifying all 8 cups: 1.4% chance to occur if the lady can’t taste the difference\n1.4% < 5%"
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#whats-a-p-value",
    "href": "content/02-whats-power/slides/index.html#whats-a-p-value",
    "title": "What’s power?",
    "section": "What’s a p-value",
    "text": "What’s a p-value\nInformally: What’s the chance of observing something like this if there were nothing going on?\n\\[\\begin{gather*}\nChance = (Finding \\ something \\ like \\ this \\ | \\ Nothing \\ going \\ on)\n\\end{gather*}\\]\nFormally: The probability of observing data this extreme or more extreme under the null hypothesis\n\\[\\begin{gather*}\nP = (Data|H_0)\n\\end{gather*}\\]"
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#decisions",
    "href": "content/02-whats-power/slides/index.html#decisions",
    "title": "What’s power?",
    "section": "Decisions",
    "text": "Decisions\n\n\nJerzy Neyman (1894-1981)\n\n\n\nWe want to make a decision\nJust rejecting H0 doesn’t help much\nLet’s introduce an alternative: HA or H1\nSo we need decision rules!"
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#error-rates",
    "href": "content/02-whats-power/slides/index.html#error-rates",
    "title": "What’s power?",
    "section": "Error rates",
    "text": "Error rates\nIf we’re forced to make a decision, then error rates are what we deem acceptable levels of being right/wrong in the long-run:\n\n\\(\\alpha\\)\n\\(\\beta\\)\n1 - \\(\\beta\\)\n1 - \\(\\alpha\\)"
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#when-theres-nothing",
    "href": "content/02-whats-power/slides/index.html#when-theres-nothing",
    "title": "What’s power?",
    "section": "When there’s nothing",
    "text": "When there’s nothing\nWhen there truly is no effect, two things can happen: We find a significant effect or we don’t.\n\nFalse positive: Saying there’s something when there’s nothing (Type I error)\nTrue negative: Saying there isn’t something when there is nothing. (Correct)"
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#when-theres-nothing-1",
    "href": "content/02-whats-power/slides/index.html#when-theres-nothing-1",
    "title": "What’s power?",
    "section": "When there’s nothing",
    "text": "When there’s nothing\nWhen there truly is no effect, two things can happen: We find a significant effect (error) or we don’t (no error).\n\n\\(\\alpha\\): The probability of observing a significant result when H0 is true.\n1 - \\(\\alpha\\): The probability of observing a nonsignificant result when H0 is true."
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#when-theres-something",
    "href": "content/02-whats-power/slides/index.html#when-theres-something",
    "title": "What’s power?",
    "section": "When there’s something",
    "text": "When there’s something\nWhen there truly is an effect, two things can happen: We find no significant effect (error) or we find one (correct).\n\nFalse negative: Saying there isn’t something when there’s something (Type II error)\nTrue positive: Saying there’s something when there’s something. (Correct)"
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#when-theres-something-1",
    "href": "content/02-whats-power/slides/index.html#when-theres-something-1",
    "title": "What’s power?",
    "section": "When there’s something",
    "text": "When there’s something\nWhen there truly is an effect, two things can happen: We find no significant effect (error) or we find one (correct).\n\n\\(\\beta\\): The probability of observing a nonsignificant result when H1 is true\n1 - \\(\\beta\\): The probability of observing a significant result when H1 is true."
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#why-am-i-talking-so-weird",
    "href": "content/02-whats-power/slides/index.html#why-am-i-talking-so-weird",
    "title": "What’s power?",
    "section": "Why am I talking so weird?",
    "text": "Why am I talking so weird?\n“When there isn’t something?” Why not just say there’s nothing?\n\\[\\begin{gather*}\n(Data|H_0) \\neq (H_0|Data)\n\\end{gather*}\\]\nWe can’t find evidence for H0 with “classical” NHST (unless we use equivalence tests). A nonsignificant p-value only means we can’t reject H0, and therefore can’t accept H1, but we can’t accept H0."
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#wheres-our-power",
    "href": "content/02-whats-power/slides/index.html#wheres-our-power",
    "title": "What’s power?",
    "section": "Where’s our power?",
    "text": "Where’s our power?\n\n\n\n\n\n\n\n\n\nH0 true\nH1 true\n\n\n\n\nSignificant\nFalse Positive (\\(\\alpha\\))\nTrue Positive (1-\\(\\beta\\))\n\n\nNonsignificant\nTrue Negative (1-\\(\\alpha\\))\nFalse negative (\\(\\beta\\))"
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#power",
    "href": "content/02-whats-power/slides/index.html#power",
    "title": "What’s power?",
    "section": "Power",
    "text": "Power\nPower is the probability of finding a significant result when there is an effect. It’s determined (simplified) by:\n\nEffect size\nSample size\nError rates (\\(\\alpha\\) and \\(\\beta\\))"
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#pictures-please",
    "href": "content/02-whats-power/slides/index.html#pictures-please",
    "title": "What’s power?",
    "section": "Pictures, please",
    "text": "Pictures, please\nLet’s assume we want to know whether the population mean is larger than 50. We sample n = 100."
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#the-null-distribution",
    "href": "content/02-whats-power/slides/index.html#the-null-distribution",
    "title": "What’s power?",
    "section": "The null distribution",
    "text": "The null distribution\nThis is the sampling distribution if the null were true: The true effect is 50."
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#the-null-distribution-1",
    "href": "content/02-whats-power/slides/index.html#the-null-distribution-1",
    "title": "What’s power?",
    "section": "The null distribution",
    "text": "The null distribution\nWhere does a sample need to fall for us to wrongly conclude there’s a difference?"
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#the-null-distribution-2",
    "href": "content/02-whats-power/slides/index.html#the-null-distribution-2",
    "title": "What’s power?",
    "section": "The null distribution",
    "text": "The null distribution\nThat’s our \\(\\alpha\\): our false positives. Left of it: our true negatives (1-\\(\\alpha\\))."
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#where-would-we-conclude-its-coming-from-then",
    "href": "content/02-whats-power/slides/index.html#where-would-we-conclude-its-coming-from-then",
    "title": "What’s power?",
    "section": "Where would we conclude it’s coming from then?",
    "text": "Where would we conclude it’s coming from then?\nOur sampling distribution if the population value is 60. We commit a false positive if we assume a sample comes from the right distribution if in fact it comes from the left."
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#what-about-the-reverse",
    "href": "content/02-whats-power/slides/index.html#what-about-the-reverse",
    "title": "What’s power?",
    "section": "What about the reverse?",
    "text": "What about the reverse?\nOur \\(\\beta\\): our false negatives. We commit a false negative if we assume a sample comes from the left distribution if in fact it comes from the right."
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#wheres-power-then",
    "href": "content/02-whats-power/slides/index.html#wheres-power-then",
    "title": "What’s power?",
    "section": "Where’s power then?",
    "text": "Where’s power then?"
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#wheres-power-then-1",
    "href": "content/02-whats-power/slides/index.html#wheres-power-then-1",
    "title": "What’s power?",
    "section": "Where’s power then?",
    "text": "Where’s power then?\nEverything right of the critical value: If a sample comes from the right distribution, this is how often we’ll correctly identify it."
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#what-determined-power-again",
    "href": "content/02-whats-power/slides/index.html#what-determined-power-again",
    "title": "What’s power?",
    "section": "What determined power again?",
    "text": "What determined power again?\nPower is the probability of finding a significant result when there is an effect. It’s determined (simplified) by:\n\nEffect size\nSample size\nError rates (\\(\\alpha\\) and \\(\\beta\\))\n\nLet’s have a look how: Preview"
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#why-does-power-matter",
    "href": "content/02-whats-power/slides/index.html#why-does-power-matter",
    "title": "What’s power?",
    "section": "Why does power matter?",
    "text": "Why does power matter?\nRunning studies with low power (aka underpowered studies) risks:\n\nMissing effects\nInflating those effects we find\nLower chance that a significant result is true"
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#missing-effects",
    "href": "content/02-whats-power/slides/index.html#missing-effects",
    "title": "What’s power?",
    "section": "Missing effects",
    "text": "Missing effects\nSociety has commissioned us to find out something. Why would we start by setting us up so that we’re barely able to do that?\n\nWaste of resources\nSuper frustrating\nDissuades others\nCan slow down entire research lines"
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#inflating-those-effects-we-find",
    "href": "content/02-whats-power/slides/index.html#inflating-those-effects-we-find",
    "title": "What’s power?",
    "section": "Inflating those effects we find",
    "text": "Inflating those effects we find\nLet’s go back to our example. Let’s assume we want to know whether the population mean is larger than 50. We sample n = 100."
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#what-if-we-sample-only-10",
    "href": "content/02-whats-power/slides/index.html#what-if-we-sample-only-10",
    "title": "What’s power?",
    "section": "What if we sample only 10?",
    "text": "What if we sample only 10?\nThe sampling distribution gets wider: Now a sample mean needs to be really large to be significant. The smaller our sample (aka the lower our power), the more extreme a sample has to be to “make it” across the critical value."
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#low-power-inflates-effects",
    "href": "content/02-whats-power/slides/index.html#low-power-inflates-effects",
    "title": "What’s power?",
    "section": "Low power inflates effects",
    "text": "Low power inflates effects\nIf our study is small (has low power), only an overestimate will pass our threshold for significance. With underpowered studies, significant results will always be an overestimate. Below, even effects that are larger than the average true effect won’t be found."
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#how-true-is-a-study",
    "href": "content/02-whats-power/slides/index.html#how-true-is-a-study",
    "title": "What’s power?",
    "section": "How true is a study?",
    "text": "How true is a study?\nHow many effects will we expect?\n\nProbability to find an effect = power\nOdds of there being an effect = R\n\nSo:\n\\[\\begin{gather*}\npower \\times R\n\\end{gather*}\\]"
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#how-true-is-a-study-1",
    "href": "content/02-whats-power/slides/index.html#how-true-is-a-study-1",
    "title": "What’s power?",
    "section": "How true is a study?",
    "text": "How true is a study?\nHow many significant results do we expect?\n\nTrue effects (power x R)\nFalse positive\n\n\\[\\begin{gather*}\npower \\times R + \\alpha\n\\end{gather*}\\]"
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#positive-predictive-value",
    "href": "content/02-whats-power/slides/index.html#positive-predictive-value",
    "title": "What’s power?",
    "section": "Positive predictive value",
    "text": "Positive predictive value\nWhat is the probability that a significant effect is indeed true? The rate of significant results that represent true effects divided by all significant results.\n\\[\\begin{gather*}\nPPV = \\frac{power \\times R}{power \\times R + \\alpha}\n\\end{gather*}\\]"
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#an-example",
    "href": "content/02-whats-power/slides/index.html#an-example",
    "title": "What’s power?",
    "section": "An example",
    "text": "An example\nLet’s assume our hypothesis has a 25% of being true and we go for the “conventional” alpha-level (5%).\n\\[\\begin{gather*}\nPPV = \\frac{power \\times R}{power \\times R + \\alpha}\n= \\frac{power \\times \\frac{P(effect)}{P(No \\ effect)}}{power \\times \\frac{P(effect)}{P(No \\ effect)} + \\alpha}\n\\end{gather*}\\]\n\n\nWith 95% power and 1/4 odds?\n86%\nWith 40% power and 1/4 odds?\n73%"
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#what-does-this-mean",
    "href": "content/02-whats-power/slides/index.html#what-does-this-mean",
    "title": "What’s power?",
    "section": "What does this mean?",
    "text": "What does this mean?\nBottom line: The lower our power, the lower the probability that our significant effects represent the truth. Aka: Low power produces false findings."
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#why-should-we-care",
    "href": "content/02-whats-power/slides/index.html#why-should-we-care",
    "title": "What’s power?",
    "section": "Why should we care?",
    "text": "Why should we care?\nHeard of the replication crisis?\n\n(Baker 2016)"
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#killer-combo",
    "href": "content/02-whats-power/slides/index.html#killer-combo",
    "title": "What’s power?",
    "section": "Killer combo",
    "text": "Killer combo\n\nBad research + low power\nFalse positives\nInflated effect sizes\nInflated false positives = low credibility and a waste of resources\n\n\n“[The] lack of transparency in science has led to quality uncertainty, and . . . this threatens to erode trust in science” (Vazire 2017)"
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#on-the-flip-side",
    "href": "content/02-whats-power/slides/index.html#on-the-flip-side",
    "title": "What’s power?",
    "section": "On the flip side",
    "text": "On the flip side\n\nOversampling risks wasting resources too\nValue of information: Not every data point has the same value\nOur power should align with our inferential goals"
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#references",
    "href": "content/02-whats-power/slides/index.html#references",
    "title": "What’s power?",
    "section": "References",
    "text": "References\n\n\nBaker, Monya. 2016. “1,500 Scientists Lift the Lid on Reproducibility.” Nature 533 (7604): 452–54. https://doi.org/10.1038/533452a.\n\n\nVazire, Simine. 2017. “Quality uncertainty erodes trust in science.” Collabra: Psychology 3 (1): 1. https://doi.org/10.1525/collabra.74.\n\n\n\n\nHome"
  },
  {
    "objectID": "content/03-simulations-in-r/03-exercise.html#block-1",
    "href": "content/03-simulations-in-r/03-exercise.html#block-1",
    "title": "Exercise I",
    "section": "0.1 Block 1",
    "text": "0.1 Block 1\n\n0.1.1 Exercise\nYou have three groups. Name the groups, randomly sample ten cases (in total, so uneven numbers per group), and then create a simple data frame that contains a variable for the group called condition.\n\ngroups <- letters[1:3]\nmy_sample <- sample(groups, 10, TRUE)\nd <- data.frame(condition = as.factor(my_sample))\n\n\n\n0.1.2 Exercise\nSame three groups. This time you want each case to have a 70% to be in the first group, a 20% to be in the second group, and a 10% to be in the third group. Get 100 participants (no need for a data frame). Use set.seed(1). How many are in the first group?\n\nset.seed(1)\ngroups <- letters[1:3]\nmy_sample <- \n  sample(\n    groups,\n    100,\n    TRUE,\n    prob = c(0.7, 0.2, 0.1)\n  )\nsum(my_sample==groups[1])\n\n[1] 68\n\n\n\n\n0.1.3 Exercise\nShow that sample with assigned probability (prob = argument) is the same as rbinom. Conduct 10 coin flips with a an unfair coin that has a 60% of landing heads. Remember to set a seed (tip: twice).\n\nset.seed(1)\nsample(0:1, 10, replace = TRUE, prob = c(0.4, 0.6))\n\n [1] 1 1 1 0 1 0 0 0 0 1\n\nset.seed(1)\nrbinom(10, 1, 0.6)\n\n [1] 1 1 1 0 1 0 0 0 0 1\n\n\n\n\n0.1.4 Exercise\nDraw random letters from the alphabet until the alphabet is empty.\n\nsample(letters, length(letters))\n\n [1] \"s\" \"a\" \"u\" \"x\" \"j\" \"n\" \"v\" \"g\" \"i\" \"o\" \"e\" \"p\" \"r\" \"w\" \"q\" \"l\" \"b\" \"m\" \"d\"\n[20] \"z\" \"y\" \"h\" \"c\" \"t\" \"k\" \"f\"\n\n\n\n\n0.1.5 Exercise\nDraw all letters from the alphabet and explicitly assign the same probability for each letter (tip: repeat the same probability).\n\nprobs <- rep(1/length(letters), length(letters))\nsample(letters, prob = probs)\n\n [1] \"r\" \"v\" \"o\" \"n\" \"w\" \"b\" \"l\" \"t\" \"s\" \"k\" \"y\" \"i\" \"f\" \"c\" \"e\" \"j\" \"q\" \"u\" \"m\"\n[20] \"a\" \"g\" \"p\" \"h\" \"x\" \"d\" \"z\"\n\n\n\n\n0.1.6 Exercise\nCreate a data set. In the data set, each participant has an identifier (called id), a group identifier (condition), and an identifier of what number of measurement we have for this participant (trial). There are 3 participants in each of three groups with 5 trials in each group.\n\ngroups <- letters[1:3]\nparticipants <- length(groups) * 3\ntrials <- 5\n\nd <- \n  data.frame(\n    id = rep(1:participants, each = trials),\n    condition = rep(groups, each = trials, times = 3),\n    trial = rep(1:trials, times = participants)\n  )\n\nd\n\n   id condition trial\n1   1         a     1\n2   1         a     2\n3   1         a     3\n4   1         a     4\n5   1         a     5\n6   2         b     1\n7   2         b     2\n8   2         b     3\n9   2         b     4\n10  2         b     5\n11  3         c     1\n12  3         c     2\n13  3         c     3\n14  3         c     4\n15  3         c     5\n16  4         a     1\n17  4         a     2\n18  4         a     3\n19  4         a     4\n20  4         a     5\n21  5         b     1\n22  5         b     2\n23  5         b     3\n24  5         b     4\n25  5         b     5\n26  6         c     1\n27  6         c     2\n28  6         c     3\n29  6         c     4\n30  6         c     5\n31  7         a     1\n32  7         a     2\n33  7         a     3\n34  7         a     4\n35  7         a     5\n36  8         b     1\n37  8         b     2\n38  8         b     3\n39  8         b     4\n40  8         b     5\n41  9         c     1\n42  9         c     2\n43  9         c     3\n44  9         c     4\n45  9         c     5\n\n\n\n\n0.1.7 Exercise\nYou have two groups, a control and a treatment group. In each group, there are 10 participants. Each participant flips a coin 10 times. The control group has a fair coin: 50% heads. The treatment group has an unfair coin: 70% heads. Create a data frame with a participant identifier (id), group membership (condition), and a total head count for that participant (heads). Check that the two groups indeed have different means of how often they get heads (roughly corresponding to the two probabilities)\n\ngroups <- c(\"control\", \"treatment\")\nn <- 20\nflips <- 10\nfair <- 0.5\nunfair <- 0.7\nprobs <- rep(c(fair, unfair), each = n/2)\n\n\nd <- \n  data.frame(\n    id = 1:20,\n    condition = rep(groups, each = n/2),\n    heads = rbinom(n, flips, prob = probs)\n  )\n\naggregate(d$heads, by = list(d$condition), mean)\n\n    Group.1   x\n1   control 5.4\n2 treatment 6.5\n\n\n\n\n0.1.8 Exercise\nYou have 100 participants. Each participants reports their age which lies uniformly between 20 and 40. They also report grumpiness on a 100-point scale (with a 10-point SD). Each extra year predicts 0.5 higher grumpiness (Tip: Check the lecture slides; you’ll need to add some error). Create the two variables (no need for a data frame) and conduct a correlation with cor. What’s the correlation?\n\nn <- 100\nage <- runif(n, 20, 40)\ngrumpiness <- age*0.5 + rnorm(n, 100, 10)\ncor(age, grumpiness)\n\n[1] 0.2127516\n\n\n\n\n0.1.9 Exercise\nWe track how many calls you get during this exercise. Nobody calls anymore, so there’ll be very few. Create a data frame with 20 participants, a participant number and the number of calls per participant. Plot the calls and show that 0 is the most common value.\n\nn <- 20\n\nd <- data.frame(\n  id = 1:n,\n  calls = rpois(n, 0.5)\n)\n\nplot(density(d$calls)) \n\n\n\n\n\n\n0.1.10 Exercise\nProfessors get more calls. Add 20 more participants who have much higher call numbers. Also include a condition variable that marks whether participants are students (first 20 people) or professors (new 20 people). Conduct an independent sample t-test (t.test) and also plot the different groups as a boxplot.\n\nd <- rbind(\n  d,\n    data.frame(\n    id = n+1:2*n,\n    calls = rpois(n, 3)\n  )\n)\n\nd$condition <- \n  rep(c(\"students\", \"profs\"), each = 20)\n\nt.test(d$calls ~ d$condition)\n\n\n    Welch Two Sample t-test\n\ndata:  d$calls by d$condition\nt = 5.8023, df = 26.202, p-value = 3.989e-06\nalternative hypothesis: true difference in means between group profs and group students is not equal to 0\n95 percent confidence interval:\n 1.679272 3.520728\nsample estimates:\n   mean in group profs mean in group students \n                  3.25                   0.65 \n\nboxplot(d$calls ~ d$condition)"
  },
  {
    "objectID": "content/03-simulations-in-r/03-exercise.html#block-2",
    "href": "content/03-simulations-in-r/03-exercise.html#block-2",
    "title": "Exercise I",
    "section": "0.2 Block 2",
    "text": "0.2 Block 2\nIn this section, you’ll use the basics from above to perform your first power analysis. You’ll apply repeated simulations over a range of values and extracting and storing results to summarize them.\n\n0.2.1 Exercise\nThere are four groups. Each group comes from a different normal distribution. The means are c(100, 105, 107, 109). The SDs are c(9, 12, 10, 17). Each group should be 20 cases. Store everything in a data frame and have a variable that indicates the group. Tip: Remember that R uses vectors, even for arguments in a function.\n\nmeans <- c(100, 105, 107, 109)\nsds <- c(9, 12, 10, 17)\nn <- 20\n\nd <- \n  data.frame(\n    group = rep(c(letters[1:4]), times = n),\n    score = rnorm(n*4, means, sds)\n  )\n\n\n\n0.2.2 Exercise\nYou need 5 samples. Each sample contains 10 unique letters from the alphabet. (Use replicate.)\n\nreplicate(5, sample(letters, 10))\n\n      [,1] [,2] [,3] [,4] [,5]\n [1,] \"o\"  \"u\"  \"p\"  \"l\"  \"e\" \n [2,] \"p\"  \"e\"  \"g\"  \"j\"  \"w\" \n [3,] \"q\"  \"l\"  \"z\"  \"a\"  \"f\" \n [4,] \"s\"  \"c\"  \"q\"  \"f\"  \"s\" \n [5,] \"n\"  \"r\"  \"k\"  \"q\"  \"t\" \n [6,] \"f\"  \"n\"  \"i\"  \"m\"  \"l\" \n [7,] \"k\"  \"m\"  \"h\"  \"b\"  \"c\" \n [8,] \"h\"  \"o\"  \"m\"  \"o\"  \"d\" \n [9,] \"e\"  \"b\"  \"f\"  \"d\"  \"o\" \n[10,] \"b\"  \"t\"  \"x\"  \"z\"  \"n\" \n\n\n\n\n0.2.3 Exercise\nSame as before, but this time you need 10 samples from a normal distribution with a mean of 10 and an SD of 2. Use replicate first, then a for loop.\n\nreplicate(5, rnorm(10, 10, 2))\n\n           [,1]      [,2]      [,3]      [,4]      [,5]\n [1,] 10.629034 10.077856 10.916251 11.087316  8.782663\n [2,] 10.444436 10.029294  7.095943 12.082121  9.397606\n [3,]  8.312769  9.627366 10.154685 10.395012 11.952405\n [4,] 10.887611 12.801182 11.119791  6.740843 10.912017\n [5,] 10.111594 10.036971  9.850107 10.242080 12.588816\n [6,] 10.135945 10.498392 11.565395  6.725156  7.733596\n [7,]  9.596077 10.298441  9.654663  8.937914  8.261079\n [8,]  7.683969  8.073534  7.897412 11.907360  8.490059\n [9,]  8.814513  9.867066 11.458903  6.558699  9.740729\n[10,] 11.532131 12.573844 10.525330 10.212641  7.996397\n\nfor (i in 1:5) {\n  print(rnorm(10, 10, 2))\n}\n\n [1]  8.360263  8.050895 11.208619 11.097575 11.832865 15.323133  9.639486\n [8] 11.370030 16.532829 11.121201\n [1]  9.861965  8.055114  8.906827  6.622615  6.855255  9.190026 10.638573\n [8] 10.080855  9.219981  6.361556\n [1] 11.318361 10.919243 13.233253  6.287619  9.426352 13.500644 10.232827\n [8] 12.768506 11.148442 10.272982\n [1] 11.828432  6.398347  9.320239 11.212529 12.682261 11.534575 10.387451\n [8] 12.281133 10.027730  7.789388\n [1]  9.949675  9.672653 10.740119  9.238351 11.305905 14.122684  6.406710\n [8] 11.168154  8.554494  8.741671\n\n\n\n\n0.2.4 Exercise\nAssume we know the population mean in height (168cm) and its standard deviation (20). Assume we draw 10,000 samples from this distribution. Each sample has 50 participants. The standard deviation of these 10,000 sample means is the standard error.\nSimulate the standard error and compare it to the theoretical value: \\(SE = \\frac{\\sigma}{\\sqrt{n}}\\). (\\(\\sigma\\) is the standard deviation of the population.)\n\nn <- 50\nmu <- 168\nsd <- 20\nse <- sd / sqrt(n)\nmeans <- NULL \ndraws <- 1e5\n\nfor (i in 1:draws) {\n  means[i] <- mean(rnorm(n, mu, sd))\n}\n\ncat(\"The real SE is:\", round(se, digits = 2), \". The simulated SE is:\", round(sd(means), digits = 2), \".\")\n\nThe real SE is: 2.83 . The simulated SE is: 2.83 .\n\n# with replicate\nmeans <- \n  replicate(\n    draws,\n    rnorm(n, mu, sd)\n  )\n\nsd(colMeans(means))\n\n[1] 2.827195\n\n\n\n\n0.2.5 Exercise\nSame population. Draw 1,000 observations for each sample size between 20 and 100. Calculate the standard error for each sample size (like you did above) and plot it against the sample size. (Tip: You’ll need to iterate over two things.)\n\nmu <- 168\nsd <- 20\ndraws <- 1e3\nmin_sample <- 20\nmax_sample <- 100\n\nresults <- \n  data.frame(\n    sample_size = NULL,\n    se = NULL\n  )\n\nfor (i in min_sample:max_sample) {\n  \n  sample_means <- NULL\n  \n  for (j in 1:draws) {\n    sample_means[j] <- mean(rnorm(i, mu, sd))\n  }\n  \n  se <- sd(sample_means)\n  \n  results <- rbind(\n    results,\n    data.frame(\n      sample_size = i,\n      se = se\n    )\n  )\n}\n\nplot(results$sample_size, results$se)\n\n\n\n\n\n\n0.2.6 Exercise\nTurn the above into a function so that you can change the population effect size, SD, number of simulations, and sample size range. The function should also return the plot from above.\n\nsample_against_se <- \n  function(\n    mu = 168,\n    sd = 20,\n    draws = 1e3,\n    min_sample = 20,\n    max_sample = 100\n  ) {\n    results <- \n      data.frame(\n        sample_size = NULL,\n        se = NULL\n      )\n    \n    for (i in min_sample:max_sample) {\n      \n      sample_means <- NULL\n      \n      for (j in 1:draws) {\n        sample_means[j] <- mean(rnorm(i, mu, sd))\n      }\n      \n      se <- sd(sample_means)\n      \n      results <- rbind(\n        results,\n        data.frame(\n          sample_size = i,\n          se = se\n        )\n      )\n    }\n    \n    return(plot(results$sample_size, results$se))\n  }\n\n\n\n0.2.7 Exercise\nTry out the function with two plots: when the population SD is 5 and when you do 10 draws. What changes?\n\nsample_against_se(sd = 5)\n\n\n\nsample_against_se(draws = 10)\n\n\n\n\n\n\n0.2.8 Exercise\nThe average height of men in Spain is 173cm (says Wikipedia). The population standard deviation is probablay around 7cm (source).\nYou draw a sample of men and want to test whether they’re significantly different from that mean (our H0). In fact, these men you have sampled are truly French (175.6cm, our true “effect size”). In other words, can we reject the null hypothesis that these men we sampled come from the Spanish distribution in favor of our alternative hypothesis that the true population value is greater than the Spanish population mean?\nYou calculate the z-statistic, which is calculated as follows: \\(\\frac{\\bar{X} - \\mu_0}{\\sigma/\\sqrt{N}}\\) This simply tells us how far from the population mean (well, the suspected population mean under the null hypothesis) our sample mean is in terms of standard errors. \\(\\bar{X}\\) is the sample mean, \\(\\mu_0\\) is the population mean under H0, \\(\\sigma\\) is the population standard deviation, and \\(N\\) is the sample size.\nThen we can look up the z-score to see what the probability is to score this high or higher (does that definition ring a bell?). In R, you can simply do that with a built-in function: pnorm(). For example, if we have a z-score of 1.645, our probability of obtaining such a value (or higher) is pnorm(1.645, lower.tail = FALSE) = 0.0499849 – our p-value for a one-sided test.\nWe can simulate the power of our statistical test (in this case, the z-statistic). Take a sample of 30 people from the French population, calculate the z-statistic, its p-value, and store the p-value. Do this 1,000 times. Plot the distribution of p-values. What can we conclude about the sample size?\n\n# sample size\nn <- 30\nh0 <- 173 # test against this null\nh1 <- 175.6 # true population  effect\nsd <- 7\nalpha <- 0.05\ndraws <- 1e3\n\npvalues <- NULL\n\nfor (i in 1:draws) {\n  d <- rnorm(n, h1, sd)\n  z <- (mean(d) - h0) / (sd / sqrt(n))\n  pvalues[i] <- pnorm(z, lower.tail = FALSE)\n}\n\nplot(density(pvalues))\n\n\n\n\n\n\n0.2.9 Exercise\nNow calculate the proportion of p-values that are below 0.05. That’s your power: The proportion of tests that will detect that there’s a true effect. In our case, that effect is a difference of 2.6cm.\n\nsum(pvalues < 0.05)/length(pvalues)\n\n[1] 0.668\n\n\n\n\n0.2.10 Exercise\nNow let’s do what we did before: Put the loop from above inside another loop that iterates over different sample sizes. Then put that all into a function that let’s you set the parameters of interest (sample size range, h0, h1, etc.). Then simulate power (1,000 simulations each) for samples between 30 and 100. Plot the sample size against power.\n\npower_function <- \n  function(\n    h0 = 173,\n    h1 = 175.6,\n    sd = 7,\n    alpha = 0.05,\n    draws = 1e3,\n    min_sample = 30,\n    max_sample = 100\n  ) {\n    results <- \n      data.frame(\n        sample_size = NULL,\n        power = NULL\n      )\n    \n    for (i in min_sample:max_sample) {\n      \n      pvalues <- NULL\n      \n      for (j in 1:draws) {\n        d <- rnorm(i, h1, sd)\n        z <- (mean(d) - h0) / (sd / sqrt(i))\n        pvalues[j] <- pnorm(z, lower.tail = FALSE)\n      }\n      \n      results <- rbind(\n        results,\n        data.frame(\n          sample_size = i,\n          power = sum(pvalues < 0.05)/length(pvalues)\n        )\n      )\n    }\n    \n    return(plot(results$sample_size, results$power, type = \"l\"))\n  }\n\npower_function()\n\n\n\n\n\n\n0.2.11 Exercise\nNow do the same thing with a one-sample t-test. (Tip: You only need to replace the z-scores with a t.test from which you can extract the p-value). (Another tip: Use the $ sign on where you stored the t-test results.)\n\npower_function_t <- \n  function(\n    h0 = 173,\n    h1 = 175.6,\n    sd = 7,\n    alpha = 0.05,\n    draws = 1e3,\n    min_sample = 30,\n    max_sample = 100\n  ) {\n    results <- \n      data.frame(\n        sample_size = NULL,\n        power = NULL\n      )\n    \n    for (i in min_sample:max_sample) {\n      \n      pvalues <- NULL\n      \n      for (j in 1:draws) {\n        d <- rnorm(i, h1, sd)\n        t <- t.test(d, mu = h0,alternative = \"greater\")\n        pvalues[j] <- t$p.value\n      }\n      \n      results <- rbind(\n        results,\n        data.frame(\n          sample_size = i,\n          power = sum(pvalues < 0.05)/length(pvalues)\n        )\n      )\n    }\n    \n    return(plot(results$sample_size, results$power, type = \"l\"))\n  }\n\npower_function()\n\n\n\n\n\n\n0.2.12 Exercise\nJust for funsies (and for our next session), see what happens when the true effect is only 1cm in difference.\n\npower_function_t(h1 = h0+1)"
  },
  {
    "objectID": "content/03-simulations-in-r/03-slides.html",
    "href": "content/03-simulations-in-r/03-slides.html",
    "title": "Slides",
    "section": "",
    "text": "Below you can find the slides for the intro to simulations:\n\n\n\nIf you want to see the slides in full screen, you can click here. To download them as PDFs, hit the ‘e’ button when you got the presentation open and then Ctrl+P print to PDF. This will work best in Chrome (in Firefox, it didn’t print the headings for me)."
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#why-should-i-simulate",
    "href": "content/03-simulations-in-r/slides/index.html#why-should-i-simulate",
    "title": "Simulations in R",
    "section": "Why should I simulate",
    "text": "Why should I simulate\n\nCooking vs. eating\nMakes you truly understand what you’re doing\nForces you to put a number on things\nSimulate the data generating process (we’ll get back to that)"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#going-through-the-motions",
    "href": "content/03-simulations-in-r/slides/index.html#going-through-the-motions",
    "title": "Simulations in R",
    "section": "Going through the motions",
    "text": "Going through the motions\n\nYou’ll often find that you don’t know nearly enough for a prediction–or even a study\nIf we can’t generate the pattern we’re interested in how can we explain a pattern?\nI take a proper description over a flawed confirmation"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#technicalities",
    "href": "content/03-simulations-in-r/slides/index.html#technicalities",
    "title": "Simulations in R",
    "section": "Technicalities",
    "text": "Technicalities\nAbove all, you want to be able to reproduce your analysis–much later, on a different computer, etc. (Trisovic et al. 2022)"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#computational-reproducibility",
    "href": "content/03-simulations-in-r/slides/index.html#computational-reproducibility",
    "title": "Simulations in R",
    "section": "Computational reproducibility",
    "text": "Computational reproducibility\n\nEnvironment\nPackages for project\nPackages for script\nVersion control"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#for-now",
    "href": "content/03-simulations-in-r/slides/index.html#for-now",
    "title": "Simulations in R",
    "section": "For now",
    "text": "For now\n\nset.seed(): Reproduce random numbers within same script\nsessionInfo(): Prints computational environment\nRelative paths (here package)\nExplicit caching or markdown"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#monte-carlo-simulations",
    "href": "content/03-simulations-in-r/slides/index.html#monte-carlo-simulations",
    "title": "Simulations in R",
    "section": "Monte Carlo simulations",
    "text": "Monte Carlo simulations\n\nMonte Carlo methods, or Monte Carlo experiments, are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. The underlying concept is to use randomness to solve problems that might be deterministic in principle. Wikipedia\n\n\n\n\n\n\n\n\nSource"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#in-plain-english",
    "href": "content/03-simulations-in-r/slides/index.html#in-plain-english",
    "title": "Simulations in R",
    "section": "In plain English",
    "text": "In plain English\n\nDefine relevant outcome, the process leading to outcome, and potential inputs that go into the process\nThen decide what inputs you want to vary\nRun the process many, many times, with different inputs, summarize and plot the outcomes"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#a-simple-example",
    "href": "content/03-simulations-in-r/slides/index.html#a-simple-example",
    "title": "Simulations in R",
    "section": "A simple example",
    "text": "A simple example\nTotally unrelated to why you are here:\n\nOutcome: How much power do I have?\nProcess: The statistical test I plan to do\nInputs: Effect size, sample size, \\(\\alpha\\)"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#some-basic-commands-first",
    "href": "content/03-simulations-in-r/slides/index.html#some-basic-commands-first",
    "title": "Simulations in R",
    "section": "Some basic commands first",
    "text": "Some basic commands first\nSampling a certain number of elements from a set.\n\nmy_sample <- 1:20\n\nsample(\n  x = my_sample,\n  size = 2\n)\n\n[1] 12  2"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#sampling",
    "href": "content/03-simulations-in-r/slides/index.html#sampling",
    "title": "Simulations in R",
    "section": "Sampling",
    "text": "Sampling\nWith or without replacement?\n\nmy_sample <- c(\"a\", \"b\", \"c\")\n\nsample(\n  x = my_sample,\n  size = 4,\n  replace = TRUE\n)\n\n[1] \"b\" \"b\" \"c\" \"c\""
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#on-those-letters",
    "href": "content/03-simulations-in-r/slides/index.html#on-those-letters",
    "title": "Simulations in R",
    "section": "On those letters",
    "text": "On those letters\nR has some neat built-in stuff.\n\nletters[5]\n\n[1] \"e\"\n\nletters[7:10]\n\n[1] \"g\" \"h\" \"i\" \"j\"\n\nLETTERS[20:26]\n\n[1] \"T\" \"U\" \"V\" \"W\" \"X\" \"Y\" \"Z\""
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#sampling-1",
    "href": "content/03-simulations-in-r/slides/index.html#sampling-1",
    "title": "Simulations in R",
    "section": "Sampling",
    "text": "Sampling\nUsing those letters.\n\nmy_sample <- letters[1:3]\n\nsample(\n  x = my_sample,\n  size = 4,\n  replace = TRUE\n)\n\n[1] \"a\" \"a\" \"b\" \"b\""
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#sampling-2",
    "href": "content/03-simulations-in-r/slides/index.html#sampling-2",
    "title": "Simulations in R",
    "section": "Sampling",
    "text": "Sampling\nAssigning different probabilities.\n\nmy_sample <- c(\"black\", \"blue\")\n\nsample(\n  x = my_sample,\n  size = 10,\n  replace = TRUE,\n  prob = c(0.2, 0.8)\n)\n\n [1] \"blue\" \"blue\" \"blue\" \"blue\" \"blue\" \"blue\" \"blue\" \"blue\" \"blue\" \"blue\""
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#remember-seeds",
    "href": "content/03-simulations-in-r/slides/index.html#remember-seeds",
    "title": "Simulations in R",
    "section": "Remember seeds?",
    "text": "Remember seeds?\n\nmy_sample <- c(\"black\", \"blue\")\n\nset.seed(42)\nsample(\n  x = my_sample,\n  size = 10,\n  replace = TRUE,\n  prob = c(0.2, 0.8)\n)\n\n [1] \"black\" \"black\" \"blue\"  \"black\" \"blue\"  \"blue\"  \"blue\"  \"blue\"  \"blue\" \n[10] \"blue\" \n\nset.seed(42)\nsample(\n  x = my_sample,\n  size = 10,\n  replace = TRUE,\n  prob = c(0.2, 0.8)\n)\n\n [1] \"black\" \"black\" \"blue\"  \"black\" \"blue\"  \"blue\"  \"blue\"  \"blue\"  \"blue\" \n[10] \"blue\""
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#how-do-we-generate-randomness",
    "href": "content/03-simulations-in-r/slides/index.html#how-do-we-generate-randomness",
    "title": "Simulations in R",
    "section": "How do we generate randomness?",
    "text": "How do we generate randomness?\nBuilt-in R functions that create a random number following a process with a given probability distribution.\n\nrnorm: Normal distribution\nrbinom: Binomial distribution\nrpois: Poissong distribution\nrunif: Uniform distribution"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#normal-distribution",
    "href": "content/03-simulations-in-r/slides/index.html#normal-distribution",
    "title": "Simulations in R",
    "section": "Normal distribution",
    "text": "Normal distribution\nDraw n numbers from a normal distribution.\n\nrnorm(\n  n = 5,\n  mean = 0,\n  sd = 1\n)\n\n[1] -0.10612452  1.51152200 -0.09465904  2.01842371 -0.06271410"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#once-more-seed",
    "href": "content/03-simulations-in-r/slides/index.html#once-more-seed",
    "title": "Simulations in R",
    "section": "Once more: seed",
    "text": "Once more: seed\n\nrnorm(1)\n\n[1] 1.30487\n\nrnorm(1)\n\n[1] 2.286645\n\nset.seed(42)\nrnorm(1)\n\n[1] 1.370958\n\nset.seed(42)\nrnorm(1)\n\n[1] 1.370958"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#r-uses-vectors",
    "href": "content/03-simulations-in-r/slides/index.html#r-uses-vectors",
    "title": "Simulations in R",
    "section": "R uses vectors",
    "text": "R uses vectors\nGives us 4 draws total: 1 draw from a normal distribution with mean = 0 and sd = 1, 1 draw from a normal distribution with mean = 10 and sd = 50, and so on.\n\nrnorm(\n  n = 4,\n  mean = c(0, 10, 20, 30),\n  sd = c(1, 50, 100, 150)\n)\n\n[1] -0.5646982 28.1564206 83.2862605 90.6402485"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for",
    "href": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for",
    "title": "Simulations in R",
    "section": "What would I use this for?",
    "text": "What would I use this for?\nSimulating different groups.\n\ncontrol <- \n  rnorm(\n    n = 100,\n    mean = 100,\n    sd = 15\n  )\n\ntreatment <- \n  rnorm(\n    n = 100,\n    mean = 150,\n    sd = 15\n  )"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-1",
    "href": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-1",
    "title": "Simulations in R",
    "section": "What would I use this for?",
    "text": "What would I use this for?\nSimulating different groups.\n\nboxplot(control, treatment)"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-2",
    "href": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-2",
    "title": "Simulations in R",
    "section": "What would I use this for?",
    "text": "What would I use this for?\nSimulating a correlation.\n\ncontrol <- \n  rnorm(\n    n = 1000,\n    mean = 100,\n    sd = 15\n  )\n\ntreatment <- \n  control*0.5 + rnorm(\n    n = 1000,\n    mean = 100,\n    sd = 15\n  )"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-3",
    "href": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-3",
    "title": "Simulations in R",
    "section": "What would I use this for?",
    "text": "What would I use this for?\n\nsummary(lm(treatment~control))\n\n\nCall:\nlm(formula = treatment ~ control)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-42.531  -9.904  -0.444  10.315  53.874 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 100.71554    3.15133   31.96   <2e-16 ***\ncontrol       0.49205    0.03129   15.72   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 14.86 on 998 degrees of freedom\nMultiple R-squared:  0.1985,    Adjusted R-squared:  0.1977 \nF-statistic: 247.2 on 1 and 998 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#teaser-for-later",
    "href": "content/03-simulations-in-r/slides/index.html#teaser-for-later",
    "title": "Simulations in R",
    "section": "Teaser for later",
    "text": "Teaser for later\nSame data, different conventions."
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#uniform-distribution",
    "href": "content/03-simulations-in-r/slides/index.html#uniform-distribution",
    "title": "Simulations in R",
    "section": "Uniform distribution",
    "text": "Uniform distribution\nDraw n numbers from a uniform distribution.\n\nrunif(\n  n = 5,\n  min = 0,\n  max = 1\n)\n\n[1] 0.06866080 0.05036072 0.98707779 0.63460261 0.94782652"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#lets-inspect-that",
    "href": "content/03-simulations-in-r/slides/index.html#lets-inspect-that",
    "title": "Simulations in R",
    "section": "Let’s inspect that",
    "text": "Let’s inspect that\n\nhist(runif(1000))"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-4",
    "href": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-4",
    "title": "Simulations in R",
    "section": "What would I use this for?",
    "text": "What would I use this for?\nKeeping a range on predictor variables.\n\nage <- runif(n = 1000, min = 18, max = 100)\n\nhist(age, breaks = 30, xlim = c(18, 100))"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-5",
    "href": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-5",
    "title": "Simulations in R",
    "section": "What would I use this for?",
    "text": "What would I use this for?\nKeeping a range on predictor variables.\n\nage <- runif(n = 1000, min = 18, max = 100)\ny <- age*0.5 + rnorm(1000)\n\nsummary(lm(y~age))\n\n\nCall:\nlm(formula = y ~ age)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.0665 -0.6815 -0.0112  0.6320  3.4804 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.026915   0.082359  -0.327    0.744    \nage          0.499389   0.001297 384.986   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9858 on 998 degrees of freedom\nMultiple R-squared:  0.9933,    Adjusted R-squared:  0.9933 \nF-statistic: 1.482e+05 on 1 and 998 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#binomial-distribution",
    "href": "content/03-simulations-in-r/slides/index.html#binomial-distribution",
    "title": "Simulations in R",
    "section": "Binomial distribution",
    "text": "Binomial distribution\nLet’s flip a coin 100 times. That’s one “experiment”. How often will I get heads?\n\nrbinom(\n  n = 1,\n  size = 100,\n  prob = 0.50\n)\n\n[1] 43"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#many-experiments",
    "href": "content/03-simulations-in-r/slides/index.html#many-experiments",
    "title": "Simulations in R",
    "section": "Many experiments",
    "text": "Many experiments\nLet’s run 1000 experiments where we flip a coin 100 times each.\n\nexperiments <- \n  rbinom(\n    n = 1000,\n    size = 100,\n    prob = 0.50\n  )\n\nhead(experiments)\n\n[1] 46 56 49 48 53 58"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#always-inspect-and-summarize",
    "href": "content/03-simulations-in-r/slides/index.html#always-inspect-and-summarize",
    "title": "Simulations in R",
    "section": "Always inspect and summarize",
    "text": "Always inspect and summarize\nMakes sense.\n\nhist(experiments, breaks = 20)"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-6",
    "href": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-6",
    "title": "Simulations in R",
    "section": "What would I use this for?",
    "text": "What would I use this for?\nCompare groups on their probabilities.\n\ncontrol <- \n  rbinom(\n    n = 100,\n    size = 10,\n    prob = 0.5\n  )\n\ntreatment <- \n  rbinom(\n    n = 100,\n    size = 10,\n    prob = 0.9\n  )"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-7",
    "href": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-7",
    "title": "Simulations in R",
    "section": "What would I use this for?",
    "text": "What would I use this for?\nCompare groups on their probabilities.\n\nboxplot(control, treatment)"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-8",
    "href": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-8",
    "title": "Simulations in R",
    "section": "What would I use this for?",
    "text": "What would I use this for?\nBernoulli trials.\n\nrbinom(\n  n = 10,\n  size = 1,\n  prob = 0.5\n)\n\n [1] 1 0 0 1 0 0 1 0 1 0"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-9",
    "href": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-9",
    "title": "Simulations in R",
    "section": "What would I use this for?",
    "text": "What would I use this for?\nBernoulli trials and logistic regression: Does age predict our binary outcome? (Explanation here.)\n\nage <- rnorm(n = 10000, mean = 50, sd = 15)\n\nxb <- 1 + 0.2*age\n\np <- 1/(1 + exp(-xb))\n\ny <- \n  rbinom(\n    n = 10000,\n    size = 1,\n    prob = p\n  )"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-10",
    "href": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-10",
    "title": "Simulations in R",
    "section": "What would I use this for?",
    "text": "What would I use this for?\n\nm <- \n  glm(\n    y ~ age,\n    family = binomial()\n  )\n\nsummary(m)\n\n\nCall:\nglm(formula = y ~ age, family = binomial())\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.6531   0.0002   0.0011   0.0047   1.0001  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  0.13900    0.56322   0.247    0.805    \nage          0.28512    0.04927   5.787 7.18e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 171.86  on 9999  degrees of freedom\nResidual deviance:  66.03  on 9998  degrees of freedom\nAIC: 70.03\n\nNumber of Fisher Scoring iterations: 13"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#poisson-distribution",
    "href": "content/03-simulations-in-r/slides/index.html#poisson-distribution",
    "title": "Simulations in R",
    "section": "Poisson distribution",
    "text": "Poisson distribution\nLet’s see how many emails we get in an hour. We check for 10 different hours.\n\nrpois(\n  n = 10,\n  lambda = 5\n)\n\n [1] 3 4 5 2 8 5 7 5 5 5"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#lets-visualize",
    "href": "content/03-simulations-in-r/slides/index.html#lets-visualize",
    "title": "Simulations in R",
    "section": "Let’s visualize",
    "text": "Let’s visualize\n\nemails <- \n  rpois(\n    n = 1000,\n    lambda = 5\n  )\n\nhist(emails)"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#why-would-you-use-this",
    "href": "content/03-simulations-in-r/slides/index.html#why-would-you-use-this",
    "title": "Simulations in R",
    "section": "Why would you use this?",
    "text": "Why would you use this?\nCompare two groups on how many emails they get.\n\nhr <- rpois(n = 1000, lambda = 5)\nit <- rpois(n = 1000, lambda = 10)\n\n\n\n\nhist(hr)\n\n\n\n\n\n\nhist(it)"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-11",
    "href": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-11",
    "title": "Simulations in R",
    "section": "What would I use this for?",
    "text": "What would I use this for?\nPoisson regression. (More on rep in a moment.)\n\nd <- \n  data.frame(\n    emails = c(hr, it),\n    group = factor(rep(c(\"HR\", \"IT\"), each = 1000))\n  )\n\nd[c(1:5, 1001:1005),]\n\n     emails group\n1         5    HR\n2         7    HR\n3         4    HR\n4         8    HR\n5         2    HR\n1001     10    IT\n1002      9    IT\n1003      5    IT\n1004      6    IT\n1005      7    IT"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-12",
    "href": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-12",
    "title": "Simulations in R",
    "section": "What would I use this for?",
    "text": "What would I use this for?\n\nm <- \n  glm(\n    emails ~ group,\n    data = d,\n    family = poisson()\n  )\n\nsummary(m)\n\n\nCall:\nglm(formula = emails ~ group, family = poisson(), data = d)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.6837  -0.6850  -0.0559   0.5814   2.9916  \n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  1.63433    0.01397  117.01   <2e-16 ***\ngroupIT      0.67791    0.01715   39.53   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 3652.0  on 1999  degrees of freedom\nResidual deviance: 1998.5  on 1998  degrees of freedom\nAIC: 9517.7\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#making-data-sets-with-rep",
    "href": "content/03-simulations-in-r/slides/index.html#making-data-sets-with-rep",
    "title": "Simulations in R",
    "section": "Making data sets with rep",
    "text": "Making data sets with rep\nReplicates numbers and characters.\n\nrep(\n  x = \"Control Group\",\n  times = 3\n  )\n\n[1] \"Control Group\" \"Control Group\" \"Control Group\"\n\nrep(\n  x = 1,\n  times = 3\n  )\n\n[1] 1 1 1"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#making-data-sets-with-rep-1",
    "href": "content/03-simulations-in-r/slides/index.html#making-data-sets-with-rep-1",
    "title": "Simulations in R",
    "section": "Making data sets with rep",
    "text": "Making data sets with rep\nReplicates an entire vector several times.\n\nrep(\n  x = c(\"Control Group\", \"Treatment Group\"),\n  times = 3\n  )\n\n[1] \"Control Group\"   \"Treatment Group\" \"Control Group\"   \"Treatment Group\"\n[5] \"Control Group\"   \"Treatment Group\""
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#making-data-sets-with-rep-2",
    "href": "content/03-simulations-in-r/slides/index.html#making-data-sets-with-rep-2",
    "title": "Simulations in R",
    "section": "Making data sets with rep",
    "text": "Making data sets with rep\nReplicate different elements different times.\n\nrep(\n  x = c(\"Control Group\", \"Treatment Group\"),\n  times = c(1,2)\n  )\n\n[1] \"Control Group\"   \"Treatment Group\" \"Treatment Group\""
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#making-data-sets-with-rep-3",
    "href": "content/03-simulations-in-r/slides/index.html#making-data-sets-with-rep-3",
    "title": "Simulations in R",
    "section": "Making data sets with rep",
    "text": "Making data sets with rep\nReplicates an entire vector several times.\n\nrep(\n  x = c(\"Control Group\", \"Treatment Group\"),\n  each = 3\n  )\n\n[1] \"Control Group\"   \"Control Group\"   \"Control Group\"   \"Treatment Group\"\n[5] \"Treatment Group\" \"Treatment Group\""
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#making-data-sets-with-rep-4",
    "href": "content/03-simulations-in-r/slides/index.html#making-data-sets-with-rep-4",
    "title": "Simulations in R",
    "section": "Making data sets with rep",
    "text": "Making data sets with rep\nControlling the length of our output.\n\nrep(\n  x = c(\"Control Group\", \"Treatment Group\"),\n  each = 3,\n  length.out = 4\n  )\n\n[1] \"Control Group\"   \"Control Group\"   \"Control Group\"   \"Treatment Group\""
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#making-data-sets-with-rep-5",
    "href": "content/03-simulations-in-r/slides/index.html#making-data-sets-with-rep-5",
    "title": "Simulations in R",
    "section": "Making data sets with rep",
    "text": "Making data sets with rep\nCombining times with each.\n\nrep(\n  x = c(\"Control Group\", \"Treatment Group\"),\n  each = 3,\n  times = 2\n  )\n\n [1] \"Control Group\"   \"Control Group\"   \"Control Group\"   \"Treatment Group\"\n [5] \"Treatment Group\" \"Treatment Group\" \"Control Group\"   \"Control Group\"  \n [9] \"Control Group\"   \"Treatment Group\" \"Treatment Group\" \"Treatment Group\""
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-13",
    "href": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-13",
    "title": "Simulations in R",
    "section": "What would I use this for?",
    "text": "What would I use this for?\nLet’s create a group.\n\ncondition <- rep(\"control\", times = 4)\noutcome <- rnorm(4)\nd <- data.frame(condition, outcome)\nd\n\n  condition     outcome\n1   control -0.04723942\n2   control -0.12307634\n3   control  1.11130209\n4   control  1.07619064"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-14",
    "href": "content/03-simulations-in-r/slides/index.html#what-would-i-use-this-for-14",
    "title": "Simulations in R",
    "section": "What would I use this for?",
    "text": "What would I use this for?\nLet’s create two groups.\n\ncondition <- rep(c(\"control\", \"treatment\"), each = 2)\noutcome <- rnorm(4)\nd <- data.frame(condition, outcome)\nd\n\n  condition       outcome\n1   control  0.0313900684\n2   control  0.0001993327\n3 treatment -0.7676236790\n4 treatment  0.8964049160"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#different-groups",
    "href": "content/03-simulations-in-r/slides/index.html#different-groups",
    "title": "Simulations in R",
    "section": "Different groups",
    "text": "Different groups\nTwo groups from different distributions.\n\ncontrol_outcome <- rnorm(5, 100, 15)\ntreatment_outcome <- rnorm(5, 115, 15)\n\nd <- data.frame(outcome = c(control_outcome, treatment_outcome))"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#different-groups-1",
    "href": "content/03-simulations-in-r/slides/index.html#different-groups-1",
    "title": "Simulations in R",
    "section": "Different groups",
    "text": "Different groups\nNow we add group membership.\n\ncontrol_outcome <- rnorm(5, 100, 15)\ntreatment_outcome <- rnorm(5, 115, 15)\ncondition <- rep(c(\"control\", \"treatment\"), each = 5)\n\n\nd <- data.frame(\n  condition = condition,\n  outcome = c(control_outcome, treatment_outcome)\n)\nd\n\n   condition   outcome\n1    control 102.60947\n2    control 105.91958\n3    control 140.17591\n4    control  89.69311\n5    control  94.72799\n6  treatment 122.12078\n7  treatment 120.31591\n8  treatment 118.34136\n9  treatment  91.22117\n10 treatment  94.38544"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#remember-vectors",
    "href": "content/03-simulations-in-r/slides/index.html#remember-vectors",
    "title": "Simulations in R",
    "section": "Remember vectors?",
    "text": "Remember vectors?\nGets us there faster.\n\nd <- \n  data.frame(\n    condition = rep(c(\"control\", \"treatment\"), times = 5),\n    outcome = rnorm(10, mean = c(100, 115), sd = 15)\n  )\nd\n\n   condition   outcome\n1    control  88.88867\n2  treatment 118.51307\n3    control  98.56411\n4  treatment  99.00802\n5    control  75.87184\n6  treatment 129.97625\n7    control  91.31636\n8  treatment 120.37524\n9    control  94.46493\n10 treatment 125.81974"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#now-how-do-we-get-to-monte-carlo",
    "href": "content/03-simulations-in-r/slides/index.html#now-how-do-we-get-to-monte-carlo",
    "title": "Simulations in R",
    "section": "Now how do we get to Monte Carlo?",
    "text": "Now how do we get to Monte Carlo?\nCreating one “data set” isn’t enough. We need many more.\n\nreplicate(\n  n = 5,\n  expr = sample(1:4, size = 2),\n  simplify = FALSE\n)\n\n[[1]]\n[1] 1 3\n\n[[2]]\n[1] 3 4\n\n[[3]]\n[1] 1 3\n\n[[4]]\n[1] 2 3\n\n[[5]]\n[1] 3 2"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#replicating-variables",
    "href": "content/03-simulations-in-r/slides/index.html#replicating-variables",
    "title": "Simulations in R",
    "section": "Replicating variables",
    "text": "Replicating variables\nCreating one “data set” isn’t enough. We need many more.\n\nreplicate(\n  n = 5,\n  expr = rnorm(5),\n  simplify = FALSE\n)\n\n[[1]]\n[1] -1.24072975 -0.13173024  1.39906593  0.08828747  0.40500180\n\n[[2]]\n[1]  0.3126241 -0.1319878  0.4697315  0.4992376 -0.6715315\n\n[[3]]\n[1] -0.2262588 -0.1617954 -1.3745364 -1.1833134  0.8182783\n\n[[4]]\n[1]  1.2439504  1.5819136  0.6209595 -1.4514988  1.3820336\n\n[[5]]\n[1] -0.3496984  0.5724784  2.2198495 -1.2908316 -1.5562072"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#replicating-data-sets",
    "href": "content/03-simulations-in-r/slides/index.html#replicating-data-sets",
    "title": "Simulations in R",
    "section": "Replicating data sets",
    "text": "Replicating data sets\nCreating one “data set” isn’t enough. We need more.\n\nreplicate(\n  n = 5,\n  expr = data.frame(\n    condition = rep(c(\"control\", \"treatment\"), each = 2),\n    outcome = rnorm(4, c(100, 115), sd = 15)\n  ),\n  simplify = FALSE\n)\n\n[[1]]\n  condition   outcome\n1   control  86.38442\n2   control 144.21596\n3 treatment 100.04792\n4 treatment 112.80507\n\n[[2]]\n  condition  outcome\n1   control 113.5496\n2   control 114.7502\n3 treatment 101.7030\n4 treatment 127.2264\n\n[[3]]\n  condition   outcome\n1   control 128.53812\n2   control 119.93447\n3 treatment  99.95619\n4 treatment 102.77010\n\n[[4]]\n  condition   outcome\n1   control  92.93538\n2   control 112.54882\n3 treatment  99.87659\n4 treatment 120.94084\n\n[[5]]\n  condition   outcome\n1   control  60.82896\n2   control 110.58693\n3 treatment 119.18822\n4 treatment 126.65385"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#for-loops",
    "href": "content/03-simulations-in-r/slides/index.html#for-loops",
    "title": "Simulations in R",
    "section": "for loops",
    "text": "for loops\nFor each element in a vector, do the following:\n\nfor (variable in vector) {\n  \n}\n\n\nfor (i in 1:10) {\n  print(i)\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#same-as-replicate",
    "href": "content/03-simulations-in-r/slides/index.html#same-as-replicate",
    "title": "Simulations in R",
    "section": "Same as replicate",
    "text": "Same as replicate\nFive times we sample and store it in a list. Equivalent to replicate example.\n\noutcome <- NULL\n\nfor (i in 1:5) {\n  outcome[[i]] <- sample(1:4, size = 2)\n}\n\noutcome\n\n[[1]]\n[1] 3 2\n\n[[2]]\n[1] 1 2\n\n[[3]]\n[1] 1 2\n\n[[4]]\n[1] 4 3\n\n[[5]]\n[1] 1 4"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#get-our-data-sets",
    "href": "content/03-simulations-in-r/slides/index.html#get-our-data-sets",
    "title": "Simulations in R",
    "section": "Get our data sets",
    "text": "Get our data sets\n\ndatasets <- NULL\n\nfor (i in 1:5) {\n  datasets[[i]] <- \n    data.frame(\n      condition = rep(c(\"control\", \"treatment\"), each = 2),\n      outcome = rnorm(4, c(100, 115), sd = 15)\n    )\n}\n\ndatasets\n\n[[1]]\n  condition   outcome\n1   control  97.99421\n2   control 111.00762\n3 treatment  99.06100\n4 treatment  81.91246\n\n[[2]]\n  condition  outcome\n1   control 113.0777\n2   control 126.1959\n3 treatment 102.3874\n4 treatment 104.5351\n\n[[3]]\n  condition   outcome\n1   control 123.36611\n2   control 134.27978\n3 treatment  70.14792\n4 treatment 112.37140\n\n[[4]]\n  condition   outcome\n1   control 103.39652\n2   control 119.44954\n3 treatment  81.99905\n4 treatment  97.97151\n\n[[5]]\n  condition  outcome\n1   control 112.8112\n2   control 119.5977\n3 treatment 112.1696\n4 treatment 113.9696"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#a-concrete-example",
    "href": "content/03-simulations-in-r/slides/index.html#a-concrete-example",
    "title": "Simulations in R",
    "section": "A concrete example",
    "text": "A concrete example\nLet’s model the growth of our department. This year, we have 12 PhD students. Each year, our 4 professors write one grant application each. If they get the money, they’ll hire one new PhD student. Their chance of getting the money is 15%. But academia also sucks sometimes, so each PhD student each year has a 5% chance of quitting and finally doing something with their lives. How large is the department after 25 years?"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#lets-put-that-into-numbers",
    "href": "content/03-simulations-in-r/slides/index.html#lets-put-that-into-numbers",
    "title": "Simulations in R",
    "section": "Let’s put that into numbers",
    "text": "Let’s put that into numbers\n\nstarting <- 12\nprofs <- 4\nmoney <- 0.15\nquitting <- 0.05\nyears <- 25\nresults <- NULL\nresults[1] <- starting"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#into-a-loop",
    "href": "content/03-simulations-in-r/slides/index.html#into-a-loop",
    "title": "Simulations in R",
    "section": "Into a loop",
    "text": "Into a loop\n\nset.seed(42)\nfor (current_year in 2:years) {\n  # how many new phds (15% chance in 4 \"trials\")\n  newbies <- rbinom(n = 1, size = profs, prob = money)\n  \n  # how many see the light and quit\n  enlightened <- rbinom(n = 1, size = results[current_year  - 1], prob = quitting)\n  \n  # new total\n  results[current_year] <- results[current_year - 1] + newbies - enlightened\n  \n  cat(\"Current year:\", current_year, \"Newbies:\", newbies, \"Enlightened:\", enlightened, \"Total:\", results[current_year], \"\\n\")\n}\n\nCurrent year: 2 Newbies: 2 Enlightened: 2 Total: 12 \nCurrent year: 3 Newbies: 0 Enlightened: 1 Total: 11 \nCurrent year: 4 Newbies: 1 Enlightened: 0 Total: 12 \nCurrent year: 5 Newbies: 1 Enlightened: 0 Total: 13 \nCurrent year: 6 Newbies: 1 Enlightened: 1 Total: 13 \nCurrent year: 7 Newbies: 0 Enlightened: 1 Total: 12 \nCurrent year: 8 Newbies: 2 Enlightened: 0 Total: 14 \nCurrent year: 9 Newbies: 0 Enlightened: 2 Total: 12 \nCurrent year: 10 Newbies: 2 Enlightened: 0 Total: 14 \nCurrent year: 11 Newbies: 0 Enlightened: 1 Total: 13 \nCurrent year: 12 Newbies: 2 Enlightened: 0 Total: 15 \nCurrent year: 13 Newbies: 3 Enlightened: 2 Total: 16 \nCurrent year: 14 Newbies: 0 Enlightened: 1 Total: 15 \nCurrent year: 15 Newbies: 0 Enlightened: 2 Total: 13 \nCurrent year: 16 Newbies: 0 Enlightened: 1 Total: 12 \nCurrent year: 17 Newbies: 1 Enlightened: 1 Total: 12 \nCurrent year: 18 Newbies: 0 Enlightened: 1 Total: 11 \nCurrent year: 19 Newbies: 0 Enlightened: 1 Total: 10 \nCurrent year: 20 Newbies: 0 Enlightened: 0 Total: 10 \nCurrent year: 21 Newbies: 2 Enlightened: 1 Total: 11 \nCurrent year: 22 Newbies: 0 Enlightened: 0 Total: 11 \nCurrent year: 23 Newbies: 0 Enlightened: 2 Total: 9 \nCurrent year: 24 Newbies: 0 Enlightened: 2 Total: 7 \nCurrent year: 25 Newbies: 1 Enlightened: 0 Total: 8"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#plot-and-summarize-the-results",
    "href": "content/03-simulations-in-r/slides/index.html#plot-and-summarize-the-results",
    "title": "Simulations in R",
    "section": "Plot and summarize the results",
    "text": "Plot and summarize the results\n\nmean(results)\n\n[1] 11.92\n\nplot(results, type = \"l\", xlab = \"Year\", ylab = \"Number of PhDs\")"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#lets-repeat-those-repetitions",
    "href": "content/03-simulations-in-r/slides/index.html#lets-repeat-those-repetitions",
    "title": "Simulations in R",
    "section": "Let’s repeat those repetitions",
    "text": "Let’s repeat those repetitions\nLet’s run the above simulation several times: a loop in a loop.\n\nset.seed(42)\n\nexperiments <- 3\noutcomes <- list()\n\nfor (i in 1:experiments) {\n  \n  results <- NULL\n  results[1] <- starting\n  \n  for (current_year in 2:years) {\n    # how many new phds (15% chance in 4 \"trials\")\n    newbies <- rbinom(n = 1, size = profs, prob = money)\n    \n    # how many see the light and quit\n    enlightened <- rbinom(n = 1, size = results[current_year - 1], prob = quitting)\n    \n    # new total\n    results[current_year] <- results[current_year - 1] + newbies - enlightened\n    \n    # store in overall outcomes\n    outcomes[[i]] <- results\n  }\n}"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#what-did-we-just-do",
    "href": "content/03-simulations-in-r/slides/index.html#what-did-we-just-do",
    "title": "Simulations in R",
    "section": "What did we just do?",
    "text": "What did we just do?\n\n# what we created\noutcomes\n\n[[1]]\n [1] 12 12 11 12 13 13 12 14 12 14 13 15 16 15 13 12 12 11 10 10 11 11  9  7  8\n\n[[2]]\n [1] 12 13 13 12 11 12 12 10 11 12 11 12 12 12 11 11 11 12 11 12 12 12 13 11 12\n\n[[3]]\n [1] 12 12 12 13 13 13 13 14 14 15 15 15 15 15 15 16 15 14 15 14 14 15 15 15 14\n\n# the means per \"experiment\"\nlapply(outcomes, mean)\n\n[[1]]\n[1] 11.92\n\n[[2]]\n[1] 11.72\n\n[[3]]\n[1] 14.12"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#lets-have-a-look",
    "href": "content/03-simulations-in-r/slides/index.html#lets-have-a-look",
    "title": "Simulations in R",
    "section": "Let’s have a look",
    "text": "Let’s have a look\n\n# the mean of the means\nmean(sapply(outcomes, mean))\n\n[1] 12.58667\n\nmatplot(matrix(unlist(outcomes), ncol = 3), type = \"l\", xlab = \"Year\", ylab = \"Number of PhDs\")"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#what-if-we-want-to-change-stuff",
    "href": "content/03-simulations-in-r/slides/index.html#what-if-we-want-to-change-stuff",
    "title": "Simulations in R",
    "section": "What if we want to change stuff?",
    "text": "What if we want to change stuff?\nIf we want to quickly change a parameter, it makes sense to turn this all into a function.\n\ncounting_phds <- \n  function(\n    starting = 12,\n    profs = 4,\n    money = 0.15,\n    quitting = 0.05,\n    years = 25\n  ) {\n    \n    # create our output vector\n    results <- NULL\n    results[1] <- starting\n    \n    # then our loop\n    for (current_year in 2:years) {\n      # how many new phds\n      newbies <- rbinom(n = 1, size = profs, prob = money)\n      \n      # how many see the light and quit\n      enlightened <- rbinom(n = 1, size = results[current_year - 1], prob = quitting)\n      \n      # new total\n      results[current_year] <- results[current_year - 1] + newbies - enlightened\n    }\n    \n    return(results)\n  }"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#lets-call-that-function",
    "href": "content/03-simulations-in-r/slides/index.html#lets-call-that-function",
    "title": "Simulations in R",
    "section": "Let’s call that function",
    "text": "Let’s call that function\nNow we can change parameters of the “experiment” as we wish.\n\ncounting_phds()\n\n [1] 12 12 10 10 11 12 12 12 11 13 14 15 15 15 16 15 15 15 15 15 15 15 16 16 15\n\ncounting_phds(profs = 10)\n\n [1] 12 12 12 13 14 16 18 17 20 21 21 22 21 22 22 23 21 20 21 21 22 23 26 23 24\n\ncounting_phds(years = 10)\n\n [1] 12 11 11 12 12 10 10 10  9  9\n\ncounting_phds(quitting = 0)\n\n [1] 12 13 14 15 15 15 17 17 17 18 18 20 21 21 22 22 22 23 23 23 23 23 23 23 23"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#running-experiments-one-more-time",
    "href": "content/03-simulations-in-r/slides/index.html#running-experiments-one-more-time",
    "title": "Simulations in R",
    "section": "Running experiments one more time",
    "text": "Running experiments one more time\nWe can do the loop in a loop again, but this time we just call the function.\n\nset.seed(42)\n\nexperiments <- 3\noutcomes <- list()\n\nfor (i in 1:experiments) {\n  \n  # run one experiment and store the results\n  results <- counting_phds()\n  \n  # put results into our outcomes\n  outcomes[[i]] <- results\n\n}\n\noutcomes\n\n[[1]]\n [1] 12 12 11 12 13 13 12 14 12 14 13 15 16 15 13 12 12 11 10 10 11 11  9  7  8\n\n[[2]]\n [1] 12 13 13 12 11 12 12 10 11 12 11 12 12 12 11 11 11 12 11 12 12 12 13 11 12\n\n[[3]]\n [1] 12 12 12 13 13 13 13 14 14 15 15 15 15 15 15 16 15 14 15 14 14 15 15 15 14"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#combining-it-all",
    "href": "content/03-simulations-in-r/slides/index.html#combining-it-all",
    "title": "Simulations in R",
    "section": "Combining it all",
    "text": "Combining it all\nNow we can iterate (aka loop) over different arguments of our experiment. Let’s see what happens if we run 3 experiments, each with a different number of profs. We’ll store the total at the end of the time period for each run.\n\nset.seed(42)\n\nexperiments <- 3\nprofs <- 4\nyears <- 10\noutcomes <- data.frame(\n  experiment = NULL,\n  prof = NULL,\n  total = NULL\n)\n\nfor (i in 1:experiments) {\n  \n  # for each run/experiment, we store the results for each number of professors\n  for (aprof in 1:profs) {\n    results <- counting_phds(profs = aprof, years = years)\n    \n    # get the total at the last year\n    total <- results[years]\n    \n    # turn into a row and add to outcomes\n    our_row <- data.frame(experiment = i, prof = aprof, total = total)\n    outcomes <- rbind(outcomes, our_row)\n  }\n}"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#lets-plot",
    "href": "content/03-simulations-in-r/slides/index.html#lets-plot",
    "title": "Simulations in R",
    "section": "Let’s plot",
    "text": "Let’s plot"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#now-lets-expand",
    "href": "content/03-simulations-in-r/slides/index.html#now-lets-expand",
    "title": "Simulations in R",
    "section": "Now let’s expand",
    "text": "Now let’s expand\n\nset.seed(42)\n\nexperiments <- 1000\nprofs <- 4\nyears <- 10\noutcomes <- data.frame(\n  experiment = NULL,\n  prof = NULL,\n  total = NULL\n)\n\nfor (i in 1:experiments) {\n  \n  # for each run/experiment, we store the results for each number of professors\n  for (aprof in 1:profs) {\n    results <- counting_phds(profs = aprof, years = years)\n    \n    # get the total at the last year\n    total <- results[years]\n    \n    # turn into a row and add to outcomes\n    our_row <- data.frame(experiment = i, prof = aprof, total = total)\n    outcomes <- rbind(outcomes, our_row)\n  }\n}"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#lets-summarize-and-plot",
    "href": "content/03-simulations-in-r/slides/index.html#lets-summarize-and-plot",
    "title": "Simulations in R",
    "section": "Let’s summarize and plot",
    "text": "Let’s summarize and plot\nWith those parameters, does it make sense to have more profs?\n\n\n\n  Group.1      x\n1       1  8.630\n2       2  9.750\n3       3 10.819\n4       4 12.023"
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#a-note-on-efficiency",
    "href": "content/03-simulations-in-r/slides/index.html#a-note-on-efficiency",
    "title": "Simulations in R",
    "section": "A note on efficiency",
    "text": "A note on efficiency\nRight now, we’re appending our data to an object that grows with each iteration:\n\n# turn into a row and add to outcomes\nour_row <- data.frame(experiment = i, prof = aprof, total = total)\noutcomes <- rbind(outcomes, our_row)\n\nThis procedure helps with explaining the logic, but is inefficient. It’s much faster to pre-allocate space:\n\noutcomes <- \n  data.frame(\n    experiment = rep(1:years, each = profs),\n    prof = rep(1:profs, times = years),\n    total = NA\n  )\n\noutcomes$total[i] <- total\n\nFor your own simulations, when efficiency matters, you should always pre-allocate space."
  },
  {
    "objectID": "content/03-simulations-in-r/slides/index.html#references",
    "href": "content/03-simulations-in-r/slides/index.html#references",
    "title": "Simulations in R",
    "section": "References",
    "text": "References\n\n\nTrisovic, Ana, Matthew K. Lau, Thomas Pasquier, and Mercè Crosas. 2022. “A Large-Scale Study on Research Code Quality and Execution.” Scientific Data 9 (1): 60. https://doi.org/10.1038/s41597-022-01143-6.\n\n\n\n\nHome"
  },
  {
    "objectID": "content/04-effect-sizes/04-exercise.html#exercise",
    "href": "content/04-effect-sizes/04-exercise.html#exercise",
    "title": "Exercise II",
    "section": "1.1 Exercise",
    "text": "1.1 Exercise\nWhat’s the distribution of p-values when there’s no effect? Create two independent groups, both with the same mean and standard deviation in rnorm (because there’s no effect). Use rnorm’s default mean and SD. Each group should have 50 participants. Run an independent, two-tailed t-test and store the p-value. Repeat 10,000 times. Now plot the p-values as a histogram (hist). Did you expect them to look like this?\n\nset.seed(42)\nn <- 50\nruns <- 1e3\n\npvalues <- NULL\n\nfor (i in 1:runs) {\n  control <- rnorm(n)\n  treatment <- rnorm(50)\n  \n  t <- t.test(control, treatment)\n  \n  pvalues[i] <- t$p.value\n\n}\n\nhist(pvalues)\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nNothing fancy here. Start with declaring your variables, create an object to store the p-values, then create two groups and run a t-test in a loop:\n\nfor (i in 1:?) {\n  control <- ?\n  treatment <- ?\n  \n  t <- t.test(control, treatment)\n  \n  # store the p-value\n  ? <- t$p.value\n\n}"
  },
  {
    "objectID": "content/04-effect-sizes/04-exercise.html#exercise-1",
    "href": "content/04-effect-sizes/04-exercise.html#exercise-1",
    "title": "Exercise II",
    "section": "1.2 Exercise",
    "text": "1.2 Exercise\nNow let’s check empirically why p-values alone don’t tell us much. Repeat your simulation from above, but this time create two “experiments” per run: One where there’s no difference (like you already have above, with mean = 0 and SD = 1), but also one where there’s a massive difference (at least mean = 0.8, SD = 1). Store the p-values of the no difference experiments and the p-values of the massive difference experiments in a data frame, with one variable for condition and one for pvalue.\nNow plot both as densities (so two density lines). Us this code (call the data frame d and the number of simulations runs):\n\nlibrary(ggplot2)\n\nggplot(d, aes(x = pvalue, color = condition)) + geom_density() + xlim(c(0, 0.05)) + ylim(c(0, runs/5)) + theme_bw()\n\nLook at the p = 0.04. If were to run an experiment, not knowing which reality (no effect or massive effect) our experiments comes from, which one is more likely with a p-value of 0.04? If all we know is that our p-value is 0.04, what’s our best bet for the reality our experiment comes from: no effect or massive effect?\n\nn <- 50\nruns <- 1e3\n\nd <- \n  data.frame(\n    null = NULL,\n    effect = NULL\n  )\n\nfor (i in 1:runs) {\n  control1 <- rnorm(n)\n  treatment1 <- rnorm(n)\n  \n  t1 <- t.test(control1, treatment1)\n  \n  control2 <- rnorm(n)\n  treatment2 <- rnorm(n, 0.8)\n  \n  t2 <- t.test(control2, treatment2)\n  \n  d <- \n    rbind(\n      d,\n      data.frame(\n        condition = c(\"null\", \"effect\"),\n        pvalue = c(t1$p.value, t2$p.value)\n      )\n    )\n\n}\n\nlibrary(ggplot2)\n\nggplot(d, aes(x = pvalue, color = condition)) + geom_density() + xlim(c(0, 0.05)) + ylim(c(0, runs/5)) + theme_bw()\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nYou do the same as above, only this time you do two “experiments” inside the loop and therefore store two results.\n\nd <- \n  data.frame(\n    null = NULL,\n    effect = NULL\n  )\n\nfor (i in 1:?) {\n  \n  # no difference experiment\n  control1 <- ?\n  treatment1 <- ?\n  \n  t1 <- # first t-test\n  \n    # massive difference experiment\n  control2 <- ?\n  treatment2 <- ?\n  \n  t2 <- ?\n  \n    # then store them in a data frame\n  d <- \n    rbind(\n      d,\n      data.frame(\n        condition = c(\"null\", \"effect\"),\n        pvalue = c(t1$p.value, t2$p.value)\n      )\n    )\n\n}"
  },
  {
    "objectID": "content/04-effect-sizes/04-exercise.html#exercise-2",
    "href": "content/04-effect-sizes/04-exercise.html#exercise-2",
    "title": "Exercise II",
    "section": "1.3 Exercise",
    "text": "1.3 Exercise\nGoogle “Lindley’s paradox”. Let’s return to our alpha (0.05) in one of the next sessions: but keep this in mind."
  },
  {
    "objectID": "content/04-effect-sizes/04-exercise.html#exercise-3",
    "href": "content/04-effect-sizes/04-exercise.html#exercise-3",
    "title": "Exercise II",
    "section": "1.4 Exercise",
    "text": "1.4 Exercise\nLet’s expand on our simulation skills from the previous sessions. In previous sessions, we simulated t-tests for different sample sizes over many repetitions. That was a loop in a loop: We’re looping over sample size, then, for each sample size, we looped over the number of simulations. Now, let’s add another layer: the effect size.\nCreate a simulation for an independent t-test. The mean for the control group should be 4 with an SD of 1. The means for the treatment group should be 4.2 (small), 4.5 (medium), and 4.8 (large), with an SD that’s always 1. Create a data.frame, store effect_size (so the difference in means between the groups: small, medium, and large), the sample size (from 20:100), and the mean power for this effect size and sample size.\nTip, you could go about this with the following structure:\n\nfor (asize in c(4.2, 4.5, 4.8)) {\n  for (asample in minimum:maximum) {\n    for (arun in 1:runs) {\n      \n    }\n    \n  }\n}\n\nGo for 200 runs for each combination of effect size and sample size. Plot the results afterwards with (assuming your data frame is called outcome):\n\nlibrary(ggplot2)\n\nggplot(outcome, aes(x = sample_size, y = power, color = as.factor(effect_size))) + geom_line() + theme_bw()\n\n\nset.seed(42)\neffect_sizes <- c(4.2, 4.5, 4.8)\nmin_sample <- 20\nmax_sample <- 100\nruns <- 200\nsd <- 1\n\noutcome <- \n  data.frame(\n    effect_size = NULL,\n    sample_size = NULL,\n    power = NULL\n  )\n\nfor (asize in effect_sizes) {\n  \n  for (asample in min_sample:max_sample) {\n    \n    run_results <- NULL\n    \n    for (arun in 1:runs) {\n      control <- rnorm(asample, 4, sd)\n      treatment <- rnorm(asample, asize, sd)\n      \n      t <- t.test(control, treatment)\n      \n      run_results[arun] <- t$p.value\n    }\n    \n    outcome <- \n      rbind(\n        outcome,\n        data.frame(\n          effect_size = asize,\n          sample_size = asample,\n          power = sum(run_results<0.05) / length(run_results)\n        )\n      )\n  }\n}\n\nggplot(outcome, aes(x = sample_size, y = power, color = as.factor(effect_size))) + geom_line() + theme_bw()\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nLet’s fill in the provided structure:\n\neffect_sizes <- ?\nmin_sample <- ?\nmax_sample <- ?\nruns <- 200\nsd <- 1\n\n# a data frame to store all of our results\noutcome <- \n  data.frame(\n    effect_size = NULL,\n    sample_size = NULL,\n    power = NULL\n  )\n\nfor (asize in effect_sizes) { # iterate over the differences in effect sizes\n  \n  for (asample in min_sample:max_sample) {\n    \n    # somewhere to store the results of the following 200 runs\n    run_results <- NULL\n    \n    for (arun in 1:runs) {\n      control <- rnorm(?, 4, sd)\n      treatment <- rnorm(?, ?, sd)\n      \n      t <- t.test(control, treatment)\n      \n      run_results[arun] <- t$p.value\n    }\n    \n    # store it all\n    outcome <- \n      rbind(\n        outcome,\n        data.frame(\n          effect_size = asize,\n          sample_size = asample,\n          power = ?\n        )\n      )\n  }\n}"
  },
  {
    "objectID": "content/04-effect-sizes/04-exercise.html#exercise-4",
    "href": "content/04-effect-sizes/04-exercise.html#exercise-4",
    "title": "Exercise II",
    "section": "1.5 Exercise",
    "text": "1.5 Exercise\nGuess what? The power analysis you did above was on the standardized scale. Do you know why? Let’s look at the formula for Cohen’s d again:\n\\(d = \\frac{M_1-M_2}{pooled \\ SD}\\)\nAbove, our means were 4 for the control group, and 4.2, 4,5, and 4.8 for the treatment group. Crucially, the SD was 1 for all groups. That means, very conveniently, that the differences are 0.2, 0.5, and 0.8 standard deviations–and Cohen’s \\(d\\) is measured in standard deviations. 0.2, 0.5, and 0.8 are also the heuristics for a small, medium, and large effect, respectively.\nYou can verify that yourself. Simulate a normally distributed control group (10,000 cases) with a mean of 100 and an SD of 1. Also simulate a treatment group with a mean of 101 and and SD of 1. Then do two more groups, but this time increase the SD for both groups to 2. What should happen to Cohen’s \\(d\\) from the first comparison to the second? Verify your intuition by using the effectsize::cohens_d function.\n\nn <- 1e4\ncontrol1 <- rnorm(n, 100, 1)\ntreatment1 <- rnorm(n, 101, 1)\n\ncontrol2 <- rnorm(n, 100, 2)\ntreatment2 <- rnorm(n, 101, 2)\n\neffectsize::cohens_d(control1, treatment1)\n\nCohen's d |         95% CI\n--------------------------\n-0.99     | [-1.02, -0.96]\n\n- Estimated using pooled SD.\n\neffectsize::cohens_d(control2, treatment2)\n\nCohen's d |         95% CI\n--------------------------\n-0.49     | [-0.52, -0.46]\n\n- Estimated using pooled SD."
  },
  {
    "objectID": "content/04-effect-sizes/04-exercise.html#exercise-5",
    "href": "content/04-effect-sizes/04-exercise.html#exercise-5",
    "title": "Exercise II",
    "section": "1.6 Exercise",
    "text": "1.6 Exercise\nProbably the easiest way to simulate standardized effects is to simply rely on variables that are already standardized. Look up the default values of rnorm. SD here is set to 1, so whatever difference we put down as the difference in means between the two groups will be our Cohen’s \\(d\\). Simulate power for a Cohen’s \\(d\\) of 0.1 in an independent, two-tailed t-test for a sample size ranging from 10:1000. Use 200 simulations per sample size (or more, if you’re fine waiting for a couple of minutes). Before looking at the results: How many people do you think you’ll need per group for 95% power? Verify that in GPower.\n\nset.seed(42)\n\ndraws <- 200\nmin_sample <- 10\nmax_sample <- 1000\ncohens_d <- 0.1\n\noutcomes <- data.frame(\n  sample_size = NULL,\n  power = NULL\n)\n\nfor (i in min_sample:max_sample) {\n  \n  pvalues <- NULL\n  \n  for (j in 1:draws) {\n    control <- rnorm(i)\n    treatment <- rnorm(i, cohens_d)\n    \n    t <- t.test(control, treatment)\n    pvalues[j] <- t$p.value\n  }\n  \n  outcomes <- rbind(\n    outcomes,\n    data.frame(\n      sample_size = i,\n      power = sum(pvalues<0.05)/length(pvalues)\n    )\n  )\n}\n\nplot(outcomes$sample_size, outcomes$power, type = \"l\")\nabline(h = 0.80)\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nNothing here should be new. You define the parameters first, then apply a loop to every sample size, and for every sample size, you do a number of simulations and extract the p-values.\n\ndraws <- ?\nmin_sample <- ?\nmax_sample <- ?\ncohens_d <- ?\n\n# our outcome\noutcomes <- data.frame(\n  sample_size = NULL,\n  power = NULL\n)\n\n# first loop over the different sample sizes\nfor (i in ?:?) {\n  \n  # store our pvalues somewhere\n  \n  # second loop over the draws/runs\n  for (j in 1:?) {\n    control <- rnorm(i)\n    treatment <- rnorm(i, ?)\n    \n    t <- t.test(control, treatment)\n    \n    # store that pvalue in the object your create in the first loop level\n  }\n  \n  # store the results for this sample size in out outcomes\n  outcomes <- rbind(\n    outcomes,\n    data.frame(\n      sample_size = ?,\n      power = ? # proportion of p-values < 0.05 for this sample size\n    )\n  )\n}\n\nYou can use the following code for plotting: plot(outcomes$sample_size, outcomes$power, type = \"l\"); abline(h = 0.80)."
  },
  {
    "objectID": "content/04-effect-sizes/04-exercise.html#exercise-6",
    "href": "content/04-effect-sizes/04-exercise.html#exercise-6",
    "title": "Exercise II",
    "section": "1.7 Exercise",
    "text": "1.7 Exercise\nHow much power do we gain if we specify the direction of a test? Run a simulation with a Cohen’s \\(d\\) of 0.5 for sample sizes of 20 to 100 (1,000 samples each). For each run, do a one-tailed and a two-tailed t-test. Then plot the power curves and calculate the average power (across all sample sizes) for the two.\nIf you store the following data frame, you can use the below code for plotting.\n\nggplot(outcomes, aes(x = sample_size, y = power, color = as.factor(type))) + geom_line() + theme_bw()\n\n\nset.seed(42)\n\ndraws <- 1e3\nmin_sample <- 20\nmax_sample <- 100\ncohens_d <- 0.5\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    type = NULL,\n    power = NULL\n  )\n\nfor (i in min_sample:max_sample) {\n  \n  pvalues_one <- NULL\n  pvalues_two <- NULL\n  \n  for (j in 1:draws) {\n    control <- rnorm(i)\n    treatment <- rnorm(i, cohens_d)\n    \n    t_one <- t.test(control, treatment, alternative = \"less\")\n    t_two <- t.test(control, treatment, alternative = \"two.sided\")\n    \n    pvalues_one[j] <- t_one$p.value\n    pvalues_two[j] <- t_two$p.value\n  }\n  \n  outcomes <- rbind(\n    outcomes,\n    data.frame(\n      sample_size = c(i, i),\n      type = c(\"one-sided\", \"two-sided\"),\n      power = c(sum(pvalues_one<0.05)/length(pvalues_one), sum(pvalues_two<0.05)/length(pvalues_two))\n    )\n  )\n}\n\naggregate(outcomes$power, by = list(outcomes$type), mean)\n\n    Group.1         x\n1 one-sided 0.8121111\n2 two-sided 0.7269630\n\nggplot(outcomes, aes(x = sample_size, y = power, color = as.factor(type))) + geom_line() + theme_bw()\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nYou do the same as above, except this time you perform two t-tests inside the second loop:\n\ncontrol <- rnorm(?)\ntreatment <- rnorm(?, ?)\n\nt_one <- t.test(control, treatment, alternative = \"less\")\nt_two <- t.test(control, treatment, alternative = \"two.sided\")\n\nYou can get power for each type by running aggregate(outcomes$power, by = list(outcomes$type), mean)."
  },
  {
    "objectID": "content/04-effect-sizes/04-exercise.html#exercise-7",
    "href": "content/04-effect-sizes/04-exercise.html#exercise-7",
    "title": "Exercise II",
    "section": "1.8 Exercise",
    "text": "1.8 Exercise\nSo far, getting a pooled SD was easy because it was the same in both groups. Let’s see how we can simulate standardized effect sizes (Cohen’s \\(d\\) in this case). Write a function that calculates the pooled SD from two SDs. The formula:\n\\(pooled = \\sqrt{\\frac{(sd_1^2 + sd_2^2)}{2}}\\)\nThen think about two groups that have different SDs. Say we measured something on a 7-point scale. The control group has an SD of 0.5, whereas the treatment group has an SD of 1.7.\nHow large does the difference in means have to be for a \\(d\\) of 0.25? Prove it with a simulation, using the following code:\n\nn <- 1e4\n\ncontrol <- rnorm(n, ?, 0.5)\ntreatment <- rnorm(n, ?, 1.7)\n\neffectsize::cohens_d(control, treatment)\n\n\npooled_sd <- \n  function(\n    sd1,\n    sd2){\n    \n    p_sd <- sqrt((sd1**2 + sd2**2)/2)\n    \n    return(p_sd)\n  }\nn <- 1e4\n\nsd <- pooled_sd(0.5, 1.7)\ncohens_d <- 0.25\n\ncontrol <- rnorm(n, 4, 0.5)\ntreatment <- rnorm(n, 4 + sd*cohens_d, 1.7)\n\neffectsize::cohens_d(control, treatment)\n\nCohen's d |         95% CI\n--------------------------\n-0.24     | [-0.31, -0.21]\n\n- Estimated using pooled SD.\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nThis one isn’t much about coding, but about understanding the concept behind effect sizes. First, let’s do the function.\n\npooled_sd <- \n  function(\n    sd1,\n    sd2){\n    \n    p_sd <- ? # cohen's formula\n    \n    return(p_sd)\n  }\n\nNow you can feed the two SDs to the formula: pooled_sd(0.5, 1.7). There you go. That’s your unit for a standardized effect size. If one group is 0.25 units larger than the other, all we need to do is feed pooled_sd * cohens_d into rnorm as the mean for the treatment group."
  },
  {
    "objectID": "content/04-effect-sizes/04-exercise.html#exercise-8",
    "href": "content/04-effect-sizes/04-exercise.html#exercise-8",
    "title": "Exercise II",
    "section": "1.9 Exercise",
    "text": "1.9 Exercise\nDo the above again, but this time add 1 to the mean of both control group and treatment group. What do you notice?\n\ncontrol <- rnorm(n, 5, 0.5)\ntreatment <- rnorm(n, 5 + sd*cohens_d, 1.7)\n\neffectsize::cohens_d(control, treatment)\n\nCohen's d |         95% CI\n--------------------------\n-0.23     | [-0.26, -0.20]\n\n- Estimated using pooled SD."
  },
  {
    "objectID": "content/04-effect-sizes/04-exercise.html#exercise-9",
    "href": "content/04-effect-sizes/04-exercise.html#exercise-9",
    "title": "Exercise II",
    "section": "1.10 Exercise",
    "text": "1.10 Exercise\nThe above example shows how arbitrary it can be to go for a standardized effect size. You calculated power in two ways:\n\nYou decided what your Cohen’s \\(d\\) is and just went with an SD of 1, ignoring how the SD of your actual variables might look like\nYou specified the SDs for both groups and then calculated the pooled SD to adjust the means of the groups\n\nThe first option is easy, but comes with all the limitations of standardized effect sizes we talked about. The second option requires much more thought–but it ignores the absolute level of the means because you only specify means in their distance from each other in SD units. In fact, I’d argue that if you’ve made it this far and thought about the variation in your measures, you might as well think about the means and their differences in raw units.\nStandard deviations can be really hard to determine. In that case, you could also examine uncertainty around the SDs of each group. Simulate an independent t-test (one-sided, 1,000 simulations) over sample sizes 50:150. The control group has a mean of 100, the treatment group a mean of 105. Simulate over two conditions: Where the SD (both groups have the same SD) is 10 and one where it’s 25.\nThis time, save everything in a data frame: The sd (small or large), the sample_size, the number of the run, and the pvalue. That means your data frame will have 2 SD x 50 sample sizes x 1,000 simulations. With the data frame, get the power per SD and sample size (tip: use aggregate or group_by if you use the tidyverse). For example: aggregate(pvalue ~ sd + sample_size, data = outcomes, FUN = function(x) sum(x<.05)/length(x)). Then plot the two power curves. You can use the following code:\n\nlibrary(ggplot2)\n\nggplot(d, aes(x = sample_size, y = power, color = as.factor(sd))) + geom_line() + theme_bw()\n\nStart with the following structure:\n\nfor (an_sd in c(10, 25)) {\n  for (n_size in 50:150) {\n    for (run in 1:1e3) {\n      \n    }\n    \n  }\n}\n\nPay attention to two things: What does it do to your runtime? (Spoiler: It’ll take a while!). Second: How does the SD influence power?\n\nmin_sample <- 50\nmax_sample <- 150\nruns <- 500\nsds <- c(10, 25)\n\noutcomes <- \n  data.frame(\n    sd = NULL,\n    sample_size = NULL,\n    run = NULL,\n    pvalue = NULL\n  )\n\nfor (ansd in sds) {\n  \n  for (asize in min_sample:max_sample) {\n    \n    for (arun in 1:runs) {\n      \n      control <- rnorm(asize, 100, ansd)\n      treatment <- rnorm(asize, 105, ansd)\n      \n      t <- t.test(control, treatment, alternative = \"less\")\n      \n      outcomes <- \n        rbind(\n          outcomes,\n          data.frame(\n            sd = ansd,\n            sample_size = asize,\n            run = arun,\n            pvalue = t$p.value\n          )\n        )\n    }\n  }\n}\n\nlibrary(tidyverse)\nd <- outcomes %>% group_by(sd, sample_size) %>% summarise(power = sum(pvalue<0.05)/n()) %>% ungroup()\n\nggplot(d, aes(x = sample_size, y = power, color = as.factor(sd))) + geom_line() + theme_bw()\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nThis is a 3-level loop, starting with the SD, then the sample size, then the actual simulations. We’ll also need to store everything this time, so our results data frame will be large. That also means no need for storing summaries per run (our pvalues in earlier simulations).\n\nmin_sample <- ?\nmax_sample <- ?\nruns <- ?\nsds <- c(?, ?)\n\n# a data frame for us to store everything\noutcomes <- \n  data.frame(\n    sd = NULL,\n    sample_size = NULL,\n    run = NULL,\n    pvalue = NULL\n  )\n\n# first level: SDs\nfor (ansd in sds) {\n  \n  # second level: sample size\n  for (asize in min_sample:max_sample) {\n    \n    # third level: runs\n    for (arun in 1:runs) {\n      \n      control <- rnorm(?, 100, ?)\n      treatment <- rnorm(?, 105, ?)\n      \n      t <- t.test(control, treatment, alternative = \"less\")\n      \n      # then we store everything from this run\n      outcomes <- \n        rbind(\n          outcomes,\n          data.frame(\n            sd = ?,\n            sample_size = ?,\n            run = ?,\n            pvalue = ? # single p-value\n          )\n        )\n    }\n  }\n}\n\nGet the power per combo with aggregate: aggregate(pvalue ~ sd + sample_size, data = outcomes, FUN = function(x) sum(x<.05)/length(x))"
  },
  {
    "objectID": "content/04-effect-sizes/04-exercise.html#exercise-10",
    "href": "content/04-effect-sizes/04-exercise.html#exercise-10",
    "title": "Exercise II",
    "section": "2.1 Exercise",
    "text": "2.1 Exercise\nAlright, now let’s get familiar with the while function. Simulate (200 runs) an independent (we’ll go paired soon enough) t-test (two-tailed) for two different effect sizes. The SD for both groups is 2. The mean for the control group is 100. In the small effects condition, the treatment group is 0.5 points larger. In the large effects condition, the treatment group is 1 points larger.\nStart at a sample size of 25 and stop when you reach 95% power. Plot the power curve and show that the larger effect size stops much earlier.\n\nfor (asize in c(0.5, 1)) {\n  n <- 1\n  power <- 0\n  while (power < 0.95) {\n    for (i in 1:runs) {\n    }\n    power <- ?\n  }\n  n <- n + 1\n  \n}\n\nlibrary(ggplot2)\n\nggplot(outcomes, aes(x = sample_size, y = power, color = as.factor(effect_size))) + geom_line() + geom_hline(yintercept = 0.95) + theme_bw()\n\n\neffect_sizes <- c(small = 0.5, large = 1)\ndraws <- 200\n\noutcomes <- data.frame(\n  effect_size = NULL,\n  sample_size = NULL,\n  power = NULL\n)\n\nfor (asize in effect_sizes) {\n  \n  n <- 30\n  power <- 0\n  \n  while (power < 0.95) {\n    \n    pvalues <- NULL\n    \n    for (i in 1:draws) {\n      control <- rnorm(n, 100, 2)\n      treatment <- rnorm(n, 100 + asize, 2)\n      t <- t.test(control, treatment)\n      \n      pvalues[i] <- t$p.value\n    }\n    \n    power <- sum(pvalues < 0.05) / length(pvalues)\n    \n    outcomes <- rbind(\n      outcomes,\n      data.frame(\n        effect_size = asize,\n        sample_size = n,\n        power = power\n      )\n    )\n    \n    n <- n + 1\n  }\n}\n\nggplot(outcomes, aes(x = sample_size, y = power, color = as.factor(effect_size))) + geom_line() + geom_hline(yintercept = 0.95) + theme_bw()\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nRemember that the logic is different. You start at the lowest sample size, then you run 200 simulations for this sample size, check whether it’s below 95% and, if so, increase the sample size. You do this until power reaches our desired level.\n\neffect_sizes <- c(small = ?, large = ?)\ndraws <- ?\n\n# store our results\noutcomes <- data.frame(\n  effect_size = NULL,\n  sample_size = NULL,\n  power = NULL\n)\n\n# we iterate over the two effect sizes; everything below we do for 0.5, and then again for 1\nfor (asize in effect_sizes) {\n  \n  # our starting sample size\n  n <- 30\n  # our starting power\n  power <- 0\n  \n  # then our while statement: everything stops once we reach 95% power\n  while (power < 0.95) {\n    \n    # somewhere to store p-values\n    pvalues <- NULL\n    \n    # so while statement not satisfied, so we do simulations with the current sample size\n    for (i in 1:draws) {\n      control <- rnorm(n, ?, 2)\n      treatment <- rnorm(n, ?, 2)\n      t <- t.test(control, treatment)\n      \n      pvalues[i] <- t$p.value\n    }\n    \n    # now we store power\n    power <- ?\n    \n    # and put everything into our data frame\n    outcomes <- rbind(\n      outcomes,\n      data.frame(\n        effect_size = asize,\n        sample_size = n,\n        power = power\n      )\n    )\n    \n    # we need to increase the sample size because now we \"go up\" and check whether power < 95%; if not, we'll do this all again with our new n that is increased by 1\n    n <- n + 1\n  }\n}"
  },
  {
    "objectID": "content/04-effect-sizes/04-exercise.html#exercise-11",
    "href": "content/04-effect-sizes/04-exercise.html#exercise-11",
    "title": "Exercise II",
    "section": "2.2 Exercise",
    "text": "2.2 Exercise\nYou’re measuring people’s mood twice: right before and right after lunch. You know from extensive study on effect sizes that mood needs to increase by 1 point on a 7-point scale for other people to notice. You assume the SD is somewhere around 1.5, with a bit more variation before lunch (1.7) than after lunch (1.5). You also assume that mood is correlated to some degree, say 0.5.\nYou have budget for a maximum sample of 100. How many people do you need to measure to achieve 87% power to detect this 1-point effect–and do you need to go to your max? Use 500 simulations per run. (Tip: the samples are correlated, so you’ll need to specify a variance-covariance matrix.) Verify with GPower.\n\nlibrary(MASS)\n\nruns <- 500\nmin_sample <- 2\nmax_sample <- 100\nmeans <- c(before = 4, after = 5)\npre_sd <- 1.7\npost_sd <- 1.5\ncorrelation <- 0.5\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nsigma <- \n  matrix(\n    c(\n      pre_sd**2,\n      correlation * pre_sd * post_sd,\n      correlation * pre_sd * post_sd,\n      post_sd**2\n    ),\n    ncol = 2\n  )\n\n\nfor (asample in min_sample:max_sample) {\n  \n  pvalues <- NULL\n  \n  for (arun in 1:runs) {\n    d <- as.data.frame(\n      mvrnorm(\n        asample,\n        means,\n        sigma\n      )\n    )\n    \n    t <- t.test(d$before, d$after, paired = TRUE, alternative = \"less\")\n    \n    pvalues[arun] <- t$p.value\n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = asample,\n        power = sum(pvalues<0.05)/length(pvalues)\n      )\n    )\n}\n\nplot(outcomes$power)\nabline(h = 0.87)\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nThis exercise is about you trying out the variance-covariance matrix. First, you declare your variables, like we did before. Essentially, you store every parameter from the instructions.\n\nruns <- ?\nmin_sample <- ?\nmax_sample <- ?\nmeans <- c(before = ?, after = ?)\npre_sd <- ?\npost_sd <- ?\ncorrelation <- ?\n\n# store our results\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nAs a next step, you need to specify the variance/covariance matrix. For that, you’ll need the variance (so squared SD) and the correlation between pre- and post-measure.\n\nsigma <- \n  matrix(\n    c(\n      # pre SD squared,\n      correlation * pre_sd * post_sd,\n      correlation * pre_sd * post_sd,\n      # post SD squared\n    ),\n    ncol = ?\n  )\n\nNow you have everything you need. Everything else is just as before: Loop over the sample size, which again loops over a number of simulations. For each run, you create correlated scores with the mvrnorm function, then run a paired t-test.\n\nfor (asample in min_sample:max_sample) {\n  \n  pvalues <- NULL\n  \n  for (arun in 1:runs) {\n    d <- as.data.frame(\n      mvrnorm(\n        ?,\n        ?,\n        ?\n      )\n    )\n    \n    t <- t.test(?, ?, paired = TRUE, alternative = \"less\")\n    \n    pvalues[?] <- t$p.value\n  }\n  \n  # don't forget to store the outcomes\n}\n\nYou can plot the results with plot(outcomes$power); abline(h = 0.87)."
  },
  {
    "objectID": "content/04-effect-sizes/04-exercise.html#exercise-12",
    "href": "content/04-effect-sizes/04-exercise.html#exercise-12",
    "title": "Exercise II",
    "section": "2.3 Exercise",
    "text": "2.3 Exercise\nHow does the correlation between measures influence the power of your test? Do the above simulation again, but this time try out correlations between pre and post scores of 0.3, 0.5, 0.7. What do you expect to happen?\n\nruns <- 500\nmin_sample <- 2\nmax_sample <- 100\nmeans <- c(before = 4, after = 5)\npre_sd <- 1.7\npost_sd <- 1.5\ncorrelations <- c(small = 0.3, medium = 0.5, large = 0.7)\n\noutcomes <- \n  data.frame(\n    correlation = NULL,\n    sample_size = NULL,\n    power = NULL\n  )\n\n\n\nfor (acor in correlations) {\n  \n  sigma <- \n    matrix(\n      c(\n        pre_sd**2,\n        acor * pre_sd * post_sd,\n        acor * pre_sd * post_sd,\n        post_sd**2\n      ),\n      ncol = 2\n    )\n  \n  for (asample in min_sample:max_sample) {\n    \n    pvalues <- NULL\n    \n    for (arun in 1:runs) {\n      d <- as.data.frame(\n        mvrnorm(\n          asample,\n          means,\n          sigma\n        )\n      )\n      \n      t <- t.test(d$before, d$after, paired = TRUE, alternative = \"less\")\n      \n      pvalues[arun] <- t$p.value\n    }\n    \n    outcomes <- \n      rbind(\n        outcomes,\n        data.frame(\n          correlation = acor,\n          sample_size = asample,\n          power = sum(pvalues<0.05)/length(pvalues)\n        )\n      )\n  }\n}\n\nggplot(outcomes, aes(x = sample_size, y = power, color = as.factor(correlation))) + geom_line() + theme_classic() \n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nThis time, the variance-covariance matrix changes because the correlation between measures changes. That means you need to create a variance-covariance matrix for each correlation (aka inside that loop level). The structure below should help\n\n# iteratre over correlations\nfor (acor in correlations) {\n  \n  sigma <- ?\n  \n  for (asample in min_sample:max_sample) {\n    \n    # empty storage for p-values\n    \n    for (arun in 1:runs) {\n      # for each run, do mvrnorm and store the p-value\n    }\n    \n    # store correlation, sample size, and power in a data frame\n  }\n}"
  },
  {
    "objectID": "content/04-effect-sizes/04-exercise.html#exercise-13",
    "href": "content/04-effect-sizes/04-exercise.html#exercise-13",
    "title": "Exercise II",
    "section": "2.4 Exercise",
    "text": "2.4 Exercise\nWhat’s happening above? With higher correlations, you also have more power. The formula for Cohen’s \\(d\\) for a paired samples t-test is different from the independent samples t-test one:\n\\(Cohen's \\ d = \\frac{M_{diff}-\\mu_o}{SD_{diff}}\\)\n(\\(\\mu_0\\) is 0 because our H0 is no difference). Show the effect of the correlation between the scores on effect size with a simulation. The pre-score has a mean of 5 and an SD of 1.1; the post-score has a mean of 5.2 with an SD of 0.8. Go with a sample size of 100.\nSimulate the samples with different correlations between the scores, ranging from 0.01 to 0.99. For each correlation, run 1,000 samples, then calculate Cohen’s \\(d\\) and get the mean of \\(d\\) for this correlation. Then plot \\(d\\) for the range of correlations. (Tip: You can create the correlations in such small steps with seq.)\n\nmeans <- c(before = 5, after = 5.2)\nn <- 100\nruns <- 1e3\npre_sd <- 1.1\npost_sd <- 0.8\ncorrelations <- seq(0.01, 0.99, 0.01)\n\noutcomes <- data.frame(\n  correlation = NULL,\n  d = NULL\n)\n\nfor (acor in correlations) {\n  \n  sigma <- \n    matrix(\n      c(\n        pre_sd**2,\n        acor * pre_sd * post_sd,\n        acor * pre_sd * post_sd,\n        post_sd**2\n      ),\n      ncol = 2\n    )\n  \n  ds <- NULL\n  \n  for (arun in 1:runs) {\n    d <- as.data.frame(\n    mvrnorm(\n      n,\n      means,\n      sigma\n    )\n  )\n  \n  difference <- d$before-d$after\n  \n  cohens_d <- (mean(difference) - 0) / sd(difference)\n  ds[arun] <- cohens_d\n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        correlation = acor,\n        d = mean(ds)\n      )\n    )\n}\n\nplot(outcomes$correlation, abs(outcomes$d))\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nSo this time we’re not looking at p-values; we’re looking at \\(d\\). Like above, we declare our variables first.\n\nmeans <- c(before = ?, after = ?)\nn <- ?\nruns <- ?\npre_sd <- ?\npost_sd <- ?\ncorrelations <- seq(0.01, 0.99, 0.01)\n\noutcomes <- data.frame(\n  correlation = NULL,\n  d = NULL\n)\n\nThen you need to iterate over every correlation. For each correlation, you need a separate variance-covariance matrix. Once you have that, you do your regular for-loop, except this time you calculate Cohen’s \\(d\\), not significance. You then store all of those and take the mean for your outcome.\n\nfor (acor in correlations) {\n  \n  sigma <- /\n  \n    # somewhere to store the correlations\n  ds <- NULL\n  \n  # then get 1,000 samples for 1,000 ds\n  for (arun in 1:runs) {\n    d <- as.data.frame(\n    mvrnorm(\n      ?,\n      ?,\n      ?\n    )\n  )\n  \n  difference <- d$before-d$after\n  \n  cohens_d <- # translate the formula for Cohen's d into R code (you'll need the difference)\n    \n  # store the result\n  ds[?] <- cohens_d\n  }\n  \n  # store the results for this correlation\n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        correlation = ?,\n        d = mean(d?)\n      )\n    )\n}\n\nThen plot the effect size against the correlation with plot(outcomes$correlation, abs(outcomes$d))."
  },
  {
    "objectID": "content/04-effect-sizes/04-exercise.html#exercise-14",
    "href": "content/04-effect-sizes/04-exercise.html#exercise-14",
    "title": "Exercise II",
    "section": "2.5 Exercise",
    "text": "2.5 Exercise\nNow you see how standardized effect sizes can become quite the problem: With an increase in the correlation between measures, your \\(SD_{diff}\\) becomes smaller, whereas the means remain unchanged. In other words, the effect sizes becomes larger with decreasing SD because the difference in means turns into more standard deviation units.\nIt’s hard–if not impossible–to have an intuition about what Cohen’s \\(d\\) you can go for because you’ll need to know the means, SDs, and their correlations. If you know those, you have enough information to work on the raw scale anyway–and as we know, the raw scale is much more intuitive.\nYou can showcase how hard that intuition is by skipping straight to the difference score. That’s what a paired samples t-test is: it’s a one-sample t-test of the difference between scores against H0, so for demonstration you can also simulate from the difference. t.test(control, treatment, paired = TRUE) is the same as t.test(difference, mu = 0) (if our H0 is indeed 0).\nSimulate power for Cohen’s \\(d\\) of 0.4 in a paired sampled t-test, using difference scores. That means you just need a normal distribution of difference scores. How do you know the \\(d\\) is 0.4? Well, choose a mean that’s 40% of whatever SD you specify.\nUse 1,000 runs and while to stop whenever you reach 93% power, starting at a sample size of 20. Verify with GPower.\n\nmean_difference <- 0.4\nmean_sd <- 1\nruns <- 1e3\n\npower <- 0\n\nn <- 20\n  \noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nwhile (power < 0.93) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    \n    difference <- rnorm(n, mean_difference, mean_sd)\n    t <- t.test(difference, mu = 0)\n    \n    pvalues[i] <- t$p.value\n    \n  }\n  \n  power <- sum(pvalues < 0.05) / length(pvalues)\n  \n  outcomes <- rbind(\n    outcomes,\n    data.frame(\n      sample_size = n,\n      power = power\n    )\n  )\n  \n  n <- n + 1\n}\n\nplot(outcomes$sample_size, outcomes$power)\nabline(h = 0.93)\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nThis is actually not too hard. Like always, you start with declaring your variables:\n\nmean_sd <- ?\nmean_difference <- mean_sd * 0.4\nruns <- ?\n\n# initiate power\npower <- 0\n\n# start with our sample size\nn <- 20\n\n# store outcomes\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nNext, you define a while loop that only stops once power is high enough. Then you do your regular simulations with a one-sample t-test and store the p-values like we always do. Last, you calculate power and increase the sample size.\n\nwhile (? < ?) {\n  \n  # store our p-values\n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    \n    difference <- rnorm(n, ?, ?)\n    t <- t.test(?, mu = 0)\n    \n    pvalues[i] <- t$p.value\n    \n  }\n  \n  power <- ?\n  \n  outcomes <- rbind(\n    outcomes,\n    data.frame(\n      sample_size = n,\n      power = power\n    )\n  )\n  \n  n <- ?\n}"
  },
  {
    "objectID": "content/04-effect-sizes/04-exercise.html#exercise-15",
    "href": "content/04-effect-sizes/04-exercise.html#exercise-15",
    "title": "Exercise II",
    "section": "2.6 Exercise",
    "text": "2.6 Exercise\nYou’re working for a fitness company that helps people gain muscle mass. They’re currently working on when people should measure their muscle mass: in the morning or in the evening or does that not matter?\nIt’s your job to find out. You know that if differences are off by less than 5%, time of measurement doesn’t matter. If it does matter, though, you need to tell customers that they need to be consistent in when they measure themselves.\nNow you plan the study. It’ll be expensive: You need to provide each person with an advanced measurement scale and pay them to measure once in the morning and once in the evening.\nYou have resources for a maximum of 200 people. Calculate power (95%, you want to be certain) and check whether you need to collect the full sample. Go about it as you find most sensible. (Tip: There’s no correct answer–we simply don’t have enough information.)\n\nsesoi <- 0.05\nsd <- 1\nruns <- 1e3\n\npower <- 0\n\nn <- 2\n  \noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nwhile (power < 0.95 & n < 201) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    \n    difference <- rnorm(n, sesoi, sd)\n    t <- t.test(difference, mu = 0, alternative = \"greater\")\n    \n    pvalues[i] <- t$p.value\n    \n  }\n  \n  power <- sum(pvalues < 0.05) / length(pvalues)\n  \n  outcomes <- rbind(\n    outcomes,\n    data.frame(\n      sample_size = n,\n      power = power\n    )\n  )\n  \n  n <- n + 1\n}\n\nplot(outcomes$sample_size, outcomes$power)\nabline(h = 0.95)"
  },
  {
    "objectID": "content/04-effect-sizes/04-slides-part1.html",
    "href": "content/04-effect-sizes/04-slides-part1.html",
    "title": "Slides - Part 1",
    "section": "",
    "text": "If you want to see the slides in full screen, you can click here. To download them as PDFs, hit the ‘e’ button when you got the presentation open and then Ctrl+P print to PDF. This will work best in Chrome (in Firefox, it didn’t print the headings for me)."
  },
  {
    "objectID": "content/04-effect-sizes/04-slides-part2.html",
    "href": "content/04-effect-sizes/04-slides-part2.html",
    "title": "Slides - Part 2",
    "section": "",
    "text": "If you want to see the slides in full screen, you can click here. To download them as PDFs, hit the ‘e’ button when you got the presentation open and then Ctrl+P print to PDF. This will work best in Chrome (in Firefox, it didn’t print the headings for me)."
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#whats-an-effect-size",
    "href": "content/04-effect-sizes/slides-part1/index.html#whats-an-effect-size",
    "title": "Effect sizes",
    "section": "What’s an effect size",
    "text": "What’s an effect size\n\nSource"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#an-example",
    "href": "content/04-effect-sizes/slides-part1/index.html#an-example",
    "title": "Effect sizes",
    "section": "An example",
    "text": "An example\nAge predicts grumpiness with a large effect. But the sample is too small for significance.\n\nset.seed(1)\nage <- runif(10, 20, 80)\ngrumpiness <- 50 + 0.5 * age + rnorm(10, 0, 20)\n\ncor.test(age, grumpiness)\n\n\n    Pearson's product-moment correlation\n\ndata:  age and grumpiness\nt = 1.5624, df = 8, p-value = 0.1568\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.2100550  0.8533538\nsample estimates:\n      cor \n0.4835198"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#an-example-this-time-larger",
    "href": "content/04-effect-sizes/slides-part1/index.html#an-example-this-time-larger",
    "title": "Effect sizes",
    "section": "An example, this time larger",
    "text": "An example, this time larger\nAge predicts grumpiness with a super tiny effect, but we have a sample of a million, so the effect is significant.\n\nage <- runif(1e6, 20, 80)\ngrumpiness <- 50 + 0.01 * age + rnorm(1e6, 0, 20)\n\ncor.test(age, grumpiness)\n\n\n    Pearson's product-moment correlation\n\ndata:  age and grumpiness\nt = 8.5288, df = 999998, p-value < 2.2e-16\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.006568625 0.010488268\nsample estimates:\n        cor \n0.008528479"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#an-example-this-time-null",
    "href": "content/04-effect-sizes/slides-part1/index.html#an-example-this-time-null",
    "title": "Effect sizes",
    "section": "An example, this time null",
    "text": "An example, this time null\nAge doesn’t predict grumpiness. Can a nonsignificant p-value tell us that?\n\nage <- runif(1e6, 20, 80)\ngrumpiness <- 50 + 0 * age + rnorm(1e6, 0, 20)\n\ncor.test(age, grumpiness)\n\n\n    Pearson's product-moment correlation\n\ndata:  age and grumpiness\nt = 0.093827, df = 999998, p-value = 0.9252\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.001866138  0.002053791\nsample estimates:\n         cor \n9.382711e-05"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#problems-with-nhst",
    "href": "content/04-effect-sizes/slides-part1/index.html#problems-with-nhst",
    "title": "Effect sizes",
    "section": "Problems with NHST",
    "text": "Problems with NHST\n\nDoesn’t answer what we want to know\nThere’ll always be a difference\nNothing special about p = 0.05"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#not-what-we-want-to-know",
    "href": "content/04-effect-sizes/slides-part1/index.html#not-what-we-want-to-know",
    "title": "Effect sizes",
    "section": "Not what we want to know",
    "text": "Not what we want to know\nRemember \\(P(data|H)\\), not \\(P(H|data)\\)?\n\nWe want to know how probable our hypothesis is\nP-values don’t do that\nWrong focus on significance"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#the-typical-h0-is-unrealistic",
    "href": "content/04-effect-sizes/slides-part1/index.html#the-typical-h0-is-unrealistic",
    "title": "Effect sizes",
    "section": "The typical H0 is unrealistic",
    "text": "The typical H0 is unrealistic\n\nMeehl (1991): Everything in the social sciences correlates with everything\nSo-called “crud factor” (Orben and Lakens 2019)\nWith large enough samples, anything will be significant"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#significant-but-trivial",
    "href": "content/04-effect-sizes/slides-part1/index.html#significant-but-trivial",
    "title": "Effect sizes",
    "section": "Significant, but trivial",
    "text": "Significant, but trivial\n\n(Lantz 2013)"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#whats-so-special-about-0.05",
    "href": "content/04-effect-sizes/slides-part1/index.html#whats-so-special-about-0.05",
    "title": "Effect sizes",
    "section": "What’s so special about 0.05?",
    "text": "What’s so special about 0.05?\n\n“…If one in twenty does not seem high enough odds, we may, if we prefer it, draw the line at one in fifty or one in a hundred. Personally, the writer prefers to set a low standard of significance at the 5 per cent point, and ignore entirely all results which fails to reach this level. A scientific fact should be regarded as experimentally established only if a properly designed experiment rarely fails to give this level of significance.”\n\n(Fisher 1926)"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#what-are-we-claiming",
    "href": "content/04-effect-sizes/slides-part1/index.html#what-are-we-claiming",
    "title": "Effect sizes",
    "section": "What are we claiming?",
    "text": "What are we claiming?\n\nSignificance threshold = arbitrary\nEvidential strength clearing that threshold = arbitrary"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#how-not-to-do-it",
    "href": "content/04-effect-sizes/slides-part1/index.html#how-not-to-do-it",
    "title": "Effect sizes",
    "section": "How not to do it",
    "text": "How not to do it\nWe have three independent groups: control, treatment A, and treatment B. The pesky ethics board asks us to do a power analysis. You head to GPower.\nThankfully, there’s a previous study! It had n = 20 per condition and the conditions are only somewhat similar to our planned experiment, but they do report an effect size: \\(\\eta_2 = .21\\). Off to GPower!"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#why-this-this-approach-isnt-ideal",
    "href": "content/04-effect-sizes/slides-part1/index.html#why-this-this-approach-isnt-ideal",
    "title": "Effect sizes",
    "section": "Why this this approach isn’t ideal",
    "text": "Why this this approach isn’t ideal\n\n\nNo idea what \\(\\eta_2 = .21\\) means: Is that a lot?\nThere’s three groups: What’s the effect size for?\nCan I trust the previous study?"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#lets-simulate-that-previous-study",
    "href": "content/04-effect-sizes/slides-part1/index.html#lets-simulate-that-previous-study",
    "title": "Effect sizes",
    "section": "Let’s simulate that “previous study”",
    "text": "Let’s simulate that “previous study”\n\nset.seed(42)\nd <- data.frame(\n  id = 1:60,\n  condition = rep(c(\"control\", \"Treatment A\", \"Treatment B\"), times = 20),\n  score = rnorm(60, mean = c(0, 10, 20), sd = 15)\n)\n\nmodel <- \n  aov(\n    score ~ condition, data = d\n  )\n\neffectsize::eta_squared(model)\n\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\ncondition | 0.21 | [0.06, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00]."
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#notice-something",
    "href": "content/04-effect-sizes/slides-part1/index.html#notice-something",
    "title": "Effect sizes",
    "section": "Notice something?",
    "text": "Notice something?"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#wrong-rituals",
    "href": "content/04-effect-sizes/slides-part1/index.html#wrong-rituals",
    "title": "Effect sizes",
    "section": "Wrong rituals",
    "text": "Wrong rituals\n\nUsing effect sizes like this will get us nowhere\nRituals and rules of thumbs get in the way of understanding\nBut effect sizes might well be the most important part of our research"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#where-it-all-began",
    "href": "content/04-effect-sizes/slides-part1/index.html#where-it-all-began",
    "title": "Effect sizes",
    "section": "Where it all began",
    "text": "Where it all began\n\nCohen (1988)"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#types-of-effect-sizes",
    "href": "content/04-effect-sizes/slides-part1/index.html#types-of-effect-sizes",
    "title": "Effect sizes",
    "section": "Types of effect sizes",
    "text": "Types of effect sizes\n\nDifferences between groups (e.g., Cohen’s \\(d\\))\nStrength of association (e.g., Pearson’s \\(r\\), \\(R^2\\), \\(\\eta^2\\))\nEstimates of risks (e.g., relative risks, odds ratios)"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#differences",
    "href": "content/04-effect-sizes/slides-part1/index.html#differences",
    "title": "Effect sizes",
    "section": "Differences",
    "text": "Differences\n\nExpress difference between groups in variance units, not raw units\nNot “How many cm is the difference in height between the groups”\nBut “How many standard deviations difference in height between the groups”\n\n\\(d = \\frac{M_1-M_2}{pooled\\ \\sigma}\\)\n\\(pooled\\ \\sigma = \\sqrt{\\frac{(sd_1^2 + sd_2^2)}{2}}\\)"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#poor-cohen",
    "href": "content/04-effect-sizes/slides-part1/index.html#poor-cohen",
    "title": "Effect sizes",
    "section": "Poor Cohen",
    "text": "Poor Cohen"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#an-example-1",
    "href": "content/04-effect-sizes/slides-part1/index.html#an-example-1",
    "title": "Effect sizes",
    "section": "An example",
    "text": "An example\nControl group has a mean of 100 and an SD of 20. The treatment group has a mean of 105 and an SD of 10. The difference in the means is \\(105-100 = 5\\) (simplified). The pooled SD is (simplified!) \\(\\frac{20+10}{2} = 15\\). So our difference is \\(5/15\\) or simply \\(d = 0.33\\). In other words, our difference is a third of a standard deviation unit."
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#so",
    "href": "content/04-effect-sizes/slides-part1/index.html#so",
    "title": "Effect sizes",
    "section": "So…",
    "text": "So…\nCohen suggested (and later very much regretted) some rules of thumb if a researcher has no better idea:\n\n\\(d = 0.20\\) is a small effect: New lines of research, experiments aren’t that sophisticated yet\n\\(d = 0.50\\) is a medium effect: Visible to the naked eye\n\\(d = 0.80\\) is a large effect: Almost half of distributions aren’t overlapping"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#a-word-of-warning",
    "href": "content/04-effect-sizes/slides-part1/index.html#a-word-of-warning",
    "title": "Effect sizes",
    "section": "A word of warning",
    "text": "A word of warning\nIn small samples, Cohen’s d will be biased. Use Hedge’s g instead. In fact, you should probably always use g. (Software does it for you anyway.)\n\\(d = \\frac{M_1-M_2}{pooled\\ \\sigma^*}\\)"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#strength-of-association",
    "href": "content/04-effect-sizes/slides-part1/index.html#strength-of-association",
    "title": "Effect sizes",
    "section": "Strength of association",
    "text": "Strength of association\n\nExpress the strength of association as a regression slope when both variables have been standardized\nNot “How many points does grumpiness go up with one extra year”\nBut “How many standard deviations does grumpiness go up with one extra standard deviation of age”\n\n\\(r = B_{xy} \\frac{\\sigma_x}{\\sigma_y}\\)"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#an-example-2",
    "href": "content/04-effect-sizes/slides-part1/index.html#an-example-2",
    "title": "Effect sizes",
    "section": "An example",
    "text": "An example\nWe predict grumpiness with age. The regression slope is 2: With each year, people score 2 higher on grumpiness. The SD of grumpiness is 30. The SD of age is 10. The correlation coefficient is \\(2*10/30 = .67\\). We could’ve also just standardized both variables and run a regression."
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#so-1",
    "href": "content/04-effect-sizes/slides-part1/index.html#so-1",
    "title": "Effect sizes",
    "section": "So…",
    "text": "So…\n\n\\(r = 0.10\\) is a small effect: Cohen believed the majority of effects in the “soft” sciences are in this range\n\\(r = 0.30\\) is a medium effect: Visible to the naked eye to a “reasonably sensitive observer”\n\\(r = 0.50\\) is a large effect: “About as high as they come”"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#translating-between-the-two",
    "href": "content/04-effect-sizes/slides-part1/index.html#translating-between-the-two",
    "title": "Effect sizes",
    "section": "Translating between the two",
    "text": "Translating between the two\nCohen also provides a formula how to get \\(r\\) from \\(d\\). Remember, use Hedge’s \\(g\\) instead of \\(d\\).\n\\(r = \\frac{d}{\\sqrt{d^2 + 4}}\\)\nBack to that medium effect size:\n\\(r = \\frac{0.5}{\\sqrt{0.5^2 + 4}} = 0.24\\)"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#variance-explained",
    "href": "content/04-effect-sizes/slides-part1/index.html#variance-explained",
    "title": "Effect sizes",
    "section": "Variance explained",
    "text": "Variance explained\nStrength of association is just another way of saying magnitude of shared variance between variables. Or: Does the blue line do better than the black line?"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#variance-explained-1",
    "href": "content/04-effect-sizes/slides-part1/index.html#variance-explained-1",
    "title": "Effect sizes",
    "section": "Variance explained",
    "text": "Variance explained\n\nProportion of unexplained variance (residuals) in relation to total variance\nFor \\(r\\), this is easy to calculate if we only have two variables\n\\(r^2\\) tells us the proportion of variance we can explain = \\(R^2\\)\n\n\\(Variance \\ explained = \\frac{\\sigma_{effect}}{\\sigma_{total}}\\)"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#what-about-our-conventions",
    "href": "content/04-effect-sizes/slides-part1/index.html#what-about-our-conventions",
    "title": "Effect sizes",
    "section": "What about our conventions?",
    "text": "What about our conventions?\n\n\\(r^2 = 0.10^2 = 1\\%\\) is a small effect: Cohen believed the majority of effects in the “soft” sciences are in this range\n\\(r^2 = 0.30^2 = 9\\%\\) is a medium effect: Visible to the naked eye to a “reasonably sensitive observer”\n\\(r^2 = 0.50^2 = 25\\%\\) is a large effect: “About as high as they come”"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#thank-you-spss",
    "href": "content/04-effect-sizes/slides-part1/index.html#thank-you-spss",
    "title": "Effect sizes",
    "section": "Thank you, SPSS",
    "text": "Thank you, SPSS\nIn the ANOVA context, we often use \\(\\eta^2\\), because it has been standard in SPSS output (Lakens 2013).\n\\(\\eta^2 = \\frac{SS_{effect}}{SS_{total}}\\)\n\nTells us, once again, what % of variance is accounted for by group membership\nStraightforward with two variables (group membership and outcome)"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#insert-confusion",
    "href": "content/04-effect-sizes/slides-part1/index.html#insert-confusion",
    "title": "Effect sizes",
    "section": "Insert confusion",
    "text": "Insert confusion\n\n\\(\\eta^2_p = \\frac{SS_{effect}}{SS_{total} + SS_{error}}\\)\n\nIf there’s more than one predictor, gives us the effect size per predictor\nSo one effect size indicator for main effect(s) and interactions (Levine and Hullett 2002)"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#all-the-same",
    "href": "content/04-effect-sizes/slides-part1/index.html#all-the-same",
    "title": "Effect sizes",
    "section": "All the same?",
    "text": "All the same?\n\nWhen there’s only one predictor, \\(\\eta^2\\), \\(\\eta^2_p\\), and \\(R^2\\) are the same: Variance accounted for by effect\nWhen there’s multiple effects, you can state variance explained for the entire model or invidual effects\nMultiple effects require overall model (\\(R^2\\)) and individual effect estimates (\\(\\eta^2_p\\), partial \\(R^2\\))"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#are-we-done-please",
    "href": "content/04-effect-sizes/slides-part1/index.html#are-we-done-please",
    "title": "Effect sizes",
    "section": "Are we done, please?",
    "text": "Are we done, please?\n\\(f\\) mostly used for one-way ANOVAs\n\nA measure of how wide means are spread in ANOVA relative to variation within groups\nCut-offs suggested by Cohen: 0.10, 0.25, 0.40\n\n\\(f^2\\) mostly used for regressions, but also one-way, or multi-way ANOVAs\n\nAgain a measure of how much variance an effect (just easier to work with squared values)\nCut-offs suggested by Cohen: .02, .15, .35"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#corrections",
    "href": "content/04-effect-sizes/slides-part1/index.html#corrections",
    "title": "Effect sizes",
    "section": "Corrections",
    "text": "Corrections\nThese effect sizes of shared variance are often biased. Instead, use \\(\\omega^2\\) or \\(\\epsilon^2\\). Don’t panic: Smart people have provided spreadsheets.\nEffect size converter: https://osf.io/vbdah/"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#my-head-is-spinning",
    "href": "content/04-effect-sizes/slides-part1/index.html#my-head-is-spinning",
    "title": "Effect sizes",
    "section": "My head is spinning",
    "text": "My head is spinning\nAll you need to remember:\n\nEffect sizes can be for differences between two groups (\\(d\\))\nEffect sizes can be for strength of associations (\\(r\\), \\(R^2\\), \\(\\eta^2\\), \\(\\eta^2_p\\), \\(f\\), \\(f^2\\))\nEvery effect size can be transformed into one another\nCut-offs are really arbitrary"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#about-squaring-things",
    "href": "content/04-effect-sizes/slides-part1/index.html#about-squaring-things",
    "title": "Effect sizes",
    "section": "About squaring things",
    "text": "About squaring things\n\nHalf of a perfect correlation (\\(r\\) = 1.00, \\(r^2\\) = 100%) is \\(r\\) = 0.50, \\(r^2\\) = 25%\nWhy are we interested in variance and not standard deviations all of a sudden\nMight be useful for model fit, but less intuitive for individual effect\n\n\nSquaring the r is not merely uninformative; for purposes of evaluating effect size, the practice is actively misleading. (Funder and Ozer 2019, 3)"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#about-squaring-things-1",
    "href": "content/04-effect-sizes/slides-part1/index.html#about-squaring-things-1",
    "title": "Effect sizes",
    "section": "About squaring things",
    "text": "About squaring things\nThe moment we move beyond two groups or bivariate relationships:\n\nVariance explained can mean almost any pattern\nOur hypotheses are rarely about partial effects or total model variance\nReporting them isn’t really informative\n\n\nAs a rule, reports of effect size should focus on 1 df effects. (Baguley 2009, 614)"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#so-what-effect-sizes-are-typical",
    "href": "content/04-effect-sizes/slides-part1/index.html#so-what-effect-sizes-are-typical",
    "title": "Effect sizes",
    "section": "So what effect sizes are typical?",
    "text": "So what effect sizes are typical?\n\n\n\n708 correlations from Personality Psychology\n25th, 50th, and 75th percentiles = \\(r\\) of 0.11, 0.19, and 0.29\n< 3% of correlations were large (aka 0.50 or larger)\n\n\n\n(Gignac and Szodorai 2016)"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#so-what-effect-sizes-are-typical-1",
    "href": "content/04-effect-sizes/slides-part1/index.html#so-what-effect-sizes-are-typical-1",
    "title": "Effect sizes",
    "section": "So what effect sizes are typical?",
    "text": "So what effect sizes are typical?\n\n26,841 effects from cognitive neuroscience and psychology\nMedian \\(d\\) for significant results: 0.93\nMedian \\(d\\) for nonsignificant results: 0.24\n\n\n(Szucs and Ioannidis 2017)"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#so-what-effect-sizes-are-typical-2",
    "href": "content/04-effect-sizes/slides-part1/index.html#so-what-effect-sizes-are-typical-2",
    "title": "Effect sizes",
    "section": "So what effect sizes are typical?",
    "text": "So what effect sizes are typical?\n\n\n\n12,170 \\(r\\)s and 6,447 \\(d\\)s from 134 meta-analyses\n25th, 50th, and 75th percentiles =\\(r\\) of 0.12, 0.24, and 0.41\n\\(d\\) of 0.15, 0.36, and 0.65\n\n\n\n(Lovakov and Agadullina 2021)"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#and-in-communication",
    "href": "content/04-effect-sizes/slides-part1/index.html#and-in-communication",
    "title": "Effect sizes",
    "section": "And in communication?",
    "text": "And in communication?\n\n(Rains, Levine, and Weber 2018)"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#getting-a-feel",
    "href": "content/04-effect-sizes/slides-part1/index.html#getting-a-feel",
    "title": "Effect sizes",
    "section": "Getting a feel",
    "text": "Getting a feel\nSo… is \\(r\\) = .21 big then? (Meyer et al. 2001)\n\nExtent of social support and enhanced immune functioning: .21\nQuality of parents’ marital relationship and quality of parent-child relationship: .22\nEffect of alcohol on aggressive behavior: .23"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#getting-too-much-of-a-feel",
    "href": "content/04-effect-sizes/slides-part1/index.html#getting-too-much-of-a-feel",
    "title": "Effect sizes",
    "section": "Getting too much of a feel",
    "text": "Getting too much of a feel\n\nViolent video game vs. racing game condition: \\(d\\) = 3.46 (Hilgard 2021)\nCancer-prone personality 121 times more likely to die of disease ( source)\nMassive effect sizes are often a sign that something fishy is going on"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#heard-of-the-replication-crisis",
    "href": "content/04-effect-sizes/slides-part1/index.html#heard-of-the-replication-crisis",
    "title": "Effect sizes",
    "section": "Heard of the replication crisis?",
    "text": "Heard of the replication crisis?\n\n\n\n(Open Science Collaboration 2015)\n\n\n(Fanelli 2012)"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#a-good-bad-example",
    "href": "content/04-effect-sizes/slides-part1/index.html#a-good-bad-example",
    "title": "Effect sizes",
    "section": "A good bad example",
    "text": "A good bad example\n\n(De Vries et al. 2018)"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#were-likely-overestimating",
    "href": "content/04-effect-sizes/slides-part1/index.html#were-likely-overestimating",
    "title": "Effect sizes",
    "section": "We’re likely overestimating",
    "text": "We’re likely overestimating\n\n(Schäfer and Schwarz 2019)"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#crud",
    "href": "content/04-effect-sizes/slides-part1/index.html#crud",
    "title": "Effect sizes",
    "section": "Crud",
    "text": "Crud\nWhen we correlate variables that are specifically selected not to be related, we still reach \\(r\\) ~ .10.\n\n(Ferguson and Heene 2021)"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#okay-how-about-pilots",
    "href": "content/04-effect-sizes/slides-part1/index.html#okay-how-about-pilots",
    "title": "Effect sizes",
    "section": "Okay, how about pilots?",
    "text": "Okay, how about pilots?\n\n\n\nPilots are small and small studies have more variability\nSo we’ll often land on effects that will require massive samples\nIf those exceed our means, we run into follow-up bias\nGetting effect sizes from pilots not a good idea\n\n\n\n(Albers and Lakens 2018)"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#so-what-shall-we-do",
    "href": "content/04-effect-sizes/slides-part1/index.html#so-what-shall-we-do",
    "title": "Effect sizes",
    "section": "So what shall we do?",
    "text": "So what shall we do?\nSeveral considerations (Funder and Ozer 2019):\n\nCompare to classical studies?\nField in general?\nOther benchmarks?\nCumulative or not?"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#sesoi",
    "href": "content/04-effect-sizes/slides-part1/index.html#sesoi",
    "title": "Effect sizes",
    "section": "SESOI",
    "text": "SESOI\nSmallest effect size of interest (Anvari et al. 2021)\n\nWhy rely on previous research that is notoriously unreliable?\nYou should define what effect you find worth looking for\nAt what point do you not care about an effect anymore?\nMake falsifiable and testable studies"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#tradition",
    "href": "content/04-effect-sizes/slides-part1/index.html#tradition",
    "title": "Effect sizes",
    "section": "Tradition",
    "text": "Tradition\nMinimally detectable difference\n\nSmallest increase in an outcome that we care about\nPain, surgery, etc.\nAnywhere where we need to balance not just theory, but also limited resources"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#how-do-i-determine-the-sesoi",
    "href": "content/04-effect-sizes/slides-part1/index.html#how-do-i-determine-the-sesoi",
    "title": "Effect sizes",
    "section": "How do I determine the SESOI?",
    "text": "How do I determine the SESOI?\n\nObjective benchmarks (e.g., half an SD for health outcomes)\nSame considerations: In relation to field, time frame, etc.\nMaximum positive control\nCost benefit analysis\nEmpirical benchmarks"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#cost-benefit",
    "href": "content/04-effect-sizes/slides-part1/index.html#cost-benefit",
    "title": "Effect sizes",
    "section": "Cost-benefit",
    "text": "Cost-benefit\nOften used in medicine:\n\nWe know the effect of one drug\nOur effect becomes same size for less resources\nOr more than half the effect for half the resources"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#empirical-benchmarks",
    "href": "content/04-effect-sizes/slides-part1/index.html#empirical-benchmarks",
    "title": "Effect sizes",
    "section": "Empirical benchmarks",
    "text": "Empirical benchmarks\n\n\n\n\n\nWhat’s the performance gap between low and high performers in school?\nThat’s the minimum effect we want to achieve\nAnything less is uninteresting and we should invest our resources somewhere else\n\n(Hill et al. 2008)"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#empirical-benchmarks-1",
    "href": "content/04-effect-sizes/slides-part1/index.html#empirical-benchmarks-1",
    "title": "Effect sizes",
    "section": "Empirical benchmarks",
    "text": "Empirical benchmarks\n\nWhat’s the expected growth that would naturally occur?\nExample: Reading ability from one grade to the next\nWe want to achieve an effect of at least that size as our SESOI\n\n(Hill et al. 2008)"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#empirical-benchmarks-2",
    "href": "content/04-effect-sizes/slides-part1/index.html#empirical-benchmarks-2",
    "title": "Effect sizes",
    "section": "Empirical benchmarks",
    "text": "Empirical benchmarks\nGlobal ratings of change methods:\n\nComes from medicine\nPsychological states are inherently subjective\nSo we need to rely on people informing us when they can feel a difference"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#empirical-benchmarks-3",
    "href": "content/04-effect-sizes/slides-part1/index.html#empirical-benchmarks-3",
    "title": "Effect sizes",
    "section": "Empirical benchmarks",
    "text": "Empirical benchmarks\nProcedure (Anvari and Lakens 2021):\n\nAsk participants how they feel\nPerform intervention\nAsk them again how they feel\nAsk whether it has gotten better or not\nLook at the average difference in scores for those who say there’s improvement"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#empirical-benchmarks-4",
    "href": "content/04-effect-sizes/slides-part1/index.html#empirical-benchmarks-4",
    "title": "Effect sizes",
    "section": "Empirical benchmarks",
    "text": "Empirical benchmarks"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#changes-my-interpretation-and-conclusions",
    "href": "content/04-effect-sizes/slides-part1/index.html#changes-my-interpretation-and-conclusions",
    "title": "Effect sizes",
    "section": "Changes my interpretation and conclusions",
    "text": "Changes my interpretation and conclusions\n\nMy study has 80% power to detect a medium sized effect, as shown by the meta-analysis by XYZ.\n\nTranslation: If this doesn’t work, we have learned close to nothing.\n\nI designed my study to be able to detect an effect of a certain size with 95% power. Anything smaller than that is uninteresting. Don’t waste resources if you’re hoping to find an effect this large.\n\nTranslation: I thought about what I want and I’m putting that part of the process up for debate."
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#maximum-positive-controls-hilgard2021",
    "href": "content/04-effect-sizes/slides-part1/index.html#maximum-positive-controls-hilgard2021",
    "title": "Effect sizes",
    "section": "Maximum positive controls (Hilgard 2021)",
    "text": "Maximum positive controls (Hilgard 2021)\n\nProduce the largest effect you possibly can\nTell participants to imagine what would happen (aka induce demand artifacts)\nPuts a limit on the maximum effect you can expect"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#on-what-scale",
    "href": "content/04-effect-sizes/slides-part1/index.html#on-what-scale",
    "title": "Effect sizes",
    "section": "On what scale",
    "text": "On what scale\nUnstandardized measures have several advantages:\n\nScale independent of variance\nMore intuitive and easier to understand\nLess prone to error in calculation\n\n(Baguley 2009)"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#raw-for-the-win",
    "href": "content/04-effect-sizes/slides-part1/index.html#raw-for-the-win",
    "title": "Effect sizes",
    "section": "Raw for the win",
    "text": "Raw for the win\n\nStandardized effects can be helpful in comparison or initial explorations\nBut standard deviations aren’t objective units that just happen\nRaw effect sizes force you to put a number on things and think about whether you know enough for a confirmatory study"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#references",
    "href": "content/04-effect-sizes/slides-part1/index.html#references",
    "title": "Effect sizes",
    "section": "References",
    "text": "References\n\n\nAlbers, Casper J., and Daniël Lakens. 2018. “When Power Analyses Based on Pilot Data Are Biased: Inaccurate Effect Size Estimators and Follow-up Bias.” Journal of Experimental Social Psychology 74: 187–95. https://doi.org/10.17605/OSF.IO/B7Z4Q.\n\n\nAnvari, Farid, Rogier Kievit, Daniel Lakens, Andrew K. Przybylski, Leo Tiokhin, Brenton M. Wiernik, and Amy Orben. 2021. “Evaluating the Practical Relevance of Observed Effect Sizes in Psychological Research,” June. https://doi.org/10.31234/osf.io/g3vtr.\n\n\nAnvari, Farid, and Daniël Lakens. 2021. “Using Anchor-Based Methods to Determine the Smallest Effect Size of Interest.” Journal of Experimental Social Psychology 96 (September): 104159. https://doi.org/10.1016/j.jesp.2021.104159.\n\n\nBaguley, Thom. 2009. “Standardized or Simple Effect Size: What Should Be Reported?” British Journal of Psychology 100 (3): 603–17. https://doi.org/10.1348/000712608X377117.\n\n\nCohen, J. 1988. Statistical Power Analysis for the Behavioral Sciences. 2nd ed. Hillsdale, NJ: Lawrence Erlbaum.\n\n\nDe Vries, Y. A., A. M. Roest, Peter de Jonge, Pim Cuijpers, M. R. Munafò, and J. A. Bastiaansen. 2018. “The Cumulative Effect of Reporting and Citation Biases on the Apparent Efficacy of Treatments: The Case of Depression.” Psychological Medicine 48 (15): 24532455.\n\n\nFanelli, Daniele. 2012. “Negative Results Are Disappearing from Most Disciplines and Countries.” Scientometrics 90 (3): 891–904. https://doi.org/10.1007/s11192-011-0494-7.\n\n\nFerguson, Christopher J., and Moritz Heene. 2021. “Providing a Lower-Bound Estimate for Psychology’s “Crud Factor”: The Case of Aggression.” Professional Psychology: Research and Practice 52 (6): 620–26. https://doi.org/10.1037/pro0000386.\n\n\nFisher, Ronald A. 1926. “The Arrangement of Field Experiments.” Journal of the Ministry of Agriculture 33: 503–15.\n\n\nFunder, David C., and Daniel J. Ozer. 2019. “Evaluating Effect Size in Psychological Research: Sense and Nonsense.” Advances in Methods and Practices in Psychological Science 2 (2): 156–68. https://doi.org/10.1177/2515245919847202.\n\n\nGignac, Gilles E., and Eva T. Szodorai. 2016. “Effect Size Guidelines for Individual Differences Researchers.” Personality and Individual Differences 102 (November): 74–78. https://doi.org/10.1016/j.paid.2016.06.069.\n\n\nHilgard, Joseph. 2021. “Maximal Positive Controls: A Method for Estimating the Largest Plausible Effect Size.” Journal of Experimental Social Psychology 93 (March): 104082. https://doi.org/10.1016/j.jesp.2020.104082.\n\n\nHill, Carolyn J., Howard S. Bloom, Alison Rebeck Black, and Mark W. Lipsey. 2008. “Empirical Benchmarks for Interpreting Effect Sizes in Research.” Child Development Perspectives 2 (3): 172–77. https://doi.org/10.1111/j.1750-8606.2008.00061.x.\n\n\nLakens, Daniël. 2013. “Calculating and Reporting Effect Sizes to Facilitate Cumulative Science: A Practical Primer for t-Tests and ANOVAs.” Frontiers in Psychology 4 (NOV): 1–12. https://doi.org/10.3389/fpsyg.2013.00863.\n\n\nLantz, Björn. 2013. “The Large Sample Size Fallacy.” Scandinavian Journal of Caring Sciences 27 (2): 487–92. https://doi.org/10.1111/j.1471-6712.2012.01052.x.\n\n\nLevine, Timothy R., and Craig R. Hullett. 2002. “Eta Squared, Partial Eta Squared, and Misreporting of Effect Size in Communication Research.” Human Communication Research 28 (4): 612–25. https://doi.org/10.1111/j.1468-2958.2002.tb00828.x.\n\n\nLovakov, Andrey, and Elena R. Agadullina. 2021. “Empirically Derived Guidelines for Effect Size Interpretation in Social Psychology.” European Journal of Social Psychology 51 (3): 485–504. https://doi.org/10.1002/ejsp.2752.\n\n\nMeyer, Gregory J., Stephen E. Finn, Lorraine D. Eyde, Gary G. Kay, Kevin L. Moreland, Robert R. Dies, Elena J. Eisman, Tom W. Kubiszyn, and Geoffrey M. Reed. 2001. “Psychological Testing and Psychological Assessment: A Review of Evidence and Issues.” American Psychologist 56 (2): 128.\n\n\nOpen Science Collaboration. 2015. “Estimating the Reproducibility of Psychological Science.” Science 349 (6251): aac4716–16. https://doi.org/10.1126/science.aac4716.\n\n\nOrben, Amy, and Daniel Lakens. 2019. “Crud (Re)defined,” May. https://doi.org/10.31234/osf.io/96dpy.\n\n\nRains, Stephen A., Timothy R. Levine, and Rene Weber. 2018. “Sixty Years of Quantitative Communication Research Summarized: Lessons from 149 Meta-Analyses.” Annals of the International Communication Association 8985: 1–20. https://doi.org/10.1080/23808985.2018.1446350.\n\n\nSchäfer, Thomas, and Marcus A. Schwarz. 2019. “The Meaningfulness of Effect Sizes in Psychological Research: Differences Between Sub-Disciplines and the Impact of Potential Biases.” Frontiers in Psychology 10. https://doi.org/10.3389/fpsyg.2019.00813.\n\n\nSzucs, Denes, and John P. A. Ioannidis. 2017. “Empirical Assessment of Published Effect Sizes and Power in the Recent Cognitive Neuroscience and Psychology Literature.” PLoS Biology 15 (3). https://doi.org/10.1371/journal.pbio.2000797.\n\n\n\n\nHome"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part2/index.html#so-far",
    "href": "content/04-effect-sizes/slides-part2/index.html#so-far",
    "title": "Interlude: Correlated measures",
    "section": "So far",
    "text": "So far\nSo far we’ve been drawing samples from two independent groups.\n\nset.seed(42)\n\ncontrol <- rnorm(100)\ntreatment <- rnorm(100, 0.2)\n\nt.test(control, treatment)\n\n\n    Welch Two Sample t-test\n\ndata:  control and treatment\nt = -0.58009, df = 194.18, p-value = 0.5625\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.3519980  0.1919951\nsample estimates:\n mean of x  mean of y \n0.03251482 0.11251629"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part2/index.html#what-about-two-measures-from-the-same-person",
    "href": "content/04-effect-sizes/slides-part2/index.html#what-about-two-measures-from-the-same-person",
    "title": "Interlude: Correlated measures",
    "section": "What about two measures from the same person",
    "text": "What about two measures from the same person\n\nThink of a typical pre/posttest design\nSomeone who has a tendency to score low will therefore score low on both pre- and post-test measure\nThe measures are correlated\nWe need to take into account that measures come from the same unit when simulating"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part2/index.html#so-how-do-we-get-correlated-measures",
    "href": "content/04-effect-sizes/slides-part2/index.html#so-how-do-we-get-correlated-measures",
    "title": "Interlude: Correlated measures",
    "section": "So how do we get correlated measures?",
    "text": "So how do we get correlated measures?\n\nWe need to increase the dimensions\nSo far, we’ve worked with one dimension: our dependent variable only\nBut if a person has multiple measures, that means we don’t just have one normal distribution\nWe have two correlated normal distributions: a multivariate normal distribution"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part2/index.html#how-does-that-look-like",
    "href": "content/04-effect-sizes/slides-part2/index.html#how-does-that-look-like",
    "title": "Interlude: Correlated measures",
    "section": "How does that look like?",
    "text": "How does that look like?\nFor univariate, we pick from a single value (left). For bivariate, we pick two values, or a point on the the plane (right).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFor the left, we need a mean and SD. What do we need for the right?"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part2/index.html#what-goes-into-a-multivariate-distribution",
    "href": "content/04-effect-sizes/slides-part2/index.html#what-goes-into-a-multivariate-distribution",
    "title": "Interlude: Correlated measures",
    "section": "What goes into a multivariate distribution",
    "text": "What goes into a multivariate distribution\nEverything’s double:\n\n2 means\n2 SDs\nCorrelation between variables\nAn SD for the entire “mountain”"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part2/index.html#sd-variance-covariance-matrix",
    "href": "content/04-effect-sizes/slides-part2/index.html#sd-variance-covariance-matrix",
    "title": "Interlude: Correlated measures",
    "section": "SD = Variance-covariance matrix",
    "text": "SD = Variance-covariance matrix\nThe SD for the “mountain” is just the SDs and correlations between the two variables in one place so that we can draw our data from them.\n\\[\n\\begin{bmatrix}\nvar  & cov \\\\\ncov & var \\\\\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part2/index.html#this-isnt-new",
    "href": "content/04-effect-sizes/slides-part2/index.html#this-isnt-new",
    "title": "Interlude: Correlated measures",
    "section": "This isn’t new",
    "text": "This isn’t new\nAll of you have done correlation tables: they’re just standardized versions of the variance-covariance matrix.\n\\[\n\\begin{bmatrix}\nSD  & r \\\\\nr & SD \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1  & 0.5 \\\\\n0.5 & 1 \\\\\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part2/index.html#so-how-do-we-make-this",
    "href": "content/04-effect-sizes/slides-part2/index.html#so-how-do-we-make-this",
    "title": "Interlude: Correlated measures",
    "section": "So how do we make this?",
    "text": "So how do we make this?\n\\[\n\\begin{bmatrix}\nvar  & cov \\\\\ncov & var \\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n?  & ? \\\\\n? & ? \\\\\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part2/index.html#our-values",
    "href": "content/04-effect-sizes/slides-part2/index.html#our-values",
    "title": "Interlude: Correlated measures",
    "section": "Our values",
    "text": "Our values\nSay we have an experiment where people give us a baseline measure, then the treatment happens, and we get a post-treatment measures. The measures are normally distributed with means of 10 and 10.5 and SDs of 1.5 and 2. The pre- and post-measure are correlated with \\(r\\) = 0.4.\n\nmeans <- c(pre = 10, post = 10.5)\npre_sd <- 1.5\npost_sd <- 2\ncorrelation <- 0.4"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part2/index.html#getting-variance-and-covariances",
    "href": "content/04-effect-sizes/slides-part2/index.html#getting-variance-and-covariances",
    "title": "Interlude: Correlated measures",
    "section": "Getting variance and covariances",
    "text": "Getting variance and covariances\nSD is just the square root of the variance. So we go \\(Var = sd^2\\) and we got our variance.\nCovariance is just the correlation times the SDs. So we go \\(covariance = r(pre, post) \\times sd_{pre} \\times sd_{post}\\)\n\nvar_pre <- pre_sd**2\nvar_post <- post_sd**2\n\ncovariance <- correlation * pre_sd * post_sd"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part2/index.html#now-lets-combine-all-that-into-a-matrix",
    "href": "content/04-effect-sizes/slides-part2/index.html#now-lets-combine-all-that-into-a-matrix",
    "title": "Interlude: Correlated measures",
    "section": "Now let’s combine all that into a matrix",
    "text": "Now let’s combine all that into a matrix\n\nour_matrix <- matrix(\n  c(var_pre, covariance, \n    covariance, var_post),\n  ncol = 2\n)\n\nour_matrix\n\n     [,1] [,2]\n[1,] 2.25  1.2\n[2,] 1.20  4.0"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part2/index.html#ready-to-simulate-now",
    "href": "content/04-effect-sizes/slides-part2/index.html#ready-to-simulate-now",
    "title": "Interlude: Correlated measures",
    "section": "Ready to simulate now",
    "text": "Ready to simulate now\nWe use the mvrnorm function for, well, multivariate normal distributions, from the MASS package. Let’s get a massive sample of 10,000 people.\n\nlibrary(MASS)\n\nd <- \n  mvrnorm(\n    10000,\n    means,\n    our_matrix\n  )\nd <- as.data.frame(d)\nhead(d)\n\n        pre      post\n1  9.750371  7.936974\n2 10.001516 10.519466\n3 11.279047 10.630203\n4 11.081401  8.563613\n5 10.086696  9.591592\n6 11.432761 10.631188"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part2/index.html#lets-check-that",
    "href": "content/04-effect-sizes/slides-part2/index.html#lets-check-that",
    "title": "Interlude: Correlated measures",
    "section": "Let’s check that",
    "text": "Let’s check that\nLet’s first check the sample to see whether we can recover our numbers.\n\nmean(d$pre); mean(d$post)\n\n[1] 10.00843\n\n\n[1] 10.48749\n\nsd(d$pre); sd(d$post)\n\n[1] 1.496285\n\n\n[1] 2.004229\n\ncor(d$pre, d$post)\n\n[1] 0.4119397"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part2/index.html#run-our-test",
    "href": "content/04-effect-sizes/slides-part2/index.html#run-our-test",
    "title": "Interlude: Correlated measures",
    "section": "Run our test",
    "text": "Run our test\n\nt.test(\n  d$pre,\n  d$post,\n  paired = TRUE\n)\n\n\n    Paired t-test\n\ndata:  d$pre and d$post\nt = -24.623, df = 9999, p-value < 2.2e-16\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -0.5171917 -0.4409192\nsample estimates:\nmean difference \n     -0.4790554"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part2/index.html#the-while-operator",
    "href": "content/04-effect-sizes/slides-part2/index.html#the-while-operator",
    "title": "Interlude: Correlated measures",
    "section": "The while operator",
    "text": "The while operator\nAt this point, we’ve worked with for loops and went from a minimum to a maximum. If that maximum is large, that can take quite some time. You can also consider the while function to stop when you’ve reached the point you want to be at.\n Source"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part2/index.html#easy-example",
    "href": "content/04-effect-sizes/slides-part2/index.html#easy-example",
    "title": "Interlude: Correlated measures",
    "section": "Easy example",
    "text": "Easy example\n\ni <- 1\n\nwhile (i < 5) {\n  print(i)\n  \n  i <- i + 1\n}\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part2/index.html#how-would-that-work-for-our-purposes",
    "href": "content/04-effect-sizes/slides-part2/index.html#how-would-that-work-for-our-purposes",
    "title": "Interlude: Correlated measures",
    "section": "How would that work for our purposes?",
    "text": "How would that work for our purposes?\n\ndraws <- 1e3\nn <- 60\neffect_size <- 0.5\n\nd <- data.frame(\n  sample_size = NULL,\n  power = NULL\n)\n\npower <- 0\n\nwhile (power<0.95) {\n  \n  pvalues <- NULL\n  for (i in 1:draws) {\n    control <- rnorm(n)\n    treatment <- rnorm(n, effect_size)\n    t <- t.test(control, treatment, alternative = \"less\")\n    \n    pvalues[i] <- t$p.value\n  }\n  \n  power <- sum(pvalues<0.05) / length(pvalues)\n  \n  d <- rbind(\n    d,\n    data.frame(\n      sample_size = n,\n      power = power\n    )\n  )\n  \n  n <- n + 1\n}"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part2/index.html#what-did-we-just-do",
    "href": "content/04-effect-sizes/slides-part2/index.html#what-did-we-just-do",
    "title": "Interlude: Correlated measures",
    "section": "What did we just do?",
    "text": "What did we just do?\n\nhead(d)\n\n  sample_size power\n1          60 0.845\n2          61 0.856\n3          62 0.872\n4          63 0.854\n5          64 0.885\n6          65 0.874\n\nplot(d$sample_size, d$power)\nabline(h = 0.95)"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/05-exercise.html#exercise",
    "href": "content/05-alpha-beta-sensitivity/05-exercise.html#exercise",
    "title": "Exercise III",
    "section": "0.1 Exercise",
    "text": "0.1 Exercise\nThe False Positive Rate is the proportion of false positive findings among all positive (aka signifiant) findings. It’s defined as follows:\n\\[\\begin{align}\nFalse \\ positive \\ rate = \\frac{False \\ positives}{False \\ positives + True \\ positives}\\\\\\\\\nFalse \\ positives = \\phi * \\alpha\\\\\nTrue \\ positives = power * (1 - \\phi)\\\\\\\\\nFalse \\ positive \\ rate = \\frac{\\phi * \\alpha}{\\phi * \\alpha + power * (1 - \\phi)}\n\\end{align}\\]\n\\(\\phi\\) is the proportion of null hypotheses, in general in a field, that are true, \\(\\alpha\\) your false positive error rate, and power is \\((1-\\beta)\\).\nPlot how the false positive rate develops as \\(\\phi\\) goes from 0 to 1 for two \\(\\alpha\\) levels (.05 and .01.) and two levels of power (80% and 95%). No need for a simulation here. You can just straight up use the formula above to calculate the false positive rate. For that, it’s probably easiest to create a data.frame. Try out the expand.grid command which creates a data frame of all combinations of several variables. For example:\n\niq_scores <- seq(100, 105, 1)\nsample_size <- c(10, 20)\nd <- expand.grid(iq_scores, sample_size)\n\nd\n\n   Var1 Var2\n1   100   10\n2   101   10\n3   102   10\n4   103   10\n5   104   10\n6   105   10\n7   100   20\n8   101   20\n9   102   20\n10  103   20\n11  104   20\n12  105   20\n\n\nYou can use the following code (make sure the variables are named accordingly):\n\nlibrary(ggplot2)\nggplot(d, aes(x = phis, y = fpr, color = as.factor(alphas))) + geom_line() + facet_wrap(~ power) + theme_bw()\n\n\nphis <- seq(0, 1, 0.01)\nalphas <- c(standard = .05, low = .01)\npower <- c(0.80, 0.95)\n\nd <- expand.grid(phis, alphas, power)\nnames(d) <- c(\"phis\", \"alphas\", \"power\")\nd$fpr <- (d$phis * d$alphas) / ((d$phis * d$alphas) + (d$power * (1 - d$phis)))\n\nlibrary(ggplot2)\nggplot(d, aes(x = phis, y = fpr, color = as.factor(alphas))) + geom_line() + facet_wrap(~ power) + theme_bw()\n\n\n\n\nIn physics, they use a five sigma rule. That means their alpha is \\(3*10^-7\\) or 1 in 3.5 million. Do the above again, but this time plot “our” 0.05 against five sigma and compare false positive rates.\n\nphis <- seq(0, 1, 0.01)\nalphas <- c(standard = .05, physics = 3*10^-7)\npower <- c(0.80, 0.95)\n\nd <- expand.grid(phis, alphas, power)\nnames(d) <- c(\"phis\", \"alphas\", \"power\")\nd$fpr <- (d$phis * d$alphas) / ((d$phis * d$alphas) + (d$power * (1 - d$phis)))\n\nggplot(d, aes(x = phis, y = fpr, color = as.factor(alphas))) + geom_line() + facet_wrap(~ power) + theme_bw()"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/05-exercise.html#exercise-1",
    "href": "content/05-alpha-beta-sensitivity/05-exercise.html#exercise-1",
    "title": "Exercise III",
    "section": "0.2 Exercise",
    "text": "0.2 Exercise\nHow does the alpha level influence your power? Simulate two correlated scores. The means of the scores are 4 and 4.2; their SDs are 0.4 and 0.7. Their correlation is 0.65. Simulate power (500 runs) for sample sizes starting at 30 and going to a maximum of 110. Stop whenever you reach 95% power (so use while). Do that for 5 different alpha levels: c(0.005, 0.001, 0.01, 0.05, 0.10). Plot the results. As always, you can use the code below. What’s the influence of the alpha compared to the sample size?\n\nlibrary(ggplot2)\n\nggplot(outcomes, aes(x = sample_size, y = power, color = as.factor(alpha))) + geom_line() + geom_hline(yintercept = 0.95) %>%  theme_classic()\n\n\nlibrary(MASS)\n\nmeans <- c(pre = 4, post = 4.3)\npre_sd <- 0.4\npost_sd <- 0.7\ncorrelation <- 0.65\nalphas <- c(0.005, 0.001, 0.01, 0.05, 0.10)\nruns <- 500\nn_max <- 110\n\nsigma <- \n    matrix(\n      c(\n        pre_sd**2,\n        correlation * pre_sd * post_sd,\n        correlation * pre_sd * post_sd,\n        post_sd**2\n      ),\n      ncol = 2\n    )\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    alpha = NULL,\n    power = NULL\n  )\n\nfor (analpha in alphas) {\n  \n  n <- 30\n  power <- 0\n  \n  while (power < 0.95 & n <= n_max) {\n    \n    pvalues <- NULL\n    \n    for (i in 1:runs) {\n      \n      d <- as.data.frame(\n        mvrnorm(\n          n,\n          means,\n          sigma\n        )\n      )\n      \n      t <- t.test(d$pre, d$post, paired = TRUE)\n      \n      pvalues[i] <- t$p.value\n    }\n    \n    power <- sum(pvalues < analpha) / length(pvalues)\n    \n    outcomes <- rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        alpha = analpha,\n        power = power\n      )\n    )\n    \n    n <- n + 1\n  }\n}\n\nggplot(outcomes, aes(x = sample_size, y = power, color = as.factor(alpha))) + geom_line() + geom_hline(yintercept = 0.95) + theme_classic()"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/05-exercise.html#exercise-2",
    "href": "content/05-alpha-beta-sensitivity/05-exercise.html#exercise-2",
    "title": "Exercise III",
    "section": "0.3 Exercise",
    "text": "0.3 Exercise\nYou have a large sample (2,000 people) from a public cohort study. You’re interested in comparing two groups on their intelligence. Your smallest effect effect size of interest is 3 IQ points. You know of Lindley’s paradox where even small p-values are actually evidence for H0 if the test has a lot of power. Therefore, you decide to conduct a compromise analysis in GPower for an independent, one-tailed t-test. You think that type 2 errors in this case are twice as bad as Type I errors. (Tip: IQ scores are standardized with a mean of 100 and an SD of 15).\nObtain the new alpha from GPower. Then check it and simulate drawing 100,000 samples with exactly your SESOI and that sample size as well as 100,000 where there is 0 difference. What proportion of the p-values are below your new alpha? (Aka: Does your power estimate align with GPower’s power output?). Then plot the p-values between 0 and alpha. Have you taken care of Lindley’s paradox?\nYou can use this code (if your data d are in the long formate where the variable type indicates whether we have the effect distribution or the null distribution):\n\nlibrary(ggplot2)\n\nggplot(d, aes(x = pvalue, color = type)) + geom_density() + xlim(c(0, 0.02)) + ylim(c(0, draws/10)) + geom_vline(xintercept = alpha) + theme_bw()\n\n\nset.seed(42)\n\nn <- 1e3\nm <- 100\nsd <- 15\nsesoi <- 3\ndraws <- 1e4\nalpha <- 0.008925552\n\npvalues <- NULL\nnulls <- NULL\n\nfor (i in 1:draws) {\n  \n  control <- rnorm(n, m, sd)\n  treatment <- rnorm(n, m + sesoi, sd)\n  \n  pvalues[i] <- t.test(control, treatment, alternative = \"less\")$p.value\n  \n  control <- rnorm(n, m, sd)\n  treatment <- rnorm(n, m, sd)\n  \n  nulls[i] <- t.test(control, treatment, alternative = \"less\")$p.value\n}\n\npower <- sum(pvalues < alpha) / length(pvalues)\n\nd <- data.frame(\n  pvalue = c(pvalues, nulls),\n  type = rep(c(\"effect\", \"no effect\"), each = draws)\n)\n\nggplot(d, aes(x = pvalue, color = type)) + geom_density() + xlim(c(0, 0.02)) + ylim(c(0, draws/10)) + geom_vline(xintercept = alpha) + theme_bw()"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/05-exercise.html#exercise-3",
    "href": "content/05-alpha-beta-sensitivity/05-exercise.html#exercise-3",
    "title": "Exercise III",
    "section": "0.4 Exercise",
    "text": "0.4 Exercise\nFor your master thesis, you ran a study where you conducted a paired-samples t-test. At the time, you didn’t know about power analysis. Now as you write the paper up for publication, you state that you didn’t conduct a power analysis, but you want to at least report the sensitivity of the test. Your sample size was 27 and you conducted a two-tailed test. Your alpha was 0.05. Simulate the sensitivity of your study (1,000 runs) for standardized effects ranging from 0 to 1. Verify with GPower. (Tip: Remember that the test is just on the difference of the two scores, so you can directly draw the difference.)\n\nset.seed(42)\n\nn <- 27\neffects <- seq(0, 1, 0.01)\ndraws <- 1e3\n\n\noutcomes <- \n  data.frame(\n    effect_size = NULL,\n    power = NULL\n  )\n\nfor (aneffect in effects) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:draws) {\n    \n    differences <- rnorm(n, aneffect)\n    \n    t <- t.test(differences, mu = 0)\n    \n    pvalues[i] <- t$p.value\n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        effect_size = aneffect,\n        power = sum(pvalues < 0.05) / length(pvalues)\n      )\n    )\n}\n\nwith(outcomes, plot(effect_size, power))\nabline(h = 0.95)"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/05-exercise.html#exercise-4",
    "href": "content/05-alpha-beta-sensitivity/05-exercise.html#exercise-4",
    "title": "Exercise III",
    "section": "0.5 Exercise",
    "text": "0.5 Exercise\nRun a sensitivity analysis on a paired samples t-test (one-tailed). You had 47 participants; the means were 56 and 60; the SDs were 16 and 13; the correlation between the measures was 0.4. Get sensitivity for three different alpha levels: c(0.005, 0.01, 0.05). As for effect sizes: Increase the effect size by 1 until you have 90% power. For each combination, do 1,000 simulations. Plot the results with ggplot like you did earlier.\n\nn <- 32\nsd_control <- 16\nsd_treatment <- 13\ncorrelation <- 0.4\nalphas <- c(0.005, 0.01, 0.05)\nruns <- 1e3\n\nlibrary(MASS)\n\noutcomes <- \n  data.frame(\n    effect_size = NULL,\n    alpha = NULL,\n    power = NULL\n  )\n\nfor (analpha in alphas) {\n  \n  effect_size <- 0\n  power <- 0\n  \n  while (power < 0.90) {\n    \n    pvalues <- NULL\n    \n    means <- c(control = 56, treatment = 56 + effect_size)\n    \n    sigma <- \n      matrix(\n        c(\n          sd_control**2,\n          correlation * sd_control * sd_treatment,\n          correlation * sd_control * sd_treatment,\n          sd_treatment**2\n        ),\n        ncol = 2\n      )\n    \n    for (i in 1:runs) {\n      \n      d <- as.data.frame(\n        mvrnorm(\n          n,\n          means,\n          sigma\n        )\n      )\n      \n      t <- t.test(d$control, d$treatment, paired = TRUE, alternative = \"less\")\n      \n      pvalues[i] <- t$p.value\n    }\n    \n    power <- sum(pvalues < analpha) / length(pvalues)\n    \n    outcomes <- rbind(\n      outcomes,\n      data.frame(\n        effect_size = effect_size,\n        alpha = analpha,\n        power = power\n      )\n    )\n    \n    effect_size <- effect_size + 1\n  }\n}\n\nggplot(outcomes, aes(x = effect_size, y = power, color = as.factor(alpha))) + geom_line() + geom_hline(yintercept = 0.9) + theme_bw()"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/05-exercise.html#exercise-5",
    "href": "content/05-alpha-beta-sensitivity/05-exercise.html#exercise-5",
    "title": "Exercise III",
    "section": "0.6 Exercise",
    "text": "0.6 Exercise\nDo the above again, but this time on the standardized scale. Verify your results with GPower. Now you can’t plot the raw effect size, but you need the standardized one. However, the standardized one in this setting will depend on the SD of the difference. Therefore, you need to calculate the empirical Cohen’s \\(d\\) for each run, store it, and then take the mean of all \\(d\\)s per combination.\n\nn <- 32\nsd_control <- 16\nsd_treatment <- 13\ncorrelation <- 0.4\nalphas <- c(0.005, 0.01, 0.05)\nruns <- 1e3\n\nlibrary(MASS)\n\noutcomes <- \n  data.frame(\n    effect_size = NULL,\n    alpha = NULL,\n    power = NULL\n  )\n\nfor (analpha in alphas) {\n  print(analpha)\n  effect_size <- 0\n  power <- 0\n  \n  while (power < 0.90) {\n    \n    pvalues <- NULL\n    ds <- NULL\n    means <- c(control = 56, treatment = 56 + effect_size)\n    \n    sigma <- \n      matrix(\n        c(\n          sd_control**2,\n          correlation * sd_control * sd_treatment,\n          correlation * sd_control * sd_treatment,\n          sd_treatment**2\n        ),\n        ncol = 2\n      )\n    \n    for (i in 1:runs) {\n      \n      d <- as.data.frame(\n        mvrnorm(\n          n,\n          means,\n          sigma\n        )\n      )\n      \n      t <- t.test(d$control, d$treatment, paired = TRUE, alternative = \"less\")\n      \n      pvalues[i] <- t$p.value\n      \n      ds[i] <- (effect_size - 0) / sd(d$control-d$treatment)\n    }\n    \n    power <- sum(pvalues < analpha) / length(pvalues)\n    \n    outcomes <- rbind(\n      outcomes,\n      data.frame(\n        effect_size = mean(ds),\n        alpha = analpha,\n        power = power\n      )\n    )\n    \n    effect_size <- effect_size + 1\n  }\n}\n\n[1] 0.005\n[1] 0.01\n[1] 0.05\n\nggplot(outcomes, aes(x = effect_size, y = power, color = as.factor(alpha))) + geom_line() + geom_hline(yintercept = 0.9) + theme_bw()"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/05-slides.html",
    "href": "content/05-alpha-beta-sensitivity/05-slides.html",
    "title": "Slides",
    "section": "",
    "text": "Below you can find the slides for alpha, beta, and sensitivity:\n\n\n\nIf you want to see the slides in full screen, you can click here. To download them as PDFs, hit the ‘e’ button when you got the presentation open and then Ctrl+P print to PDF. This will work best in Chrome (in Firefox, it didn’t print the headings for me)."
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#remember-what-determines-power",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#remember-what-determines-power",
    "title": "Alpha, beta, and sensitivity",
    "section": "Remember what determines power?",
    "text": "Remember what determines power?\nPower is the probability of finding a significant result when there is an effect. It’s determined (simplified) by:\n\nEffect size\nSample size\nError rates (\\(\\alpha\\) and \\(\\beta\\))"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#the-alpha-debate",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#the-alpha-debate",
    "title": "Alpha, beta, and sensitivity",
    "section": "The alpha debate",
    "text": "The alpha debate\nRemember what happens if we change our alpha?\nLet’s take a look again."
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#remember-positive-predictive-value",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#remember-positive-predictive-value",
    "title": "Alpha, beta, and sensitivity",
    "section": "Remember positive predictive value?",
    "text": "Remember positive predictive value?\nHow likely is it that our significant result represents a true effect? Depends on:\n\nThe odds of it being true in the first place (\\(R\\))\nOur power (how sensitive was our test to detect it)\nOur \\(\\alpha\\) (how many false positives we get)\n\n\\[\\begin{gather*}\nPPV = \\frac{power \\times R}{power \\times R + \\alpha}\n\\end{gather*}\\]"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#reversing-the-logic",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#reversing-the-logic",
    "title": "Alpha, beta, and sensitivity",
    "section": "Reversing the logic",
    "text": "Reversing the logic\nFalse positive rate: How likely is it that our significant result represents a, well, false positive:\n\\[\\begin{gather*}\nFalse \\ positive \\ rate = \\frac{False \\ positives}{False \\ positives + True \\ positives}\n\\end{gather*}\\]"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#formally",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#formally",
    "title": "Alpha, beta, and sensitivity",
    "section": "Formally",
    "text": "Formally\n\nProportion of null hypotheses that are true (\\(\\phi\\))\nOur error rate for false positives (\\(\\alpha\\))\nOur power (how good are we in finding true positives)\n\n\\[\\begin{gather*}\nFalse \\ positives = \\phi \\times \\alpha\\\\\nTrue \\ positives = power \\times (1 - \\phi)\n\\end{gather*}\\]"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#plug-it-all-in",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#plug-it-all-in",
    "title": "Alpha, beta, and sensitivity",
    "section": "Plug it all in",
    "text": "Plug it all in\n\\[\\begin{gather*}\nFalse \\ positive \\ rate = \\frac{False \\ positives}{False \\ positives + True \\ positives}\\\\\\\\\nFalse \\ positives = \\phi \\times \\alpha\\\\\nTrue \\ positives = power \\times (1 - \\phi)\\\\\\\\\nFalse \\ positive \\ rate = \\frac{\\phi \\times \\alpha}{\\phi \\times \\alpha + power \\times (1 - \\phi)}\n\\end{gather*}\\]"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#the-alpha-debate-1",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#the-alpha-debate-1",
    "title": "Alpha, beta, and sensitivity",
    "section": "The \\(\\alpha\\) debate",
    "text": "The \\(\\alpha\\) debate\n\n(Benjamin et al. 2018)"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#whats-the-effect-of-alpha",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#whats-the-effect-of-alpha",
    "title": "Alpha, beta, and sensitivity",
    "section": "What’s the effect of alpha?",
    "text": "What’s the effect of alpha?\n\n(Benjamin et al. 2018)"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#the-alpha-debate-2",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#the-alpha-debate-2",
    "title": "Alpha, beta, and sensitivity",
    "section": "The \\(\\alpha\\) debate",
    "text": "The \\(\\alpha\\) debate\n\n(Lakens et al. 2018)"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#counterpoints",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#counterpoints",
    "title": "Alpha, beta, and sensitivity",
    "section": "Counterpoints",
    "text": "Counterpoints\n\n\\(\\alpha\\) = .005 is just as arbitrary\nThe biggest factor is still the odds of a hypothesis being true\nWe’ll need massive samples = fewer replications and narrower research\n\nInstead: Just like any other thing in our research, we should justify our choice of \\(\\alpha\\)."
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#why-do-we-even-use-.05",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#why-do-we-even-use-.05",
    "title": "Alpha, beta, and sensitivity",
    "section": "Why do we even use .05?",
    "text": "Why do we even use .05?\n\nPersonally, the writer prefers to set a low standard of significance at the 5 per cent point, and ignore entirely all results which fail to reach this level (Fisher 1926)."
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#but-right-before",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#but-right-before",
    "title": "Alpha, beta, and sensitivity",
    "section": "But right before",
    "text": "But right before\n\nIf one in twenty does not seem high enough odds, we may, if we prefer it, draw the line at one in fifty or one in a hundred (Fisher 1926)."
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#and-why-do-we-use-80-power",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#and-why-do-we-use-80-power",
    "title": "Alpha, beta, and sensitivity",
    "section": "And why do we use 80% power?",
    "text": "And why do we use 80% power?\n\nIt is proposed here as a convention that, when the investigator has no other basis for setting the desired power value, the value .80 be used. This means that beta is set at .20. This value is offered for several reasons. The chief among them takes into consideration the implicit convention for alpha of .05. The beta of .20 is chosen with the idea that the general relative seriousness of these two kinds of errors is of the order of .20/.05, i.e., that Type I errors are of the order of four times as serious as Type II errors. (Cohen 1988)."
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#where-we-stopped-reading",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#where-we-stopped-reading",
    "title": "Alpha, beta, and sensitivity",
    "section": "Where we stopped reading",
    "text": "Where we stopped reading\n\nThis .80 desired power convention is offered with the hope that it will be ignored whenever an investigator can find a basis in his substantive concerns in his specific research investigation to choose a value ad hoc errors. (Cohen 1988)."
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#unethical",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#unethical",
    "title": "Alpha, beta, and sensitivity",
    "section": "Unethical?",
    "text": "Unethical?"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#the-tyranny-of-rules-of-thumb",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#the-tyranny-of-rules-of-thumb",
    "title": "Alpha, beta, and sensitivity",
    "section": "The tyranny of rules of thumb",
    "text": "The tyranny of rules of thumb\n\nWe might naively assume that when all researchers do something, there must be a good reason for such an established practice. An important step towards maturity as a scholar is the realization that this is not the case. [Maier and Lakens (2021), p. 4]\n\nShould we just replace one rule of thumb with another rule of thumb?"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#that-koala-example",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#that-koala-example",
    "title": "Alpha, beta, and sensitivity",
    "section": "That Koala example",
    "text": "That Koala example\n\n(Field et al. 2004)"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#better-safe-than-sorry",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#better-safe-than-sorry",
    "title": "Alpha, beta, and sensitivity",
    "section": "Better safe than sorry?",
    "text": "Better safe than sorry?\nImagine you’re thirsty. You down a glass of milk. Right after drinking it, you think that it tasted funny.\n\nH1: The milk was expired.\nH0: The milk wasn’t expired."
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#how-should-we-act",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#how-should-we-act",
    "title": "Alpha, beta, and sensitivity",
    "section": "How should we act?",
    "text": "How should we act?\nExpired milk means a night of diarrhea. Imagine there’s a pill that will prevent that, with no side effects. How should you act?\n\nType I error: You act as if the milk was expired even though it wasn’t (false positive).\nType II error: You act as if the milk wasn’t expired even though it was (false negative)."
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#assessing-the-consequences",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#assessing-the-consequences",
    "title": "Alpha, beta, and sensitivity",
    "section": "Assessing the consequences",
    "text": "Assessing the consequences\nConsequences:\n\nType I error: None.\nType II error: Severe.\n\nWe should always act as if H1 is true: \\(\\alpha\\) = 1."
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#a-statistical-reason",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#a-statistical-reason",
    "title": "Alpha, beta, and sensitivity",
    "section": "A statistical reason",
    "text": "A statistical reason\nRemember this?"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#jeffreys-lindleys-paradox",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#jeffreys-lindleys-paradox",
    "title": "Alpha, beta, and sensitivity",
    "section": "Jeffreys-Lindley’s paradox",
    "text": "Jeffreys-Lindley’s paradox"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#with-great-power",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#with-great-power",
    "title": "Alpha, beta, and sensitivity",
    "section": "With great power…",
    "text": "With great power…\n\nLarge samples (or massive effects) result in extremely high power (e.g., 99%)\nRemember Crud: Anything could be significant\nWe should consider lowering the \\(\\alpha\\)"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#balancing-and-minimizing",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#balancing-and-minimizing",
    "title": "Alpha, beta, and sensitivity",
    "section": "Balancing and minimizing",
    "text": "Balancing and minimizing\n\nThey’re called errors for a reasons\nThey have a costs, so we want to reduce them\nLeads to effective decision making"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#theyre-not-independent",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#theyre-not-independent",
    "title": "Alpha, beta, and sensitivity",
    "section": "They’re not independent",
    "text": "They’re not independent\n\n\\(\\alpha\\) influences our power\nPower is \\((1 - \\beta)\\) (e.g., when power 87%, then our \\(\\beta\\) = .13)\nChange \\(\\alpha\\), change \\(\\beta\\)"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#a-worked-example",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#a-worked-example",
    "title": "Alpha, beta, and sensitivity",
    "section": "A worked example",
    "text": "A worked example\nLet’s assume we’re comparing two groups. We’re certain the effect exists (Cohen’s \\(d\\) = 0.5) because there’s lots of literature. We decide to go with the conventional thresholds for our error rates (although we should know better). Therefore:\n\n\\(\\alpha\\) = .05\n\\(\\beta\\) = .20 (aka power is 80%)\nPrior odds of H1 being true: 3:1"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#whats-our-real-type-i-error-rate",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#whats-our-real-type-i-error-rate",
    "title": "Alpha, beta, and sensitivity",
    "section": "What’s our (real) Type I error rate?",
    "text": "What’s our (real) Type I error rate?\nProbability of committing a false positive: How many nulls are there and what’s our chance of falsely flagging a null as a true effect?\n\\[\\begin{gather*}\nProbability \\ H_0 \\ is \\ true \\times Probability \\ Type \\ I \\ error \\\\\nP(H_0) \\times \\alpha\n\\end{gather*}\\]"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#whats-our-real-type-ii-error-rate",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#whats-our-real-type-ii-error-rate",
    "title": "Alpha, beta, and sensitivity",
    "section": "What’s our (real) Type II error rate?",
    "text": "What’s our (real) Type II error rate?\nProbability of commmiting a false negative: How many true effects are there and what’s our chance of missing them?\n\\[\\begin{gather*}\nProbability \\ H_1 \\ is \\ true \\times Probability \\ Type \\ II \\ error \\\\\nP(H_1) \\times \\beta\n\\end{gather*}\\]"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#weighted-combined-error-rate",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#weighted-combined-error-rate",
    "title": "Alpha, beta, and sensitivity",
    "section": "Weighted combined error rate",
    "text": "Weighted combined error rate\nLet’s combine our (real) Type I and Type II error rates and put them in:\n\n\\(\\alpha\\) = .05\n\\(\\beta\\) = .20 (aka power is 80%)\nPrior odds of H1 being true: 3:1 (i.e., 75%)\n\n\\[\\begin{gather*}\n(P(H_0) \\times \\alpha) + (P(H_1) \\times \\beta) \\\\\n(0.25 \\times 0.05) + (0.75 \\times 0.20) = 0.1625\n\\end{gather*}\\]"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#is-that-the-best-we-can-do",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#is-that-the-best-we-can-do",
    "title": "Alpha, beta, and sensitivity",
    "section": "Is that the best we can do?",
    "text": "Is that the best we can do?\nNow let’s change our \\(\\alpha\\) to 0.10. That makes our power go up to 88%. Our \\(\\beta\\), therefore, is .12. (Remember: iterative). Put those in:\n\\[\\begin{gather*}\n(P(H_0) \\times \\alpha) + (P(H_1) \\times \\beta) \\\\\n(0.25 \\times 0.10) + (0.75 \\times 0.12) = 0.115\n\\end{gather*}\\]"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#comparing-the-two",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#comparing-the-two",
    "title": "Alpha, beta, and sensitivity",
    "section": "Comparing the two",
    "text": "Comparing the two\nRaising the alpha, in this case, decreases our combined error rate. What’s the optimum?\n\nTry it out."
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#pesky-reality",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#pesky-reality",
    "title": "Alpha, beta, and sensitivity",
    "section": "Pesky reality",
    "text": "Pesky reality\nWhat if the probabilities of our hypotheses are different? Say the odds of H1 being true aren’t 3:1, but 1:3.\n\\[\\begin{gather*}\n(0.75 \\times 0.05) + (0.25 \\times 0.20) = 0.0875\\\\\n(0.75 \\times 0.10) + (0.25 \\times 0.12) = 0.105\n\\end{gather*}\\]\nTry it out."
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#general-logic",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#general-logic",
    "title": "Alpha, beta, and sensitivity",
    "section": "General logic",
    "text": "General logic\nIf an effect is likely, we don’t want to miss it. We cast a wider net and consider more effects significant because the (real) risk of false positives is lowered. The higher \\(P(H_1)\\), the less strict \\(\\alpha\\).\nIf an effect is unlikely, we don’t want to falsely claim it’s there. We cast a narrower net and consider less effects significant because the (real) risk of false positive is increased. The higher \\(P(H_0)\\), the more strict \\(\\alpha\\)."
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#a-balancing-act",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#a-balancing-act",
    "title": "Alpha, beta, and sensitivity",
    "section": "A balancing act",
    "text": "A balancing act\n\nIdeally, we balance prior probabilities and cost-benefits\nPrior probabilities influence expected number of errors\nBut we must still weigh the relative costs of these errors"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#remember-the-milk",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#remember-the-milk",
    "title": "Alpha, beta, and sensitivity",
    "section": "Remember the milk?",
    "text": "Remember the milk?\nWhen \\(P(H_0)\\) is high and we don’t lower our alpha, we have a completely unbalanced error rate with a super high false positive rate. Whenever we drink funny tasting milk, we’ll make the error and assume it has expired (and therefore take the pill). The low costs of Type I errors justifies making lots of them, but not making Type II errors.\nTry it out."
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#some-considerations",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#some-considerations",
    "title": "Alpha, beta, and sensitivity",
    "section": "Some considerations",
    "text": "Some considerations\n\nPublishing a false positive: bad!\nBut the benefits could be huge: publish away then!\nBut it costs a lot to get these benefits: Hm, maybe not."
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#the-compromise",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#the-compromise",
    "title": "Alpha, beta, and sensitivity",
    "section": "The compromise",
    "text": "The compromise\nWhen we know the effect size, the sample size, and what ratio of \\(\\alpha\\) and \\(\\beta\\) we want, we’re performing a compromise analysis. It finds the optimum point of high power and the right ratio. It’s usually performed with very small or very large samples."
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#lots-of-moving-parts",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#lots-of-moving-parts",
    "title": "Alpha, beta, and sensitivity",
    "section": "Lots of moving parts",
    "text": "Lots of moving parts\nWhat if we don’t know our sample size?\n\nSESOI\nCosts of Type I and Type II errors\nPrior probabilities\nWhat we want the combined error rate to be\n\nTry it out."
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#this-has-just-been-a-teaser",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#this-has-just-been-a-teaser",
    "title": "Alpha, beta, and sensitivity",
    "section": "This has just been a teaser",
    "text": "This has just been a teaser\nFor the full experience: https://psyarxiv.com/ts4r6"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#the-bible",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#the-bible",
    "title": "Alpha, beta, and sensitivity",
    "section": "The Bible",
    "text": "The Bible\n\n(Lakens 2022)"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#other-ways",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#other-ways",
    "title": "Alpha, beta, and sensitivity",
    "section": "Other ways",
    "text": "Other ways\n\nMeasure entire population\nResource constraints\nAccuracy\nA priori power analysis\nHeuristics\nNo justification"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#resource-constraints",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#resource-constraints",
    "title": "Alpha, beta, and sensitivity",
    "section": "Resource constraints",
    "text": "Resource constraints\n\nSometimes we have limited resources\nNeed to balance our resources over the course of a project\nIs it worth conducting the study?"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#is-it-worth-it",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#is-it-worth-it",
    "title": "Alpha, beta, and sensitivity",
    "section": "Is it worth it?",
    "text": "Is it worth it?\nDepends on our goal.\n\nIf we have to make a decision: Any data will be helpful and a compromise analysis is a good idea\nIf we’re interested in the effect: Will a meta-analysis be performed?\nThen consider a sensitivity analysis"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#sensitivity-analysis",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#sensitivity-analysis",
    "title": "Alpha, beta, and sensitivity",
    "section": "Sensitivity analysis",
    "text": "Sensitivity analysis\nReverses the logic:\n\nIf we have a given sample size (aka what our resources allow to collect)\nWhich effect size can we detect with how much power given our sample size?\nCompare that effect to your SESOI"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#simulating-sensitivity",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#simulating-sensitivity",
    "title": "Alpha, beta, and sensitivity",
    "section": "Simulating sensitivity",
    "text": "Simulating sensitivity\n\nn <- 25\neffects <- seq(1, 20, 1)\ndraws <- 1e3\nsd <- 15\n\noutcomes <- \n  data.frame(\n    effect_size = NULL,\n    power = NULL\n  )\n\nfor (aneffect in effects) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:draws) {\n    \n    control <- rnorm(n, 100, sd)\n    treatment <- rnorm(n, 100 + aneffect, sd)\n    t <- t.test(control, treatment)\n    \n    pvalues[i] <- t$p.value\n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        effect_size = aneffect,\n        power = sum(pvalues < 0.05) / length(pvalues)\n      )\n    )\n}"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#plotting",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#plotting",
    "title": "Alpha, beta, and sensitivity",
    "section": "Plotting",
    "text": "Plotting"
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/slides/index.html#references",
    "href": "content/05-alpha-beta-sensitivity/slides/index.html#references",
    "title": "Alpha, beta, and sensitivity",
    "section": "References",
    "text": "References\n\n\nBenjamin, Daniel J., James O. Berger, Magnus Johannesson, Brian A. Nosek, E.-J. Wagenmakers, Richard Berk, Kenneth A. Bollen, et al. 2018. “Redefine Statistical Significance.” Nature Human Behaviour 2 (1): 6–10. https://doi.org/10.1038/s41562-017-0189-z.\n\n\nCohen, J. 1988. Statistical Power Analysis for the Behavioral Sciences. 2nd ed. Hillsdale, NJ: Lawrence Erlbaum.\n\n\nField, Scott A., Andrew J. Tyre, Niclas Jonzen, Jonathan R. Rhodes, and Hugh P. Possingham. 2004. “Minimizing the Cost of Environmental Management Decisions by Optimizing Statistical Thresholds.” Ecology Letters 7 (8): 669–75. https://doi.org/10.1111/j.1461-0248.2004.00625.x.\n\n\nFisher, Ronald A. 1926. “The Arrangement of Field Experiments.” Journal of the Ministry of Agriculture 33: 503–15.\n\n\nLakens, Daniël. 2022. “Sample Size Justification.” Collabra: Psychology 8 (1): 33267. https://doi.org/10.1525/collabra.33267.\n\n\nLakens, Daniël, Federico G. Adolfi, Casper J. Albers, Farid Anvari, Matthew A. J. Apps, Shlomo E. Argamon, Thom Baguley, et al. 2018. “Justify Your Alpha.” Nature Human Behaviour 2 (3): 168–71. https://doi.org/10.1038/s41562-018-0311-x.\n\n\nMaier, Maximilian, and Daniel Lakens. 2021. “Justify Your Alpha: A Primer on Two Practical Approaches,” June. https://doi.org/10.31234/osf.io/ts4r6.\n\n\n\n\nHome"
  },
  {
    "objectID": "content/06-recap/06-slides.html",
    "href": "content/06-recap/06-slides.html",
    "title": "Slides",
    "section": "",
    "text": "Below you can find the slides for the mid-workshop recap:\n\n\n\nIf you want to see the slides in full screen, you can click here. To download them as PDFs, hit the ‘e’ button when you got the presentation open and then Ctrl+P print to PDF. This will work best in Chrome (in Firefox, it didn’t print the headings for me)."
  },
  {
    "objectID": "content/06-recap/slides/index.html#why-this-workshop",
    "href": "content/06-recap/slides/index.html#why-this-workshop",
    "title": "Recap",
    "section": "Why this workshop",
    "text": "Why this workshop\n\nDesigning an informative study is a key skill\nA study is rarely informative if it can’t detect what you’re after\nNeglecting power means not knowing what our results mean"
  },
  {
    "objectID": "content/06-recap/slides/index.html#so-why-are-we-here",
    "href": "content/06-recap/slides/index.html#so-why-are-we-here",
    "title": "Recap",
    "section": "So why are we here?",
    "text": "So why are we here?\nThe goal of the workshop is for you to (1) have an understanding of the philosophy behind using data to test claims, (2) get an intuition of how data generation processes work, (3) learn the technical skills to turn these processes into data, and (4) use these skills to simulate power for an informative study."
  },
  {
    "objectID": "content/06-recap/slides/index.html#whats-power",
    "href": "content/06-recap/slides/index.html#whats-power",
    "title": "Recap",
    "section": "What’s power",
    "text": "What’s power\n\nUnderstanding of the logic behind NHST\nIntuition about what power is\nSee why power, perhaps, potentially isn’t just a hoop to jump through"
  },
  {
    "objectID": "content/06-recap/slides/index.html#simulations-in-r",
    "href": "content/06-recap/slides/index.html#simulations-in-r",
    "title": "Recap",
    "section": "Simulations in R",
    "text": "Simulations in R\n\nUnderstand why simulations are useful\nLogic of Monte Carlo Simulations\nBasic tools"
  },
  {
    "objectID": "content/06-recap/slides/index.html#effect-sizes",
    "href": "content/06-recap/slides/index.html#effect-sizes",
    "title": "Recap",
    "section": "Effect sizes",
    "text": "Effect sizes\n\nUnderstand the importance of effect sizes\nHow to formulate a smallest effect size of interest\nKnow when you don’t have enough information"
  },
  {
    "objectID": "content/06-recap/slides/index.html#alpha-beta-sensitivity",
    "href": "content/06-recap/slides/index.html#alpha-beta-sensitivity",
    "title": "Recap",
    "section": "Alpha, beta, sensitivity",
    "text": "Alpha, beta, sensitivity\n\nQuestion the default of \\(\\alpha\\) = 0.05 and power = 80%\nUnderstand how terribly complex designing an informative study is\nKnow where to turn when you don’t have enough information"
  },
  {
    "objectID": "content/07-categorical-predictors/07-exercise.html#exercise",
    "href": "content/07-categorical-predictors/07-exercise.html#exercise",
    "title": "Exercise IV",
    "section": "0.1 Exercise",
    "text": "0.1 Exercise\nANOVAs are just linear models. There’s nothing wrong with using R’s aov command. In fact, it’s just a wrapper for lm. Type ?aov and convince yourself by reading the (short) description. There’s another reason why directly calling lm can be an advantage: efficiency. So far, we haven’t really cared much about the time our simulations take, but with more complicated designs (or more runs) it can easily take several hours, days, or even weeks.\nLet’s see whether lm gives us an advantage. For that, we’ll need the Sys.time() command. It does exactly what it says: tells you the time of your computer system. When we store the time before and after a simulation, we can check how long it took and compare different functions.\nRun the following to get familiar with how this works:\n\nt1 <- Sys.time()\n# wait a couple of seconds\nt2 <- Sys.time()\n\nt2-t1\n\nNow create a data frame with 4 (independent) groups whose means are c(4.2, 4.5, 4.6, 4.6) and their SDs are c(0.7, 0.8, 0.6, 0.5). The sample size is 240 (equal size in the groups). Check for an effect of condition on the outcome in 10,000 simulations. Do that once with aov and once with lm and measure how long it took. Are there differences? (Tip: Remember vectorization for rnorm to create the groups.)\n\nset.seed(42)\n\nmeans <- c(4.2, 4.5, 4.6, 4.6)\nsd <- c(0.7, 0.8, 0.6, 0.5)\nn <- 240\nruns <- 1e4\n\n# aov run\nt1_aov <- Sys.time()\n\nfor (i in 1:runs) {\n  \n  d <- \n    data.frame(\n      scores = rnorm(n, means, sd),\n      condition = rep(letters[1:4], n/4)\n    )\n  \n  aov(scores ~ condition, d)\n}\n\nt2_aov <- Sys.time()\n\n# lm run\nt1_lm <- Sys.time()\n\nfor (i in 1:runs) {\n  \n  d <- \n    data.frame(\n      scores = rnorm(n, means, sd),\n      condition = rep(letters[1:4], n/4)\n    )\n  \n  lm(scores ~ condition, d)\n}\n\nt2_lm <- Sys.time()\n\naov_time <- t2_aov-t1_aov\nlm_time <- t2_lm-t1_lm\n\naov_time-lm_time\n\nTime difference of 2.933449 secs"
  },
  {
    "objectID": "content/07-categorical-predictors/07-exercise.html#exercise-1",
    "href": "content/07-categorical-predictors/07-exercise.html#exercise-1",
    "title": "Exercise IV",
    "section": "0.2 Exercise",
    "text": "0.2 Exercise\nTime to run a power analysis. You have 3 independent groups. Their means are c(5, 5.2, 5.6); the SD is constant: 1. Your minimum sample size is 120; your maximum sample size is 300. How large does your sample have to be for 95% power? Do 1,000 per run. Go in steps of 6 for the sample size (so increase 2 per group). (Tip: You can use seq and remember this link). This can take a minute.\n\nmeans <- c(5, 5.2, 5.6)\nsd <- 1\nsizes <- seq(120, 300, 6)\nruns <- 1e3\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nfor (n in sizes) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    d <- \n      data.frame(\n        scores = rnorm(n, means, sd),\n        condition = rep(letters[1:3], n/3)\n      )\n    \n    m <- summary(lm(scores ~ condition, d))\n    pvalues[i] <- broom::glance(m)$p.value\n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        power = sum(pvalues < 0.05) / length(pvalues)\n      )\n    )\n}\n\nplot(outcomes$sample_size, outcomes$power)"
  },
  {
    "objectID": "content/07-categorical-predictors/07-exercise.html#exercise-2",
    "href": "content/07-categorical-predictors/07-exercise.html#exercise-2",
    "title": "Exercise IV",
    "section": "0.3 Exercise",
    "text": "0.3 Exercise\nDo the above again, but this time there’s no difference between the first two groups: c(5, 5, 5.6). What’s the effect on power if there’s a null effect for one contrast? Go in steps of 12 to speed it up.\n\nmeans <- c(5, 5, 5.6)\nsd <- 1\nsizes <- seq(120, 300, 12)\nruns <- 1e3\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nfor (n in sizes) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    d <- \n      data.frame(\n        scores = rnorm(n, means, sd),\n        condition = rep(letters[1:3], n/3)\n      )\n    \n    m <- summary(lm(scores ~ condition, d))\n    pvalues[i] <- broom::glance(m)$p.value\n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        power = sum(pvalues < 0.05) / length(pvalues)\n      )\n    )\n}\n\nplot(outcomes$sample_size, outcomes$power)"
  },
  {
    "objectID": "content/07-categorical-predictors/07-exercise.html#exercise-3",
    "href": "content/07-categorical-predictors/07-exercise.html#exercise-3",
    "title": "Exercise IV",
    "section": "0.4 Exercise",
    "text": "0.4 Exercise\nLet’s try to work on the standardized scale. You again have 3 groups. Their SDs are c(2, 2.2, 1.8). Choose means such that the Cohen’s \\(d\\)s are as follows:\n\nGroup 1 vs. Group 2: \\(d\\) = 0.2\nGroup 1 vs. Group 3: \\(d\\) = 0.6\nGroup 2 vs. Group 3: ?\n\nRemember the formulate for Cohen’s \\(d\\): \\(d = \\frac{M_1-M_2}{\\sqrt{\\frac{(sd_1^2 + sd_2^2)}{2}}}\\)\nIt might help to look at the previous exercises about choosing Cohen’s \\(d\\). Create the data frame and calculate power for samples ranging from 20 per group to 60 per group. Go in steps of 3 per group. Do 500 runs. Tip: You can choose means in relation to the pooled SD. Run effectsize::cohens_d() for each run (and contrast) and store the mean in your outcomes data frame. Plot the power, but also check the effect sizes for each contrast.\n\npooled_sd <- \n  function(sd1, sd2){\n    pooled_sd <- sqrt((sd1**2 + sd2**2) / 2)\n    \n    return(pooled_sd)\n  }\n\nsds <- c(2, 2.2, 1.8)\n\n# pooled sd for first two combos (the third is implied when we determine the means)\ng1_g2_sd <- pooled_sd(sds[1], sds[2])\ng1_g3_sd <- pooled_sd(sds[1], sds[3])\n\n# choose random mean for first group and other groups as proportion of the pooled SD. the last one, logically, has to be 0.4\nm1 <- 10\nm2 <- m1 + 0.2*g1_g2_sd\nm3 <- m1 + 0.6*g1_g3_sd\n\nmeans <- c(m1, m2, m3)\nsizes <- seq(20*3, 60*3, 3*3)\nruns <- 500\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL,\n    d12 = NULL,\n    d13 = NULL,\n    d23 = NULL\n  )\n\nfor (n in sizes) {\n  \n  pvalues <- NULL\n  d12 <- NULL\n  d13 <- NULL\n  d23 <- NULL\n  \n  for (i in 1:runs) {\n    d <- \n      data.frame(\n        scores = rnorm(n, means, sds),\n        condition = rep(letters[1:3], n/3)\n      )\n    \n    m <- summary(lm(scores ~ condition, d))\n    pvalues[i] <- broom::glance(m)$p.value\n    \n    # get cohen's for each contrast\n    d12[i] <- effectsize::cohens_d(d$scores[d$condition==letters[1]], d$scores[d$condition==letters[2]])$Cohens_d\n    d13[i] <- effectsize::cohens_d(d$scores[d$condition==letters[1]], d$scores[d$condition==letters[3]])$Cohens_d\n    d23[i] <- effectsize::cohens_d(d$scores[d$condition==letters[2]], d$scores[d$condition==letters[3]])$Cohens_d\n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        power = sum(pvalues < 0.05) / length(pvalues),\n        d12 = mean(d12),\n        d13 = mean(d13),\n        d23 = mean(d23)\n      )\n    )\n}\n\nplot(outcomes$sample_size, outcomes$power)\n\n\n\nmean(outcomes$d12);mean(outcomes$d13);mean(outcomes$d23)\n\n[1] -0.2022801\n\n\n[1] -0.6103061\n\n\n[1] -0.3666429"
  },
  {
    "objectID": "content/07-categorical-predictors/07-exercise.html#exercise-4",
    "href": "content/07-categorical-predictors/07-exercise.html#exercise-4",
    "title": "Exercise IV",
    "section": "0.5 Exercise",
    "text": "0.5 Exercise\nSo far, we’ve worked with dummy coding for our explanatory factor. That presumes that we’re interested in those contrasts where the second and third condition are compared to the first.\nSometimes, however, we’re not interested in these comparisons and would rather like to know the difference between our conditions and the grand mean. In that case, we should use sum to zero coding.\nTake the data set below:\n\nset.seed(42)\nn <- 100\nd <- \n  data.frame(\n    condition = as.factor(rep(letters[1:3], times = n)),\n    scores = rnorm(n*3, c(100, 150, 200), c(7, 10, 12))\n  )\n\nLet’s have a look at the contrasts:\n\ncontrasts(d$condition)\n\n  b c\na 0 0\nb 1 0\nc 0 1\n\n\nIt’s the familiar dummy coding. This means, going row by row, that the intercept will be the mean of condition a, the first contrast will be the mean of condition b in comparison to a, and the third contrast will be the mean of condition c in comparison to a.\nLet’s have a look at contrast coding instead:\n\ncontrasts(d$condition) <- contr.sum\ncontrasts(d$condition)\n\n  [,1] [,2]\na    1    0\nb    0    1\nc   -1   -1\n\n\nNow the intercept is our grand mean (the overall mean of the outcome), the first contrast compares the score in condition a against the grand mean and the second contrast compares the mean of condition b against the grand mean.\n\nsummary(lm(scores~condition, d))\n\n\nCall:\nlm(formula = scores ~ condition, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.364  -6.417  -0.009   6.081  26.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 149.8348     0.5700 262.872   <2e-16 ***\ncondition1  -50.0140     0.8061 -62.045   <2e-16 ***\ncondition2   -0.6376     0.8061  -0.791     0.43    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.873 on 297 degrees of freedom\nMultiple R-squared:  0.946, Adjusted R-squared:  0.9456 \nF-statistic:  2600 on 2 and 297 DF,  p-value: < 2.2e-16\n\n\nIf we want a different comparison, we can reorder the factor levels:\n\nd$condition <- factor(d$condition, levels = c(\"b\", \"c\", \"a\")) \ncontrasts(d$condition) <- contr.sum\ncontrasts(d$condition)\n\n  [,1] [,2]\nb    1    0\nc    0    1\na   -1   -1\n\n\nNow the first contrasts compares condition b against the grand mean, and the second contrast compares condition c against the grand mean.\nFor later sessions on interactions, we’ll rely on this kind of sum-to-zero coding (also called effect coding), so let’s get familiar with it here. Create four groups: a placebo group, a low dose group, a medium dose group, and a high dose group.\nYou want to know whether the low dose, the medium dose, and the high dose are significantly different from the grand mean. Simulate a data set, set the contrasts, and run a linear model. Choose mean values as you like, but make sure you can find them again in the model output. How would you get the mean of the placebo group now? (Tip: Look at the contrasts, take the grand mean, and apply the contrasts to each estimate.)\n\nn <- 50\nd <- \n  data.frame(\n    condition = factor(rep(c(\"placebo\", \"low dose\", \"medium dose\", \"high dose\"), times = n)),\n    scores = rnorm(n*4, c(10, 20, 30, 40), 2)\n  )\n\nd$condition <- factor(d$condition, levels = c(\"low dose\", \"medium dose\", \"high dose\", \"placebo\"))\ncontrasts(d$condition) <- contr.sum\ncontrasts(d$condition)\n\n            [,1] [,2] [,3]\nlow dose       1    0    0\nmedium dose    0    1    0\nhigh dose      0    0    1\nplacebo       -1   -1   -1\n\nsummary(lm(scores ~ condition, d))\n\n\nCall:\nlm(formula = scores ~ condition, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3991 -1.2198 -0.1447  1.3646  5.5612 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  24.9151     0.1334  186.74   <2e-16 ***\ncondition1   -5.4787     0.2311  -23.71   <2e-16 ***\ncondition2    5.4555     0.2311   23.61   <2e-16 ***\ncondition3   14.9170     0.2311   64.55   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.887 on 196 degrees of freedom\nMultiple R-squared:  0.9731,    Adjusted R-squared:  0.9726 \nF-statistic:  2360 on 3 and 196 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "content/07-categorical-predictors/07-exercise.html#exercise-5",
    "href": "content/07-categorical-predictors/07-exercise.html#exercise-5",
    "title": "Exercise IV",
    "section": "0.6 Exercise",
    "text": "0.6 Exercise\nYou plan an experiment where you want to test the effect of framing on agreeing with a message. You run a within-person experiment where people read a text with either neutral, positive, or negative framing. Each person reads each text (so we have a within-subjects design) and rates their level of agreeing on a 7-point Likert-scale.\nYou determine that the smallest contrast you care about is 0.25 points between the neutral and the negative condition and between the neutral and the positive condition. You’ve found in previous testing that only such a difference on agreeing actually has an effect on behavior. That’s your SESOI.\nYou decide that the midpoint of the scale is a decent starting point for the neutral condition. The negative condition should be 0.25 points lower and the positive condition 0.25 points higher. You generally expect that the variation should be somewhere around 1 Likert-point; this way, most scores are within -2 and +2 from the mean. However, you know that negative framing usually increases variation, so you’ll set the SD for that score 30% higher.\nThe experiment takes place within 20 minutes, so you expect a decent amount of consistency in answers per participant: a person who generally agrees more should also agree more to all conditions. In other words, you set the correlation between scores to 0.6.\nLast, you determined that you’re early in the research process, so you definitely don’t want to miss any effects (aka commit a Type II error). You also consider that a false positive won’t have any negative consequences. Therefore, you set your \\(\\alpha\\) to 0.10.\nYou can afford to collect 100 participants in total. Simulate power (500 runs each); go in steps of 5 (start with 20). How many participants do you need for 95% power? Use contrast coding. (Tip: Draw the variance-covariance matrix first before coding it. It helps.) (Tip: Remember the data format the data have to be in. You’ll need to transform them.) (You might have to Google how to get the p-value out of aov.)\n\nlibrary(MASS)\nlibrary(tidyr)\n\nmeans <- c(neutral = 4, negative = 3.75, positive = 4.25)\nsd_neutral_po <- 1\nsd_negative <- 1.3\ncorrelation <- 0.6\nsizes <- seq(10, 100, 5)\nruns <- 500\nalpha <- 0.10\n\n# covariance for negative with the two others is identical because the sd for the other two is identical\ncov_negative <- correlation * sd_neutral_po * sd_negative\ncov_other <- correlation * sd_neutral_po * sd_neutral_po\n\nour_matrix <- matrix(\n  c(\n    sd_neutral_po**2, cov_negative, cov_other,\n    cov_negative, sd_negative**2, cov_negative,\n    cov_other, cov_negative, sd_neutral_po**2\n  ),\n  ncol = 3\n)\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nfor (n in sizes) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    d <- mvrnorm(\n      n,\n      means,\n      our_matrix\n    )\n    \n    d <- as.data.frame(d)\n    d$id <- factor(1:n)\n    \n    d <- pivot_longer(\n      d,\n      cols = -id,\n      values_to = \"scores\",\n      names_to = \"condition\"\n    )\n    \n    d$condition <- as.factor(d$condition)\n    \n    contrasts(d$condition) <- contr.sum\n    \n    m <- summary(aov(scores ~ condition + Error(id), d))\n    \n    pvalues[i] <- unlist(m)[\"Error: Within.Pr(>F)1\"]\n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        power = sum(pvalues < alpha) / length(pvalues)\n      )\n    )\n}\n\nplot(outcomes$sample_size, outcomes$power)"
  },
  {
    "objectID": "content/07-categorical-predictors/07-exercise.html#exercise-6",
    "href": "content/07-categorical-predictors/07-exercise.html#exercise-6",
    "title": "Exercise IV",
    "section": "0.7 Exercise",
    "text": "0.7 Exercise\nThe previous study worked. Nice. Now you want to know whether positive framing scales: Can you reduce positivity and agreeing reduces proportionally? You run a follow-up experiment where you now have a neutral condition, a low positivity condition (half the positivity of your previous positive condition), and a high positivity condition (the positivity of your original positive condition). Your effect size remains: 0.25 for the comparison between neutral and high positivity (the original positive condition). You expect, therefore, that half the positivity should lead to half the effect size: 0.125. That’s your new SESOI.\nThe previous study also showed that you weren’t too far off with an SD of 1, but this time you’ll use 0.8 for all scores.\nHowever, you expect that, because the two positivity conditions are so similar, they’ll correlate higher than the correlation between neutral and positivity. So you expect a correlation between neutral and either positivity condition to be 0.3, but 0.6 between the two positivity conditions.\nBecause your department was so happy with how well you designed the previous study, they gave you more money. Now you can collect a maximum of 500 participants. Last time, the power simulation took quite some time, so this time you want to stop whenever you reach 95% power. Also, because this time you want to be more stringent, you set \\(\\alpha\\) to 0.01.\nDo 500 runs, start with 60 people and go in in steps of 10. Use treatment contrasts. This can take a while.\n\nlibrary(MASS)\nlibrary(tidyr)\n\nsesoi <- 0.125\nmeans <- c(neutral = 4, low = 4+sesoi, high = 4+2*sesoi)\nsd <- 0.8\ncor_positive <- 0.6\ncor_neutral <- 0.3\nruns <- 500\nalpha <- 0.01\nmax_n <- 500\nsteps <- 10\n\n# covariance for negative with the two others is identical because the sd for the other two is identical\ncov_positive <- cor_positive * sd * sd\ncov_neutral <- cor_neutral * sd * sd\n\nour_matrix <- matrix(\n  c(\n    sd**2, cov_neutral, cov_neutral,\n    cov_neutral, sd**2, cov_positive,\n    cov_neutral, cov_positive, sd**2\n  ),\n  ncol = 3\n)\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\npower <- 0\nn <- 60\n\nwhile (power < 0.95 & n <= max_n) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    \n    d <- mvrnorm(\n      n,\n      means,\n      our_matrix\n    )\n    \n    d <- as.data.frame(d)\n    \n    d$id <- factor(1:n)\n    \n    d <- pivot_longer(\n      d,\n      cols = -id,\n      values_to = \"scores\",\n      names_to = \"condition\"\n    )\n    \n    d$condition <- as.factor(d$condition)\n    \n    m <- summary(aov(scores ~ condition + Error(id), d))\n    \n    pvalues[i] <- unlist(m)[\"Error: Within.Pr(>F)1\"]\n  }\n  \n  power <- sum(pvalues < alpha) / length(pvalues)\n  \n  outcomes <- \n  rbind(\n    outcomes,\n    data.frame(\n      sample_size = n,\n      power = power\n    )\n  )\n  \n  n <- n + steps\n}\n  \nplot(outcomes$sample_size, outcomes$power)"
  },
  {
    "objectID": "content/07-categorical-predictors/07-exercise.html#exercise-7",
    "href": "content/07-categorical-predictors/07-exercise.html#exercise-7",
    "title": "Exercise IV",
    "section": "0.8 Exercise",
    "text": "0.8 Exercise\nUsually when you run an ANOVA, you specify your contrasts before-hand so you know which groups you want to compare beyond just the overall effect of your condition. Let’s take the data set below. The overall effect of condition is significant:\n\nlibrary(MASS)\nlibrary(tidyr)\n\nmeans <- c(neutral = 10, low = 12, high = 14)\nsd <- 8\ncorrelation <- 0.6\nn <- 80\n\ncovariance <- correlation * sd * sd\n\nsigma <- matrix(\n  c(\n    sd**2, covariance, covariance,\n    covariance, sd**2, covariance,\n    covariance, covariance, sd**2\n  ),\n  ncol = 3\n)\n\nd <- data.frame(\n  mvrnorm(\n    n,\n    means,\n    sigma\n  )\n)\n\nd$id <- factor(1:n)\n    \nd <- pivot_longer(\n  d,\n  cols = -id,\n  values_to = \"scores\",\n  names_to = \"condition\"\n)\n\nd$condition <- as.factor(d$condition)\n\nm <- aov(scores ~ condition + Error(id), d)\n\nsummary(m)\n\n\nError: id\n          Df Sum Sq Mean Sq F value Pr(>F)\nResiduals 79  15120   191.4               \n\nError: Within\n           Df Sum Sq Mean Sq F value   Pr(>F)    \ncondition   2    641   320.4   10.91 3.64e-05 ***\nResiduals 158   4640    29.4                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLet’s inspect the post-hoc comparisons between the groups with the emmeans package. Here, not every comparison is significant.\n\nposthocs <- emmeans::emmeans(m, ~ condition)\nposthocs\n\n condition emmean   SE  df lower.CL upper.CL\n high       12.38 1.02 129    10.36     14.4\n low        10.69 1.02 129     8.67     12.7\n neutral     8.39 1.02 129     6.38     10.4\n\nWarning: EMMs are biased unless design is perfectly balanced \nConfidence level used: 0.95 \n\npairs(posthocs)\n\n contrast       estimate    SE  df t.ratio p.value\n high - low         1.70 0.857 158   1.979  0.1208\n high - neutral     3.99 0.857 158   4.654  <.0001\n low - neutral      2.29 0.857 158   2.675  0.0224\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nRun a power simulation that finds the sample size for which we have 90% power for the smallest post-hoc contrast (aka the largest of the three p-values.) Go about it as you think is best, but save both the main effect p-value and the largest p-value of the posthoc contrasts to be able to compare power. In effect, you’re powering for a t-test, which only emphasizes that you should work on the raw scale and specify each comparison. (Tip: summary turns the pairs(postdocs) into a data frame.)\n\nmeans <- c(neutral = 10, low = 12, high = 14)\nsd <- 8\ncorrelation <- 0.6\nn <- 20\ndraws <- 1e3\n\ncovariance <- correlation * sd * sd\n\nsigma <- matrix(\n  c(\n    sd**2, covariance, covariance,\n    covariance, sd**2, covariance,\n    covariance, covariance, sd**2\n  ),\n  ncol = 3\n)\n\noutcomes <- data.frame(\n  sample_size = NULL,\n  power = NULL,\n  power_posthoc = NULL\n)\n\npower_posthoc <- 0\n\nwhile (power_posthoc < .90) {\n  \n  pvalues_posthoc <- NULL\n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    \n    d <- data.frame(\n      mvrnorm(\n        n,\n        means,\n        sigma\n      )\n    )\n    \n    d$id <- factor(1:n)\n        \n    d <- pivot_longer(\n      d,\n      cols = -id,\n      values_to = \"scores\",\n      names_to = \"condition\"\n    )\n    \n    d$condition <- as.factor(d$condition)\n    \n    m <- aov(scores ~ condition + Error(id), d)\n    \n    pvalues[i] <- unlist(summary(m))[\"Error: Within.Pr(>F)1\"]\n    \n    posthocs <- suppressMessages(emmeans::emmeans(m, ~ condition))\n    outputs <- summary(pairs(posthocs))\n    \n    pvalues_posthoc[i] <- max(outputs$p.value)\n  }\n  \n  power <- sum(pvalues < 0.05) / length(pvalues)\n  power_posthoc <- sum(pvalues_posthoc < 0.05) / length(pvalues_posthoc)\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        power = power,\n        power_posthoc = power_posthoc\n      )\n    )\n  \n  n <- n + 10\n}\n\nlibrary(ggplot2)\n\noutcomes %>% \n  pivot_longer(\n    cols = starts_with(\"power\"),\n    names_to = \"Power Type\"\n  ) %>% \n  ggplot(\n    aes(x = sample_size, y = value, color = `Power Type`)\n  ) +\n  geom_line() + theme_bw()"
  },
  {
    "objectID": "content/07-categorical-predictors/07-exercise.html#exercise-8",
    "href": "content/07-categorical-predictors/07-exercise.html#exercise-8",
    "title": "Exercise IV",
    "section": "0.9 Exercise",
    "text": "0.9 Exercise\nRather than doing posthoc comparisons between each group, we can rely on planned contrasts. If that’s completely new to you, have a read here. The blog post does a really nice job explaining planned contrasts.\nIn effect, rather than relying on sum to zero or treatment coding, we create our own comparisons. Let’s say we want to compare neutral to both low positivity and high positivity (as a general test that framing works). Then, our second contrast compares the two framing conditions. For simplicity, let’s go three independent groups with means of c(4, 4.3, 4.4) and a common SD of 1.1. Custom contrasts will allow us to directly make the two comparisons we’re interested in like such:\n\nd <- data.frame(\n  scores = rnorm(150, c(4., 4.3, 4.4), 1.1),\n  condition = factor(rep(c(\"neutral\", \"low\", \"high\"), 50))\n)\n\ncontrasts(d$condition)\n\n        low neutral\nhigh      0       0\nlow       1       0\nneutral   0       1\n\nhighlow_vs_neutral <- c(1, 1, -2)\nhigh_vs_low <- c(1, -1, 0)\n\ncontrasts(d$condition) <- cbind(highlow_vs_neutral, high_vs_low)\ncontrasts(d$condition)\n\n        highlow_vs_neutral high_vs_low\nhigh                     1           1\nlow                      1          -1\nneutral                 -2           0\n\nsummary.lm(aov(scores ~ condition, d))\n\n\nCall:\naov(formula = scores ~ condition, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3475 -0.6803  0.0627  0.6274  2.3525 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                  4.12649    0.08952  46.093  < 2e-16 ***\nconditionhighlow_vs_neutral  0.28325    0.06330   4.474 1.52e-05 ***\nconditionhigh_vs_low         0.08759    0.10964   0.799    0.426    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.096 on 147 degrees of freedom\nMultiple R-squared:  0.1232,    Adjusted R-squared:  0.1113 \nF-statistic: 10.33 on 2 and 147 DF,  p-value: 6.346e-05\n\n\nRun a power simulation where you first run standard treatment contrasts. Then, run posthoc tests (emmeans::emmeans) on that test. For simplicity, focus on the comparison of high vs. low. Select the p-value for that posthoc contrast. In the same run, change the contrasts to custom from above, run the model again, and extract the p-value for the high vs. low comparison. Do that for samples ranging from 50 to 200 in steps of 10. Do 200 runs to save a bit of time, then plot the two types of power against each other. Which approach is more “powerful”?\n\nmeans <- c(4., 4.3, 4.6)\nsd <- 1.1\nruns <- 200\nsizes <- seq(50, 200, 10)\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    type = NULL,\n    power = NULL\n  )\n\nfor (n in sizes) {\n  pvalues_treatment <- NULL\n  pvalues_contrasts <- NULL\n  \n  for (i in 1:runs) {\n    d <- data.frame(\n      scores = rnorm(n*3, means, 1.1),\n      condition = factor(rep(c(\"neutral\", \"low\", \"high\"), n))\n    )\n    posthocs <- suppressMessages(emmeans::emmeans(lm(scores~condition, d), ~ condition))\n    outputs <- summary(pairs(posthocs))\n    \n    pvalues_treatment[i] <- outputs[1,6]\n    \n    highlow_vs_neutral <- c(1, 1, -2)\n    high_vs_low <- c(1, -1, 0)\n    contrasts(d$condition) <- cbind(highlow_vs_neutral, high_vs_low)\n    \n    pvalues_contrasts[i] <- coefficients(summary(lm(scores~condition, d)))[3,4]\n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = c(n, n),\n        type = c(\"treatment\", \"contrasts\"),\n        power = c(\n          sum(pvalues_treatment < 0.05) / length(pvalues_treatment),\n          sum(pvalues_contrasts < 0.05) / length(pvalues_contrasts)\n        )\n      )\n    )\n}\n\nggplot(outcomes, aes(x = sample_size, y = power, color = type)) + geom_line() + theme_bw()"
  },
  {
    "objectID": "content/07-categorical-predictors/07-slides.html",
    "href": "content/07-categorical-predictors/07-slides.html",
    "title": "Slides",
    "section": "",
    "text": "Below you can find the slides for the session on categorical predictors (multiple groups):\n\n\n\nIf you want to see the slides in full screen, you can click here. To download them as PDFs, hit the ‘e’ button when you got the presentation open and then Ctrl+P print to PDF. This will work best in Chrome (in Firefox, it didn’t print the headings for me)."
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#whats-that-data-generating-process",
    "href": "content/07-categorical-predictors/slides/index.html#whats-that-data-generating-process",
    "title": "The linear model and comparing multiple groups",
    "section": "What’s that data generating process?",
    "text": "What’s that data generating process?\n\nThe process, in the real world, that you believe created the data\nWe don’t know the process, but we can observe the outcomes\nWhen we try to explain the outcome, we make assumptions about the data generating process\nWe collect data from samples and want to know whether our assumptions fit (and generalize)"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#whats-the-process",
    "href": "content/07-categorical-predictors/slides/index.html#whats-the-process",
    "title": "The linear model and comparing multiple groups",
    "section": "What’s the process?",
    "text": "What’s the process?\nYou measure height in a sample (the observable consequences). What’s the data generating process?"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#we-have-a-simple-process",
    "href": "content/07-categorical-predictors/slides/index.html#we-have-a-simple-process",
    "title": "The linear model and comparing multiple groups",
    "section": "We have a (simple) process",
    "text": "We have a (simple) process\n\\(Normal(\\mu, \\sigma)\\)"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#look-familiar",
    "href": "content/07-categorical-predictors/slides/index.html#look-familiar",
    "title": "The linear model and comparing multiple groups",
    "section": "Look familiar?",
    "text": "Look familiar?\n\\(Normal(174, 7)\\) aka rnorm(174, 7)\n\n\n\n\n\n\n\n\n\nplot(density(rnorm(1e4, 174, 7)))"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#all-are-processes",
    "href": "content/07-categorical-predictors/slides/index.html#all-are-processes",
    "title": "The linear model and comparing multiple groups",
    "section": "All are processes",
    "text": "All are processes\n\nrnorm: You say that the underlying data generating process is a normal distribution\nrbinom: You say that the underlying data generating process is a binomial distribution\nYou get the idea\n\nImportant: Those are assumptions you make explicit and they can be wrong."
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#remember-our-t-tests",
    "href": "content/07-categorical-predictors/slides/index.html#remember-our-t-tests",
    "title": "The linear model and comparing multiple groups",
    "section": "Remember our t-tests?",
    "text": "Remember our t-tests?\nYou are explicitly claiming that the observable outcome (the data) have been generated by this true underlying process. The process is a normal distribution with a true effect size (\\(\\mu\\)) and a true standard deviation (\\(\\sigma\\)) for two data generating processes: \\(Normal(100, 15)\\) and \\(Normal(105, 15)\\)\n\ncontrol <- rnorm(100, 100, 15)\ntreatment <- rnorm(100, 105, 15)\n\nt.test(control, treatment)\n\n\n    Welch Two Sample t-test\n\ndata:  control and treatment\nt = -0.57372, df = 197.92, p-value = 0.5668\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -5.495395  3.018476\nsample estimates:\nmean of x mean of y \n 100.3389  101.5774"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#thats-why-simulation-is-cool",
    "href": "content/07-categorical-predictors/slides/index.html#thats-why-simulation-is-cool",
    "title": "The linear model and comparing multiple groups",
    "section": "That’s why simulation is cool",
    "text": "That’s why simulation is cool\n\nYou’re being explicit about your model of the world\nYou translate that model (data generating process) into a concrete formula (code) to create data (the consequences of the data generation process)\nYou check how changes to your model influence the data, which inform the conclusions you draw from the data about your model (aka statistical inference)\nNo more hiding your model assumptions behind standard software"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#this-all-sounds-complicated",
    "href": "content/07-categorical-predictors/slides/index.html#this-all-sounds-complicated",
    "title": "The linear model and comparing multiple groups",
    "section": "This all sounds complicated",
    "text": "This all sounds complicated\nHere’s the good news: In (much of) the (social) sciences we rely on a common data generating process: the linear model.\nt-test, regression, logistic regression, machine learning are all variations of the linear model.\n\\(y = \\beta_0 + \\beta_1x\\)"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#all-stolen-from-here",
    "href": "content/07-categorical-predictors/slides/index.html#all-stolen-from-here",
    "title": "The linear model and comparing multiple groups",
    "section": "All stolen from here",
    "text": "All stolen from here\n\nSource"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#back-to-our-control-and-treatment",
    "href": "content/07-categorical-predictors/slides/index.html#back-to-our-control-and-treatment",
    "title": "The linear model and comparing multiple groups",
    "section": "Back to our control and treatment",
    "text": "Back to our control and treatment\n\\(y = \\beta_0 + \\beta_1x\\): What’s \\(x\\) here?\n\ncontrol <- rnorm(15, 0, 1)\ntreatment <- rnorm(15, 1, 1)"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#group-membership",
    "href": "content/07-categorical-predictors/slides/index.html#group-membership",
    "title": "The linear model and comparing multiple groups",
    "section": "Group membership",
    "text": "Group membership\nOur group is \\(x\\).\n\nd <- \n  data.frame(\n    group = rep(c(\"control\", \"treatment\"), each = 15),\n    y = c(control, treatment)\n  )\n\nd$x <- ifelse(d$group==\"control\", 0, 1)"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#finding-our-inputs-again",
    "href": "content/07-categorical-predictors/slides/index.html#finding-our-inputs-again",
    "title": "The linear model and comparing multiple groups",
    "section": "Finding our inputs again",
    "text": "Finding our inputs again\n\nDataBoxplot\n\n\n\n\n\n\n\ngroup\ny\nx\n\n\n\n\ncontrol\n-0.2462229\n0\n\n\ncontrol\n-0.9597997\n0\n\n\ncontrol\n-0.5238199\n0\n\n\ncontrol\n1.1407442\n0\n\n\ncontrol\n-0.6588351\n0\n\n\ncontrol\n-1.5772346\n0\n\n\ncontrol\n0.6479015\n0\n\n\ncontrol\n-0.1497161\n0\n\n\ncontrol\n1.0852342\n0\n\n\ncontrol\n0.4002139\n0\n\n\ncontrol\n0.9653700\n0\n\n\ncontrol\n0.8700338\n0\n\n\ncontrol\n-0.8435252\n0\n\n\ncontrol\n-1.4519022\n0\n\n\ncontrol\n-0.4569357\n0\n\n\ntreatment\n2.4476205\n1\n\n\ntreatment\n0.2285001\n1\n\n\ntreatment\n0.8593449\n1\n\n\ntreatment\n1.2349919\n1\n\n\ntreatment\n-0.1423595\n1\n\n\ntreatment\n0.5735239\n1\n\n\ntreatment\n0.7055050\n1\n\n\ntreatment\n2.4935786\n1\n\n\ntreatment\n-0.5713438\n1\n\n\ntreatment\n0.4377587\n1\n\n\ntreatment\n2.8445324\n1\n\n\ntreatment\n1.7562574\n1\n\n\ntreatment\n0.5085672\n1\n\n\ntreatment\n0.3798965\n1\n\n\ntreatment\n1.6759042\n1"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#what-happens-when-x-is-zero",
    "href": "content/07-categorical-predictors/slides/index.html#what-happens-when-x-is-zero",
    "title": "The linear model and comparing multiple groups",
    "section": "What happens when \\(x\\) is zero?",
    "text": "What happens when \\(x\\) is zero?\n\\[\\begin{align}\n& y = \\beta_0 + \\beta_1x\\\\\n& y = \\beta_0 + \\beta_1 \\times 0\\\\\n& y = \\beta_0\n\\end{align}\\]\nIn other words, when we predict values for the control (aka \\(x=0\\)) group, our best predictor for outcome scores becomes \\(\\beta_0\\) (also known as the intercept): the mean for the control group."
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#what-happens-when-x-is-one",
    "href": "content/07-categorical-predictors/slides/index.html#what-happens-when-x-is-one",
    "title": "The linear model and comparing multiple groups",
    "section": "What happens when \\(x\\) is one?",
    "text": "What happens when \\(x\\) is one?\n\\[\\begin{align}\n& y = \\beta_0 + \\beta_1x\\\\\n& y = \\beta_0 + \\beta_1 \\times 1\\\\\n& y = \\beta_0 + \\beta_1\\\\\n& y = mean(control) + ?\n\\end{align}\\]\nWe already know that the intercept (\\(\\beta_0\\)) is the mean of the control group. How would we now go from that mean to predicting scores of the treatment group? What do we need to add to the mean of the control group to get the mean of the treatment group?"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#pictures-please",
    "href": "content/07-categorical-predictors/slides/index.html#pictures-please",
    "title": "The linear model and comparing multiple groups",
    "section": "Pictures, please",
    "text": "Pictures, please"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#lets-check-that",
    "href": "content/07-categorical-predictors/slides/index.html#lets-check-that",
    "title": "The linear model and comparing multiple groups",
    "section": "Let’s check that",
    "text": "Let’s check that\n\n\nMean of the control group (our intercept or \\(\\beta_0\\)):\n\nmean(d$y[d$x==0])\n\n[1] -0.1172329\n\n\nDifference (our slope or \\(\\beta_1\\)):\n\nmean(d$y[d$x==1]) - mean(d$y[d$x==0])\n\n[1] 1.146051\n\n\nMean of the treatment group (\\(\\beta_0 + \\beta_1\\)):\n\nmean(d$y[d$x==1])\n\n[1] 1.028819\n\n\n\nLinear model:\n\nmodel <- lm(y ~ x, data = d)\n\nsummary(model)\n\n\nCall:\nlm(formula = y ~ x, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6002 -0.6345 -0.2464  0.7557  1.8157 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)  -0.1172     0.2496  -0.470  0.64225   \nx1            1.1461     0.3530   3.246  0.00303 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9668 on 28 degrees of freedom\nMultiple R-squared:  0.2735,    Adjusted R-squared:  0.2475 \nF-statistic: 10.54 on 1 and 28 DF,  p-value: 0.003027"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#so-theyre-really-the-same",
    "href": "content/07-categorical-predictors/slides/index.html#so-theyre-really-the-same",
    "title": "The linear model and comparing multiple groups",
    "section": "So they’re really the same?",
    "text": "So they’re really the same?\n\n\n\nt.test(control, treatment, var.equal = TRUE)\n\n\n    Two Sample t-test\n\ndata:  control and treatment\nt = -3.2464, df = 28, p-value = 0.003027\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -1.8691755 -0.4229274\nsample estimates:\n mean of x  mean of y \n-0.1172329  1.0288185 \n\n\n\n\nsummary(lm(y ~ x, d))\n\n\nCall:\nlm(formula = y ~ x, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6002 -0.6345 -0.2464  0.7557  1.8157 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)  -0.1172     0.2496  -0.470  0.64225   \nx1            1.1461     0.3530   3.246  0.00303 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9668 on 28 degrees of freedom\nMultiple R-squared:  0.2735,    Adjusted R-squared:  0.2475 \nF-statistic: 10.54 on 1 and 28 DF,  p-value: 0.003027"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#same-logic-for-a-one-sample-t-test",
    "href": "content/07-categorical-predictors/slides/index.html#same-logic-for-a-one-sample-t-test",
    "title": "The linear model and comparing multiple groups",
    "section": "Same logic for a one-sample t-test",
    "text": "Same logic for a one-sample t-test\nThere’s no \\(x\\) here, so all we’re left with is the intercept, which we test against our H0. A single number (aka the mean) predicts \\(y\\).\n\\[\\begin{align}\n& y = \\beta_0\n\\end{align}\\]"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#pictures-please-1",
    "href": "content/07-categorical-predictors/slides/index.html#pictures-please-1",
    "title": "The linear model and comparing multiple groups",
    "section": "Pictures, please",
    "text": "Pictures, please"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#lets-check-that-once-more",
    "href": "content/07-categorical-predictors/slides/index.html#lets-check-that-once-more",
    "title": "The linear model and comparing multiple groups",
    "section": "Let’s check that once more",
    "text": "Let’s check that once more\n\n\nMean of the treatment group (our intercept or \\(\\beta_0\\)):\n\nmean(d$y[d$x==1])\n\n[1] 1.028819\n\n\n\nLinear model:\n\nmodel <- lm(y ~ 1, data = d[d$x==1,])\n\nsummary(model)\n\n\nCall:\nlm(formula = y ~ 1, data = d[d$x == 1, ])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6002 -0.6200 -0.3233  0.6873  1.8157 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   1.0288     0.2619   3.929  0.00151 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.014 on 14 degrees of freedom"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#so-theyre-really-the-same-1",
    "href": "content/07-categorical-predictors/slides/index.html#so-theyre-really-the-same-1",
    "title": "The linear model and comparing multiple groups",
    "section": "So they’re really the same?",
    "text": "So they’re really the same?\n\n\n\nt.test(treatment)\n\n\n    One Sample t-test\n\ndata:  treatment\nt = 3.9289, df = 14, p-value = 0.001513\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.467180 1.590457\nsample estimates:\nmean of x \n 1.028819 \n\n\n\n\nsummary(lm(y ~ 1, d[d$x==1,]))\n\n\nCall:\nlm(formula = y ~ 1, data = d[d$x == 1, ])\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6002 -0.6200 -0.3233  0.6873  1.8157 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   1.0288     0.2619   3.929  0.00151 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.014 on 14 degrees of freedom"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#same-logic-for-a-paired-samples-t-test",
    "href": "content/07-categorical-predictors/slides/index.html#same-logic-for-a-paired-samples-t-test",
    "title": "The linear model and comparing multiple groups",
    "section": "Same logic for a paired-samples t-test",
    "text": "Same logic for a paired-samples t-test\nRemember: Paired samples t-test is just testing the difference in scores against H0. This way, it turns into a one-sample t-test, so all we’re left with is, once again, the intercept, which we test against our H0. A single number (aka the mean) predicts \\(y\\) (technically \\(y_{difference}\\)).\n\\[\\begin{align}\n& y_{treatment} - y_{control} = \\beta_0\n\\end{align}\\]"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#pictures-please-2",
    "href": "content/07-categorical-predictors/slides/index.html#pictures-please-2",
    "title": "The linear model and comparing multiple groups",
    "section": "Pictures, please",
    "text": "Pictures, please"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#pictures-please-3",
    "href": "content/07-categorical-predictors/slides/index.html#pictures-please-3",
    "title": "The linear model and comparing multiple groups",
    "section": "Pictures, please",
    "text": "Pictures, please"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#lets-check-that-once-more-1",
    "href": "content/07-categorical-predictors/slides/index.html#lets-check-that-once-more-1",
    "title": "The linear model and comparing multiple groups",
    "section": "Let’s check that once more",
    "text": "Let’s check that once more\n\n\nMean of the difference (our intercept or \\(\\beta_0\\)):\n\nmean(treatment-control)\n\n[1] 1.146051\n\n\n\nLinear model:\n\nmodel <- lm(treatment-control ~ 1)\n\nsummary(model)\n\n\nCall:\nlm(formula = treatment - control ~ 1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8026 -0.8407  0.2060  0.8599  1.5478 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   1.1461     0.3052   3.754  0.00213 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.182 on 14 degrees of freedom"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#so-theyre-really-the-same-2",
    "href": "content/07-categorical-predictors/slides/index.html#so-theyre-really-the-same-2",
    "title": "The linear model and comparing multiple groups",
    "section": "So they’re really the same?",
    "text": "So they’re really the same?\n\n\n\nt.test(treatment-control)\n\n\n    One Sample t-test\n\ndata:  treatment - control\nt = 3.7545, df = 14, p-value = 0.002134\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.4913576 1.8007453\nsample estimates:\nmean of x \n 1.146051 \n\n\n\n\nsummary(lm(treatment-control ~ 1))\n\n\nCall:\nlm(formula = treatment - control ~ 1)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.8026 -0.8407  0.2060  0.8599  1.5478 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)   \n(Intercept)   1.1461     0.3052   3.754  0.00213 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.182 on 14 degrees of freedom"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#what-about-anova-then",
    "href": "content/07-categorical-predictors/slides/index.html#what-about-anova-then",
    "title": "The linear model and comparing multiple groups",
    "section": "What about ANOVA then?",
    "text": "What about ANOVA then?\nSame as before: We predict scores with the group membership (aka the mean in each group). Doesn’t matter whether we predict it from two (t-test) or more groups (ANOVA). Now we just have an indicator for membership for each group: dummy coding.\n\n\n\ngroups\n\\(x_1\\)\n\\(x_2\\)\n\n\n\n\ncontrol\n0\n0\n\n\nlow\n1\n0\n\n\nhigh\n0\n1\n\n\n\n\\[\\begin{align}\n& y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\n\\end{align}\\]"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#getting-the-score-for-control",
    "href": "content/07-categorical-predictors/slides/index.html#getting-the-score-for-control",
    "title": "The linear model and comparing multiple groups",
    "section": "Getting the score for control",
    "text": "Getting the score for control\n\n\n\ngroups\n\\(x_1\\)\n\\(x_2\\)\n\n\n\n\ncontrol\n0\n0\n\n\nlow\n1\n0\n\n\nhigh\n0\n1\n\n\n\n\\[\\begin{align}\n& y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\\\\\n& y = \\beta_0 + \\beta_1 \\times 0 + \\beta_2 \\times 0\\\\\n& y = \\beta_0\n\\end{align}\\]"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#getting-the-score-for-low",
    "href": "content/07-categorical-predictors/slides/index.html#getting-the-score-for-low",
    "title": "The linear model and comparing multiple groups",
    "section": "Getting the score for low",
    "text": "Getting the score for low\n\n\n\ngroups\n\\(x_1\\)\n\\(x_2\\)\n\n\n\n\ncontrol\n0\n0\n\n\nlow\n1\n0\n\n\nhigh\n0\n1\n\n\n\n\\[\\begin{align}\n& y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\\\\\n& y = \\beta_0 + \\beta_1 \\times 1 + \\beta_2 \\times 0\\\\\n& y = \\beta_0 + \\beta_1\n\\end{align}\\]"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#getting-the-score-for-high",
    "href": "content/07-categorical-predictors/slides/index.html#getting-the-score-for-high",
    "title": "The linear model and comparing multiple groups",
    "section": "Getting the score for high",
    "text": "Getting the score for high\n\n\n\ngroups\n\\(x_1\\)\n\\(x_2\\)\n\n\n\n\ncontrol\n0\n0\n\n\nlow\n1\n0\n\n\nhigh\n0\n1\n\n\n\n\\[\\begin{align}\n& y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\\\\\n& y = \\beta_0 + \\beta_1 \\times 0 + \\beta_2 \\times 1\\\\\n& y = \\beta_0 + \\beta_2\n\\end{align}\\]"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#what-are-beta_1-and-beta_2-then",
    "href": "content/07-categorical-predictors/slides/index.html#what-are-beta_1-and-beta_2-then",
    "title": "The linear model and comparing multiple groups",
    "section": "What are \\(\\beta_1\\) and \\(\\beta_2\\) then?",
    "text": "What are \\(\\beta_1\\) and \\(\\beta_2\\) then?\nWhat do we need to go from the control mean to the low mean? What do we need to go from control mean to high mean? Same as with the t-test: the difference between those means."
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#pictures-please-4",
    "href": "content/07-categorical-predictors/slides/index.html#pictures-please-4",
    "title": "The linear model and comparing multiple groups",
    "section": "Pictures, please",
    "text": "Pictures, please"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#just-an-extension",
    "href": "content/07-categorical-predictors/slides/index.html#just-an-extension",
    "title": "The linear model and comparing multiple groups",
    "section": "Just an extension",
    "text": "Just an extension\nIf we know how to simulate a t-test, we know how to simulate an ANOVA (because both are just linear models): Imitate the data generating process.\n\\[\\begin{align}\n& y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2\n\\end{align}\\]\n\nn <- 5\n\ncontrol <- rnorm(n, 100, 15)\nlow <- rnorm(n, 120, 15)\nhigh <- rnorm(n, 140, 15)\n\nd <- data.frame(\n  y = c(control, low, high),\n  condition = rep(c(\"control\", \"low\", \"high\"), each = n)\n)\n\nd$condition <- as.factor(d$condition)"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#add-our-dummy-codes",
    "href": "content/07-categorical-predictors/slides/index.html#add-our-dummy-codes",
    "title": "The linear model and comparing multiple groups",
    "section": "Add our dummy codes",
    "text": "Add our dummy codes\n\nd$group_low <- rep(c(0, 1, 0), each = n)\nd$group_high <- rep(c(0, 0, 1), each = n)\n\n\n\n           y condition group_low group_high\n1  106.49227   control         0          0\n2   87.82910   control         0          0\n3  121.66152   control         0          0\n4   93.52831   control         0          0\n5  109.83472   control         0          0\n6  124.82888       low         1          0\n7  108.24242       low         1          0\n8  143.63591       low         1          0\n9  129.64349       low         1          0\n10 121.34641       low         1          0\n11 144.14826      high         0          1\n12 150.18933      high         0          1\n13 141.34749      high         0          1\n14  95.10365      high         0          1\n15 144.27324      high         0          1"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#compare-to-model",
    "href": "content/07-categorical-predictors/slides/index.html#compare-to-model",
    "title": "The linear model and comparing multiple groups",
    "section": "Compare to model",
    "text": "Compare to model\n\n\n\n\n# A tibble: 3 × 2\n  condition  mean\n  <fct>     <dbl>\n1 control    104.\n2 low        126.\n3 high       135.\n\n\n\n\nmodel <- lm(y ~ group_low + group_high, data = d)\n\nsummary(model)\n\n\nCall:\nlm(formula = y ~ group_low + group_high, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.909  -7.267   4.104   9.198  18.096 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  103.869      7.547  13.763 1.04e-08 ***\ngroup_low     21.670     10.673   2.030   0.0651 .  \ngroup_high    31.143     10.673   2.918   0.0129 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.88 on 12 degrees of freedom\nMultiple R-squared:  0.4272,    Adjusted R-squared:  0.3317 \nF-statistic: 4.475 on 2 and 12 DF,  p-value: 0.03532"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#compare-to-anova",
    "href": "content/07-categorical-predictors/slides/index.html#compare-to-anova",
    "title": "The linear model and comparing multiple groups",
    "section": "Compare to ANOVA",
    "text": "Compare to ANOVA\n\n\n\nour_anova <- aov(y ~ condition, data = d)\n\nsummary(our_anova)\n\n            Df Sum Sq Mean Sq F value Pr(>F)  \ncondition    2   2549  1274.4   4.475 0.0353 *\nResiduals   12   3417   284.8                 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nour_regression <- lm(y ~ group_low + group_high, data = d)\n\nsummary(our_regression)\n\n\nCall:\nlm(formula = y ~ group_low + group_high, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.909  -7.267   4.104   9.198  18.096 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  103.869      7.547  13.763 1.04e-08 ***\ngroup_low     21.670     10.673   2.030   0.0651 .  \ngroup_high    31.143     10.673   2.918   0.0129 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.88 on 12 degrees of freedom\nMultiple R-squared:  0.4272,    Adjusted R-squared:  0.3317 \nF-statistic: 4.475 on 2 and 12 DF,  p-value: 0.03532"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#no-need-to-use-dummies",
    "href": "content/07-categorical-predictors/slides/index.html#no-need-to-use-dummies",
    "title": "The linear model and comparing multiple groups",
    "section": "No need to use dummies",
    "text": "No need to use dummies\nThe lm call will automatically dummy code factors.\n\nsummary(lm(y ~ condition, data = d))\n\n\nCall:\nlm(formula = y ~ condition, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.909  -7.267   4.104   9.198  18.096 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    103.869      7.547  13.763 1.04e-08 ***\nconditionhigh   31.143     10.673   2.918   0.0129 *  \nconditionlow    21.670     10.673   2.030   0.0651 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.88 on 12 degrees of freedom\nMultiple R-squared:  0.4272,    Adjusted R-squared:  0.3317 \nF-statistic: 4.475 on 2 and 12 DF,  p-value: 0.03532"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#in-a-power-simulation",
    "href": "content/07-categorical-predictors/slides/index.html#in-a-power-simulation",
    "title": "The linear model and comparing multiple groups",
    "section": "In a power simulation",
    "text": "In a power simulation\n\nn <- 40\nm1 <- 100\nm2 <- 103\nm3 <- 105\nsd <- 8\ndraws <- 1e4\n\npvalues <- NULL\n\nfor (i in 1:n) {\n\n  group1 <- rnorm(n, m1, sd)\n  group2 <- rnorm(n, m2, sd)\n  group3 <- rnorm(n, m3, sd)\n\n  dat <- data.frame(\n    scores = c(group1, group2, group3),\n    condition = as.factor(rep(c(\"group1\", \"group2\", \"group3\"), each = n))\n  )\n\n  m <- summary(lm(scores ~ condition, data = dat))\n  \n  pvalues[i] <- broom::glance(m)$p.value\n}\n\nsum(pvalues < 0.05) / length(pvalues)\n\n[1] 0.7"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#getting-that-p-value",
    "href": "content/07-categorical-predictors/slides/index.html#getting-that-p-value",
    "title": "The linear model and comparing multiple groups",
    "section": "Getting that p-value",
    "text": "Getting that p-value\nYou can access the p-value by storing the summary in an object (as a list) and accessing its component. For the lm summary, that’s a bit less straightforward. See https://stackoverflow.com/questions/5587676/pull-out-p-values-and-r-squared-from-a-linear-regression."
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#about-that-effect-size",
    "href": "content/07-categorical-predictors/slides/index.html#about-that-effect-size",
    "title": "The linear model and comparing multiple groups",
    "section": "About that effect size",
    "text": "About that effect size\nLet’s just swap the low and high conditions.\n\nd2 <- d\n\nlevels(d2$condition) <- c(\"control\", \"low\", \"high\")"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#now-what-are-the-model-statistics",
    "href": "content/07-categorical-predictors/slides/index.html#now-what-are-the-model-statistics",
    "title": "The linear model and comparing multiple groups",
    "section": "Now what are the model statistics?",
    "text": "Now what are the model statistics?\n\n\n\nsummary(lm(y ~ condition, d))\n\n\nCall:\nlm(formula = y ~ condition, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.909  -7.267   4.104   9.198  18.096 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    103.869      7.547  13.763 1.04e-08 ***\nconditionlow    21.670     10.673   2.030   0.0651 .  \nconditionhigh   31.143     10.673   2.918   0.0129 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.88 on 12 degrees of freedom\nMultiple R-squared:  0.4272,    Adjusted R-squared:  0.3317 \nF-statistic: 4.475 on 2 and 12 DF,  p-value: 0.03532\n\n\n\n\nsummary(lm(y ~ condition, d2))\n\n\nCall:\nlm(formula = y ~ condition, data = d2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.909  -7.267   4.104   9.198  18.096 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    103.869      7.547  13.763 1.04e-08 ***\nconditionlow    31.143     10.673   2.918   0.0129 *  \nconditionhigh   21.670     10.673   2.030   0.0651 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 16.88 on 12 degrees of freedom\nMultiple R-squared:  0.4272,    Adjusted R-squared:  0.3317 \nF-statistic: 4.475 on 2 and 12 DF,  p-value: 0.03532"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#what-about-the-effect-size",
    "href": "content/07-categorical-predictors/slides/index.html#what-about-the-effect-size",
    "title": "The linear model and comparing multiple groups",
    "section": "What about the effect size?",
    "text": "What about the effect size?\n\neffectsize::eta_squared(lm(y ~ condition, d))\n\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\ncondition | 0.43 | [0.03, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\neffectsize::eta_squared(lm(y ~ condition, d2))\n\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\ncondition | 0.43 | [0.03, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00]."
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#how-would-i-go-for-power-now",
    "href": "content/07-categorical-predictors/slides/index.html#how-would-i-go-for-power-now",
    "title": "The linear model and comparing multiple groups",
    "section": "How would I go for power now?",
    "text": "How would I go for power now?\n\nBack to standardized vs. unstandardized\nBest to “draw” first\nDetermine contrast of interest"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#what-about-correlated-measures",
    "href": "content/07-categorical-predictors/slides/index.html#what-about-correlated-measures",
    "title": "The linear model and comparing multiple groups",
    "section": "What about correlated measures?",
    "text": "What about correlated measures?\nRemember drawing two scores from the same unit?\n\\[\n\\begin{bmatrix}\nvar  & cov \\\\\ncov & var \\\\\n\\end{bmatrix}\n\\]\nWe just need to extend that to the number of measures:\n\\[\n\\begin{bmatrix}\nvar & cov & cov\\\\\ncov & var & cov\\\\\ncov & cov & var\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#what-we-need-to-know",
    "href": "content/07-categorical-predictors/slides/index.html#what-we-need-to-know",
    "title": "The linear model and comparing multiple groups",
    "section": "What we need to know",
    "text": "What we need to know\n\nThe standard deviation for each measure (\\(\\sigma_{x_1}\\))\nThe correlation between each measure (\\(r_{x_1x_2}\\))\n\n\\[\n\\begin{bmatrix}\nSD_{x_1} & r_{x_1x_2} & r_{x_1x_3}\\\\\nr_{x_2x_1} & SD_{x_2} & r_{x_2x_3}\\\\\nr_{x_3x_1} & r_{x_3x_2} & SD_{x_3}\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#same-as-before",
    "href": "content/07-categorical-predictors/slides/index.html#same-as-before",
    "title": "The linear model and comparing multiple groups",
    "section": "Same as before",
    "text": "Same as before\nDefine our parameters and get variance-covariance matrix.\n\nmeans <- c(control = 100, low = 103, high = 105)\nsd <- 8\ncorrelation <- 0.4\ncovariance <- correlation * sd * sd\n\nour_matrix <- matrix(\n  c(\n    sd**2, covariance, covariance,\n    covariance, sd**2, covariance,\n    covariance, covariance, sd**2\n  ),\n  ncol = 3\n)\n\nour_matrix\n\n     [,1] [,2] [,3]\n[1,] 64.0 25.6 25.6\n[2,] 25.6 64.0 25.6\n[3,] 25.6 25.6 64.0"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#then-draw-from-multivariate-normal",
    "href": "content/07-categorical-predictors/slides/index.html#then-draw-from-multivariate-normal",
    "title": "The linear model and comparing multiple groups",
    "section": "Then draw from multivariate normal",
    "text": "Then draw from multivariate normal\n\nlibrary(MASS)\n\nset.seed(42)\n\nd <- \n  mvrnorm(\n    200,\n    means,\n    our_matrix\n  )\nd <- as.data.frame(d)\nd$id <- factor(1:200)\nhead(d)\n\n    control       low      high id\n1 102.65124  93.16169  86.70051  1\n2  98.88458 109.62849 109.98486  2\n3  94.79098  97.77544 108.68290  3\n4  90.66920  94.06642 111.49926  4\n5  98.78622 106.29663  95.40168  5\n6  99.54111 110.82243  99.60934  6"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#lets-check",
    "href": "content/07-categorical-predictors/slides/index.html#lets-check",
    "title": "The linear model and comparing multiple groups",
    "section": "Let’s check",
    "text": "Let’s check\nCan we recover our numbers of means = c(100, 103, 105), SD of 8, and correlation of 0.4?\n\nmean(d$control); mean(d$low); mean(d$high)\n\n[1] 99.89351\n\n\n[1] 103.4035\n\n\n[1] 105.214\n\nsd(d$control); sd(d$low); sd(d$high)\n\n[1] 7.854443\n\n\n[1] 7.706208\n\n\n[1] 8.024822\n\ncor(d$control, d$low); cor(d$control, d$high); cor(d$low, d$high);\n\n[1] 0.3166429\n\n\n[1] 0.4616155\n\n\n[1] 0.3740047"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#we-cant-use-lm-this-time",
    "href": "content/07-categorical-predictors/slides/index.html#we-cant-use-lm-this-time",
    "title": "The linear model and comparing multiple groups",
    "section": "We can’t use lm this time",
    "text": "We can’t use lm this time\nThe linear model has several assumptions, one of which is that observations are independent. They clearly aren’t (we specified a correlation between them after all). So we need to go for models that take this dependence into account.\nFor most cases, data need to be in the long format:\n\nlibrary(tidyr)\n\nd2 <- \n  pivot_longer(\n    d,\n    cols = -id,\n    names_to = \"condition\",\n    values_to = \"score\"\n  )"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#long-format",
    "href": "content/07-categorical-predictors/slides/index.html#long-format",
    "title": "The linear model and comparing multiple groups",
    "section": "Long format",
    "text": "Long format\n\n\n\n\n\n\n\ncontrol\nlow\nhigh\nid\n\n\n\n\n102.65124\n93.16169\n86.70051\n1\n\n\n98.88458\n109.62849\n109.98486\n2\n\n\n94.79098\n97.77544\n108.68290\n3\n\n\n90.66920\n94.06642\n111.49926\n4\n\n\n98.78622\n106.29663\n95.40168\n5\n\n\n99.54111\n110.82243\n99.60934\n6\n\n\n\n\n\n\n\n\n\n\n\nid\ncondition\nscore\n\n\n\n\n1\ncontrol\n102.65124\n\n\n1\nlow\n93.16169\n\n\n1\nhigh\n86.70051\n\n\n2\ncontrol\n98.88458\n\n\n2\nlow\n109.62849\n\n\n2\nhigh\n109.98486"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#lets-run-the-rm-anova",
    "href": "content/07-categorical-predictors/slides/index.html#lets-run-the-rm-anova",
    "title": "The linear model and comparing multiple groups",
    "section": "Let’s run the RM ANOVA",
    "text": "Let’s run the RM ANOVA\n\nmodel <- aov(score ~ condition + Error(id), d2)\nsummary(model)\n\n\nError: id\n           Df Sum Sq Mean Sq F value Pr(>F)\nResiduals 199  21774   109.4               \n\nError: Within\n           Df Sum Sq Mean Sq F value   Pr(>F)    \ncondition   2   2927    1464   38.48 5.25e-16 ***\nResiduals 398  15135      38                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#just-a-hierarchical-linear-model",
    "href": "content/07-categorical-predictors/slides/index.html#just-a-hierarchical-linear-model",
    "title": "The linear model and comparing multiple groups",
    "section": "Just a (hierarchical) linear model",
    "text": "Just a (hierarchical) linear model\n\n\n\nsummary(model)\n\n\nError: id\n           Df Sum Sq Mean Sq F value Pr(>F)\nResiduals 199  21774   109.4               \n\nError: Within\n           Df Sum Sq Mean Sq F value   Pr(>F)    \ncondition   2   2927    1464   38.48 5.25e-16 ***\nResiduals 398  15135      38                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nlibrary(lme4)\n\nmodel2 <- lmer(score ~ condition + (1 | id), d2)\nanova(model2)\n\nAnalysis of Variance Table\n          npar Sum Sq Mean Sq F value\ncondition    2   2927  1463.5  38.484\n\n\n\n\nCheck here for more background info."
  },
  {
    "objectID": "content/07-categorical-predictors/slides/index.html#this-is-the-max",
    "href": "content/07-categorical-predictors/slides/index.html#this-is-the-max",
    "title": "The linear model and comparing multiple groups",
    "section": "This is the max",
    "text": "This is the max\nThe workshop won’t go further than (interactions) with repeated measures. More resources on simulating more complicated designs at the final wrap up."
  },
  {
    "objectID": "content/08-interactions/08-exercise.html#exercise",
    "href": "content/08-interactions/08-exercise.html#exercise",
    "title": "Exercise V",
    "section": "0.1 Exercise",
    "text": "0.1 Exercise\nYou plan to analyze data from a content analysis. You’re interested in what predicts the number of Twitter followers for celebrities (including C-list celebrities, whatever that means). You believe that blue check mark definitely means having more followers compared to not having a check mark. Your dependent measure is, well, just the number of followers. You also expect that this difference will be stronger if a celebrity is more active, say tweets at least once a week.\nNow you want to run a power analysis to test that idea. First draw (on a piece of paper or digitally) your interaction. You can use the function below to try out some “drawings”. Start with putting in some of the means to roughly get the pattern described above.\n\nlibrary(tidyverse)\n\ntwobytwo <-\n  function(\n    means = c(0, 0, 0, 0),\n    factors = c(\"Factor 1\", \"Factor 2\"),\n    levels1 = c(\"Level 1\", \"Level 2\"),\n    levels2 = c(\"Level 1\", \"Level 2\"),\n    outcome = \"Outcome\"\n  ){\n    d <- \n      data.frame(\n        f1 = rep(levels1, times = 2),\n        f2 = rep(levels2, each = 2),\n        outcome = means\n      )\n    \n    names(d) <- c(factors, outcome)\n    \n    p <- \n      ggplot(d, aes(x = .data[[factors[1]]], y = .data[[outcome]], shape = .data[[factors[2]]], group = .data[[factors[2]]])) +\n      geom_point(size = 3) +\n      geom_line(aes(linetype = .data[[factors[2]]]), size = 1) +\n      theme_bw() +\n      theme(\n        axis.ticks.y = element_blank(),\n        axis.text.y = element_blank()\n      )\n    \n    return(p)\n  }\n\nFor example, the below would be a complete reversal:\n\ntwobytwo(\n  means = c(0, 2, 2, 0),\n  factors = c(\"Verified\", \"Activity\"),\n  levels1 = c(\"No\", \"Yes\"),\n  levels2 = c(\"Lurker\", \"Active\"),\n  outcome = \"Followers\"\n)\n\n\n\n\n\ntwobytwo(\n  means = c(2, 5, 2.5, 5.6),\n  factors = c(\"Verified\", \"Activity\"),\n  levels1 = c(\"No\", \"Yes\"),\n  levels2 = c(\"Lurker\", \"Active\"),\n  outcome = \"Followers\"\n)\n\n\n\n\nFor our case, we’re interested in an attenuation effect: There’s an effect of verification (well, it’s not a causal effect in that sense, but let’s not get into that for now) regardless of the activity type, but it’ll be stronger with higher activity. How much stronger? And what follower counts should you expect for the different groups? This is where common sense and thinking about effect sizes on the raw scale comes in. Let’s go back to the linear model:\n\\(Characters = \\beta_0 + \\beta_1Verification + \\beta_2Activity + \\beta_3Verification \\times Activity + \\epsilon\\)\nGo back to the slides if you need a refresher what each beta represents. Let’s talk about our expectations. Let’s say we expect those without verification and low activity to have something like 20,000 followers. We expect that verification makes a big difference, such that verified celebrities, even when they’re not active, have, on average, 50,000 followers. Now comes the tricky bit: How many more followers will an unverified, but active person have? Let’s say it’s 25,000. As for the interaction: How much does activity add to the already massive difference between unverified and verified lurkers? Let’s say it adds 1,000 followers.\nRun a power analysis with these values where you figure out how many participants you need per group. “Recruiting” people won’t be difficult, so all you need is a rough estimate. Start at 500 celebrities and then go up in steps of 20 until 1,500. Use the linear model from above to create the scores. As for error: You’re really not sure about your estimates, so you add a lot of erro. Use a normally distributed error term of 5,000 with a standard deviation of 5,000. For now, you’re not interested in the actual contrasts, only in whether an interaction is present. So run an lm model with and one without the interaction term and compare them with anova. Do 1,000 runs per combo. Also, you really don’t want to commit a Type I error, so you set your alpha to 0.001.\n\nset.seed(42)\n\nb0 <- 2e4\nb1 <- 3e4\nb2 <- 5e3\nb3 <- 1e3\nsizes <- seq(500, 1500, 20)\nruns <- 1e3\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nfor (n in sizes) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    \n    d <- \n      data.frame(\n        Verification = rep(0:1, times = n*2),\n        Activity = rep(0:1, each = n*2)\n      )\n    \n    d$Followers <- b0 + b1*d$Verification + b2*d$Activity + b3*d$Verification*d$Activity + rnorm(n, 5e3, 5e3)\n    \n    mains <- lm(Followers ~ Verification + Activity, d)\n    interactions <- lm(Followers ~ Verification*Activity, d)\n    \n    pvalues[i] <- anova(mains, interactions)$`Pr(>F)`[2]\n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        power = sum(pvalues < .001) / length(pvalues)\n      )\n    )\n}\n\nplot(outcomes$sample_size, outcomes$power)"
  },
  {
    "objectID": "content/08-interactions/08-exercise.html#exercise-1",
    "href": "content/08-interactions/08-exercise.html#exercise-1",
    "title": "Exercise V",
    "section": "0.2 Exercise",
    "text": "0.2 Exercise\nWe conduct an experiment. Previous research has shown that reading a story in the first person leads to more enjoyment on a 7-point Likert-scale than in the third person. For your thesis, you want to understand that effect better. You believe that the effect will be stronger if there’s lots of action compared to a tame story. In fact, you think that story type can completely knock out the effect (see here).\nThe linear model for enjoyment, measured on a 7-point Likert-scale, is as follows:\n\\(Enjoyment = \\beta_0 + \\beta_1Perspective + \\beta_2Type + \\beta_3Perspective \\times Type\\)\nYou believe the \\(\\beta\\) are as follows:\n\\[\\begin{align}\n&\\beta_0 = 4.3\\\\\n&\\beta_1 = 0.01\\\\\n&\\beta_2 = -0.03\\\\\n&\\beta_3 = 0.4\n\\end{align}\\]\nFirst draw the interaction to make sure you understand what it’s supposed to look like (either on paper or digitally). This time, you’d like to decompose the error, meaning rather than adding overall variance, you want to add variance per group. Therefore, you simulate the data with four group means, not with the linear model. You set the SD to 0.6 for all groups, except for the first-person action condition where you expect opinions will be a bit more divided, setting the SD here to 0.9.\nEven though you want to power for the interaction, you’re also interested in main effects you can interpret. Therefore, use type 3 sums of squares with contrast coding for the factors. Start at 30 participants per group and keep going in steps of 5 per group until you reach 95%. Because this is exploratory, and you really don’t want to miss an effect, you set your alpha to 0.15. Do 1,000 runs per combo.\n\nset.seed(42)\n\nmeans <- c(4.3, 4.31, 4.27, 4.7)\nsds <- c(rep(0.6, 3), 0.9)\nalpha <- 0.15\ndraws <- 1e3\nn <- 30\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\npower <- 0\n\nwhile (power < .95) {\n  \n  power <- NULL\n  pvalues <- NULL\n  \n  for (i in 1:draws) {\n    \n    d <- \n      data.frame(\n        id = factor(1:c(4*n)),\n        Enjoyment = rnorm(n*4, means, sds),\n        Perspective = factor(rep(c(\"1st Person\", \"3rd Person\"), times = n*2)),\n        Action = factor(rep(c(\"Lame\", \"Lots of action\"), times = n, each = 2))\n      )\n    \n    m <- suppressMessages(afex::aov_car(Enjoyment ~ Perspective*Action + Error(id), data = d))\n    \n    pvalues[i] <- m$anova_table$`Pr(>F)`[3]\n  }\n  \n  power <- sum(pvalues < alpha) / length(pvalues)\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        power = power \n      )\n    )\n  \n  n <- n + 5\n}\n\nplot(outcomes$sample_size, outcomes$power)"
  },
  {
    "objectID": "content/08-interactions/08-exercise.html#exercise-2",
    "href": "content/08-interactions/08-exercise.html#exercise-2",
    "title": "Exercise V",
    "section": "0.3 Exercise",
    "text": "0.3 Exercise\nGPower will give you the same sample size for a power analysis for an interaction as for a t-test if the effect size of the interaction is the same as in the t-test. Let’s check that. The effect size is the same if the effect is completely reversed (see here once more). Let’s say our first experiment produced a difference between control and treatment of 15 points. The mean and SD of the control were 100 and 15. For treatment: 115 and 15. That’s a massive effect size of one standard deviation. Now would a complete reversal look like? As in: A second factor completely reverse the effect of the first factor. Like this, using the plotting function from above:\n\ntwobytwo(means = c(100, 115, 115, 100))\n\n\n\n\nFirst, calculate power for the original effect (Circles above in the graph) in GPower (two-tailed). Alpha is 0.05 and power should be 95%. The groups should have the same size. Now simulate the above interaction and calculate power for the interaction effect. Compare your estimate to the GPower estimate you just got. Go about it as you think is best. (Tip: Turn the solution into a function; you’ll need it again for the next exercises.)\n\nset.seed(42)\n\ninteraction_power <- function(\n    means = c(100, 115, 115, 100),\n    sd = 15,\n    sizes = 1:20,\n    draws = 1e3\n) {\n  outcomes <- \n    data.frame(\n      sample_size = NULL,\n      power = NULL\n    )\n  \n  for (n in sizes) {\n    \n    pvalues <- NULL\n    \n    for (i in 1:draws) {\n      \n      d <- \n        data.frame(\n          id = factor(1:c(4*n)),\n          scores = rnorm(n*4, means, sd),\n          factor1 = factor(rep(c(\"level1\", \"level2\"), times = n*2)),\n          factor2 = factor(rep(c(\"level1\", \"level2\"), times = n, each = 2))\n        )\n      \n      mains <- lm(scores ~ factor1 + factor2, d)\n      interactions <- lm(scores ~ factor1 + factor2 + factor1:factor2, d)\n      \n      pvalues[i] <- anova(mains, interactions)$`Pr(>F)`[2]\n    }\n    \n    outcomes <- \n      rbind(\n        outcomes,\n        data.frame(\n          sample_size = n,\n          power = sum(pvalues < 0.05) / length(pvalues)\n        )\n      )\n  }\n  \n  return(outcomes)\n}\n\nresults <- interaction_power(draws = 200)\n\nplot(results$sample_size, results$power)"
  },
  {
    "objectID": "content/08-interactions/08-exercise.html#exercise-3",
    "href": "content/08-interactions/08-exercise.html#exercise-3",
    "title": "Exercise V",
    "section": "0.4 Exercise",
    "text": "0.4 Exercise\nWhat if an effect is attenuated? Say the original effect was between two groups with means of 4.4 and 4.9 and SDs of 0.6 and 0.7, respectively. Calculate Cohen’s d, then put that into GPower for an independent t-test (two-tailed) with an alpha of 0.05. Note down the sample size needed for 95% power. Next, we introduce a second factor that attenuates the original effect, such that the effect is half is large when we consider the second condition. The graph below (hopefully) shows what I mean:\n\ntwobytwo(\n  means = c(0, 1, 0, 0.5),\n  factors = c(\"Comparison\", \"Which condition?\"),\n  levels1 = c(\"A\", \"B\"),\n  levels2 = c(\"Old condition\", \"New condition\")\n)\n\n\n\n\nThat means, for the two groups under the second condition, we need to half the effect size. First, half the effect size that you put into GPower and calculate the sample size again. Note down the number you get. Then get to simulation: For the new (half-sized) two groups, half the difference between them. The SDs can be the same as for the first two groups. Calculate power for the interaction effect and compare the sample size needed for 95% to the two estimates you got from GPower. To save yourself time, use the function you wrote for the previous exercise. Also, use the GPower estimates as a ballpark figure to set your sample sizes in the simulation. What does adding an interaction add to your sample size? Compare to the case here. Tip: You’ll need to calculate the standardized effect size first for GPower; then set the means for the new two groups in relation to that standardized effect size.\n\nset.seed(42)\n\nm1 <- 4.4\nm2 <- 4.9\nsd1 <- 0.6\nsd2 <- 0.7\npooled_sd <- sqrt((sd1**2 + sd2**2)/2)\nd <- (m2-m1)/pooled_sd\n\nmeans <- c(m1, m2, m1, m1 + d/2*pooled_sd)\n\nresults <- interaction_power(\n  means = means,\n  sd = rep(c(sd1, sd2), 2),\n  sizes = seq(100, 800, 50),\n  draws = 1e3\n)\n\nplot(results$sample_size, results$power)"
  },
  {
    "objectID": "content/08-interactions/08-exercise.html#exercise-4",
    "href": "content/08-interactions/08-exercise.html#exercise-4",
    "title": "Exercise V",
    "section": "0.5 Exercise",
    "text": "0.5 Exercise\nSo far, we’ve only considered the overall interaction effect. But a significant interaction can mean many different patterns, which is why we always follow up with simple effects. Take the attenuation from the previous exercise: There are 6 comparisons that could be of interest to us, one for each possible contrasts. Say for our hypothesis of attenuation to hold, we’re interested in two simple effects: The comparison of the two levels under the old condition and the comparison of the two levels under the new condition.\nTherefore, repeat the above simulation, but this time also extract the p-values of these two posthoc simple effects. You can do that with pairs(emmeans::emmeans(interactions, \"factor1\", by = \"factor2\")). Save the power of the overall interaction term and the two simple effects and plot them. You can use the following code, assuming your data frame is called results, has a variable with the sample size sample_size, the type of power type, and power. You’ll need to adjust your function above or start from scratch. What’s the power of the simple effects compared to the overall interaction effect?\n\nlibrary(ggplot2)\n\nggplot(results, aes(x = sample_size, y = power, color = type, group = type)) + geom_point() + theme_bw()\n\n\nset.seed(42)\n\ninteraction_power <- function(\n    means = c(4.4, 4.9, 4.4, 4.65),\n    sd = rep(c(sd1, sd2), 2),\n    sizes = seq(50, 600, 50),\n    draws = 1e3\n) {\n  outcomes <- \n    data.frame(\n      sample_size = NULL,\n      type = NULL,\n      power = NULL\n    )\n  \n  for (n in sizes) {\n    \n    pvalues <- NULL\n    pvalues_simple1 <- NULL\n    pvalues_simple2 <- NULL\n    \n    for (i in 1:draws) {\n      \n      d <- \n        data.frame(\n          id = factor(1:c(4*n)),\n          scores = rnorm(n*4, means, sd),\n          factor1 = factor(rep(c(\"level1\", \"level2\"), times = n*2)),\n          factor2 = factor(rep(c(\"level1\", \"level2\"), times = n, each = 2))\n        )\n      \n      mains <- lm(scores ~ factor1 + factor2, d)\n      interactions <- lm(scores ~ factor1 + factor2 + factor1:factor2, d)\n      \n      ps <- data.frame(pairs(emmeans::emmeans(interactions, \"factor1\", by = \"factor2\")))\n      \n      pvalues[i] <- anova(mains, interactions)$`Pr(>F)`[2]\n      pvalues_simple1[i] <- ps$p.value[1]\n      pvalues_simple2[i] <- ps$p.value[2]\n    }\n    \n    outcomes <- \n      rbind(\n        outcomes,\n        data.frame(\n          sample_size = rep(n, 3),\n          type = factor(c(\"interaction\", \"simple full\", \"simple half\")),\n          power = c(\n            sum(pvalues < 0.05) / length(pvalues),\n            sum(pvalues_simple1 < 0.05) / length(pvalues_simple1),\n            sum(pvalues_simple2 < 0.05) / length(pvalues_simple2)\n          )\n        )\n      )\n  }\n  \n  return(outcomes)\n}\n\nresults <- interaction_power(draws = 500)\n\nlibrary(ggplot2)\n\nggplot(results, aes(x = sample_size, y = power, color = type, group = type)) + geom_point() + theme_bw()"
  },
  {
    "objectID": "content/08-interactions/08-exercise.html#exercise-5",
    "href": "content/08-interactions/08-exercise.html#exercise-5",
    "title": "Exercise V",
    "section": "0.6 Exercise",
    "text": "0.6 Exercise\nYou want to know what leads people to change their opinion about an issue. Previous research shows that the amount of arguments leads to stronger persuasiveness of a message. That effect will be knocked out, you believe, if the message comes from a source with low credibility. Therefore, you expect something like this:\n\n\n\n\n\nYou go for a repeated measures design, where people read four stories, one in each condition, and report how persuasive they find the story to be. (For the sake of argument, let’s ignore order or carry-over effects). You measure the outcome on a 100-point scale. Without knowing any better, you assume the no arguments, low credibility condition will fall on the middle of the scale. That makes it easy, because only the several arguments, high credibility condition will differ from the other three. You want to build in a reasonable amount of uncertainty, so you choose an SD of 20–this way, most of your values will be within 10 and 90 on the rating scale (50-+2*SD). As for the effect: Your SESOI is 10 points–anything below that is too small to care about in your opinion.\nRun a simulation where you draw from a multivariate normal distribution. The correlation between measures should be fairly low, 0.3. Power for the interaction effect (remember sum-to-zero contrasts). Start at 40 people and go up in steps of 5 until you reach 150. You’ll need to do some data transformations to get the data in the right (= long) format.\n\nlibrary(MASS)\n\nset.seed(42)\nsesoi <- 10\nmeans <- c(none_low = 50, several_low = 50, none_high = 50, several_high = 50 + sesoi)\nsd <- 20\ncorrelation <- 0.3\nruns <- 500\nsizes <- seq(20, 150, 10)\n\ncovariance <- correlation * sd * sd\nsigma <- matrix(\n  c(\n    sd**2, covariance, covariance, covariance,\n    covariance, sd**2, covariance, covariance,\n    covariance, covariance, sd**2, covariance,\n    covariance, covariance, covariance, sd**2\n  ),\n  ncol = 4\n)\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nfor (n in sizes) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n      \n      d <- mvrnorm(\n        n,\n        means,\n        sigma\n      )\n      \n      d <- as.data.frame(d)\n      \n      d$id <- factor(1:n)\n      \n      d <- pivot_longer(\n        d,\n        cols = -id,\n        values_to = \"scores\",\n        names_to = \"condition\"\n      ) %>% \n        separate(condition, into = c(\"Arguments\", \"Credibility\"), sep = \"_\") %>% \n        mutate(\n          across(c(\"Arguments\", \"Credibility\"), as.factor),\n        )\n      \n      m <- summary(afex::aov_car(scores ~ Arguments*Credibility + Error(id/Arguments*Credibility), d))\n      \n      pvalues[i] <- as.numeric(unlist(m)[26])\n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        power = sum(pvalues < 0.05) / length(pvalues)\n      )\n    )\n}\n\n\nplot(outcomes$sample_size, outcomes$power)"
  },
  {
    "objectID": "content/08-interactions/08-exercise.html#exercise-6",
    "href": "content/08-interactions/08-exercise.html#exercise-6",
    "title": "Exercise V",
    "section": "0.7 Exercise",
    "text": "0.7 Exercise\nYou have a natural experiment coming up because you know that some users in the UK get a new streaming service before users in France. You want to know whether access to another streaming service increases satisfaction with people’s media diet. From the waiting list for that streaming service, you want to sample comparable users in the UK and France, ask them how satisfied they are with their media variety, wait for a month to give the UK users time to try out the new streaming service whereas the French users will be the control group. In other words, you have a mixed design with a pre-post measure within, but condition aka country (access to streaming) between.\nYou need to run a power analysis. For your measure of satisfaction, you use a 7-point Likert-scale. You assume that France at the pre-measure will score above the midpoint, given how many providers are already out there, say 3.9. You also expect no change, maybe even a slight decline seeing how the UK is getting access, so you set the post-measure to 3.8. For the UK, you expect a somewhat higher satisfaction already at pre-measure because they usually get stuff directly after the US, say 4.3. Importantly, you believe access to this new streaming service will increase satisfaction by 0.4 points–that’s the mimimum amount of change on that measure that predicts more users in the future. As for the SDs: You don’t see any reason why variation would increase from pre- to post-measure. However, you do believe there’s a bit more variation in France than in the UK based on previous research with large samples. You set the SD for France to 1.2, but for the UK to 0.9. As for correlations between pre- and post-measures: You expect France to be more consistent (0.6) than the UK (0.4).\nSimulate power. You’ll need to do a variance-covariance matrix for correlated measures for each country. Base your power analysis on the interaction effect, not the simple effects. To run the ANOVA, you’ll need to properly nest the error, meaning Error(id/Time) in the aov_car command. You have money for a max of N total participants.You set your alpha to 0.01 and want to stop at 90% power. Start at 100 people per country and go up in steps of 20.\n\nlibrary(MASS)\n\nset.seed(42)\n\nmeans_france <- c(pre = 3.8, post = 3.9)\nsd_france <- 1.2\ncor_france <- 0.6\ncov_france <- cor_france * sd_france * sd_france\nmeans_uk <- c(pre = 4.3, post = 4.7)\nsd_uk <- 0.9\ncor_uk <- 0.4\ncov_uk <- cor_uk * sd_uk * sd_uk\nalpha <- 0.01\nn <- 100\nruns <- 500\n\nsigma_france <- \n  matrix(\n    c(\n      sd_france**2, cov_france,\n      cov_france, sd_france**2\n    ),\n    ncol = 2\n  )\n\nsigma_uk <- \n  matrix(\n    c(\n      sd_uk**2, cov_uk,\n      cov_uk, sd_uk**2\n    ),\n    ncol = 2\n  )\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\npower <- 0\n\nwhile (power < 0.90) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    \n    france <- mvrnorm(\n      n,\n      means_france,\n      sigma_france\n    )\n    \n    france <- as.data.frame(france)\n    france$country <- factor(\"france\")\n    france$id <- 1:n\n    \n    uk <- mvrnorm(\n      n,\n      means_uk,\n      sigma_uk\n    )\n    \n    uk <- as.data.frame(uk)\n    uk$country <- factor(\"uk\")\n    uk$id <- (n+1):(2*n)\n    \n    d <- rbind(france, uk)\n    \n    d <- d %>% \n      pivot_longer(\n        cols = c(pre, post),\n        names_to = \"time\",\n        values_to = \"satisfaction\"\n        \n      )\n    \n    m <- suppressMessages(summary(afex::aov_car(satisfaction ~ country*time + Error(id/time), d), type = 3))\n      \n    pvalues[i] <- as.numeric(unlist(m)[26])\n  }\n  \n  power <- sum(pvalues < alpha) / length(pvalues)\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        power = power\n      )\n    )\n  \n  n <- n + 20\n}\n\nplot(outcomes$sample_size, outcomes$power)"
  },
  {
    "objectID": "content/08-interactions/08-slides.html",
    "href": "content/08-interactions/08-slides.html",
    "title": "Slides",
    "section": "",
    "text": "If you want to see the slides in full screen, you can click here. To download them as PDFs, hit the ‘e’ button when you got the presentation open and then Ctrl+P print to PDF. This will work best in Chrome (in Firefox, it didn’t print the headings for me)."
  },
  {
    "objectID": "content/08-interactions/slides/index.html#whats-an-interaction",
    "href": "content/08-interactions/slides/index.html#whats-an-interaction",
    "title": "Interactions",
    "section": "What’s an interaction?",
    "text": "What’s an interaction?\n\nTells us whether the effect of one variable depends on another variable\nExtension of the linear model\nIn effect, it just checks whether lines are parallel"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#is-there-an-interaction",
    "href": "content/08-interactions/slides/index.html#is-there-an-interaction",
    "title": "Interactions",
    "section": "Is there an interaction?",
    "text": "Is there an interaction?"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#how-about-now",
    "href": "content/08-interactions/slides/index.html#how-about-now",
    "title": "Interactions",
    "section": "How about now?",
    "text": "How about now?"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#and-now",
    "href": "content/08-interactions/slides/index.html#and-now",
    "title": "Interactions",
    "section": "And now?",
    "text": "And now?"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#definitely-now",
    "href": "content/08-interactions/slides/index.html#definitely-now",
    "title": "Interactions",
    "section": "Definitely now",
    "text": "Definitely now"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#whats-the-interaction-then",
    "href": "content/08-interactions/slides/index.html#whats-the-interaction-then",
    "title": "Interactions",
    "section": "What’s the interaction then?",
    "text": "What’s the interaction then?\n\nIn all of these models, we need to ask what extra information about the outcome being in both groups gives us beyond individual group membership\nIf the line isn’t parallel, combined group membership gives us additional info than just individual group membership\nWhen we simulate, that’s our data generating process: The info each group membership carries as well as their combination"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#back-to-the-linear-model",
    "href": "content/08-interactions/slides/index.html#back-to-the-linear-model",
    "title": "Interactions",
    "section": "Back to the linear model",
    "text": "Back to the linear model\nSay we want to see the effect of playing a violent game on feelings of aggression, but suspect the effect depends on the difficulty of the game. We have 2 factors with 2 levels each:\n\nGame: Peaceful vs. Violent\nDifficulty: Easy vs. Hard"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#both-as-dummies",
    "href": "content/08-interactions/slides/index.html#both-as-dummies",
    "title": "Interactions",
    "section": "Both as dummies",
    "text": "Both as dummies\nWe code both as dummies:\n\n\n\nGame\nDifficulty\nx1\nx2\n\n\n\n\nPeaceful\nEasy\n0\n0\n\n\nViolent\nEasy\n1\n0\n\n\nPeaceful\nHard\n0\n1\n\n\nViolent\nHard\n1\n1"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#in-our-linear-model",
    "href": "content/08-interactions/slides/index.html#in-our-linear-model",
    "title": "Interactions",
    "section": "In our linear model",
    "text": "In our linear model\n\\[\nAggression = \\beta_0 + \\beta_1Game + \\beta_2Difficulty + \\beta_3Game \\times Difficulty\n\\]"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#lets-go-through",
    "href": "content/08-interactions/slides/index.html#lets-go-through",
    "title": "Interactions",
    "section": "Let’s go through",
    "text": "Let’s go through\n\n\n\nGame\nDifficulty\nx1\nx2\n\n\n\n\nPeaceful\nEasy\n0\n0\n\n\nViolent\nEasy\n1\n0\n\n\nPeaceful\nHard\n0\n1\n\n\nViolent\nHard\n1\n1\n\n\n\n\\[\\begin{align}\n& Aggression = \\beta_0 + \\beta_1Game + \\beta_2Difficulty + \\beta_3Game \\times Difficulty\\\\\n& Aggression = \\beta_0 + \\beta_1 \\times 0 + \\beta_2 \\times 0 + \\beta_3 \\times 0 \\times 0\\\\\n& Aggression = \\beta_0\n\\end{align}\\]"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#pictures-please",
    "href": "content/08-interactions/slides/index.html#pictures-please",
    "title": "Interactions",
    "section": "Pictures, please",
    "text": "Pictures, please\n\n\\(\\beta_0\\) is the mean of a peaceful and easy game."
  },
  {
    "objectID": "content/08-interactions/slides/index.html#lets-go-through-1",
    "href": "content/08-interactions/slides/index.html#lets-go-through-1",
    "title": "Interactions",
    "section": "Let’s go through",
    "text": "Let’s go through\n\n\n\nGame\nDifficulty\nx1\nx2\n\n\n\n\nPeaceful\nEasy\n0\n0\n\n\nViolent\nEasy\n1\n0\n\n\nPeaceful\nHard\n0\n1\n\n\nViolent\nHard\n1\n1\n\n\n\n\\[\\begin{align}\n& Aggression = \\beta_0 + \\beta_1Game + \\beta_2Difficulty + \\beta_3Game \\times Difficulty\\\\\n& Aggression = \\beta_0 + \\beta_1 \\times 1 + \\beta_2 \\times 0 + \\beta_3 \\times 1 \\times 0\\\\\n& Aggression = \\beta_0 + \\beta_1\n\\end{align}\\]"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#pictures-please-1",
    "href": "content/08-interactions/slides/index.html#pictures-please-1",
    "title": "Interactions",
    "section": "Pictures, please",
    "text": "Pictures, please\n\n\\(\\beta_1\\) is the difference between a peaceful and a violent game for easy games."
  },
  {
    "objectID": "content/08-interactions/slides/index.html#lets-go-through-2",
    "href": "content/08-interactions/slides/index.html#lets-go-through-2",
    "title": "Interactions",
    "section": "Let’s go through",
    "text": "Let’s go through\n\n\n\nGame\nDifficulty\nx1\nx2\n\n\n\n\nPeaceful\nEasy\n0\n0\n\n\nViolent\nEasy\n1\n0\n\n\nPeaceful\nHard\n0\n1\n\n\nViolent\nHard\n1\n1\n\n\n\n\\[\\begin{align}\n& Aggression = \\beta_0 + \\beta_1Game + \\beta_2Difficulty + \\beta_3Game \\times Difficulty\\\\\n& Aggression = \\beta_0 + \\beta_1 \\times 0 + \\beta_2 \\times 1 + \\beta_3 \\times 0 \\times 1\\\\\n& Aggression = \\beta_0 + \\beta_2\n\\end{align}\\]"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#pictures-please-2",
    "href": "content/08-interactions/slides/index.html#pictures-please-2",
    "title": "Interactions",
    "section": "Pictures, please",
    "text": "Pictures, please\n\n\\(\\beta_2\\) is the difference between an easy and a difficult game for peaceful games."
  },
  {
    "objectID": "content/08-interactions/slides/index.html#lets-go-through-3",
    "href": "content/08-interactions/slides/index.html#lets-go-through-3",
    "title": "Interactions",
    "section": "Let’s go through",
    "text": "Let’s go through\n\n\n\nGame\nDifficulty\nx1\nx2\n\n\n\n\nPeaceful\nEasy\n0\n0\n\n\nViolent\nEasy\n1\n0\n\n\nPeaceful\nHard\n0\n1\n\n\nViolent\nHard\n1\n1\n\n\n\n\\[\\begin{align}\n& Aggression = \\beta_0 + \\beta_1Game + \\beta_2Difficulty + \\beta_3Game \\times Difficulty\\\\\n& Aggression = \\beta_0 + \\beta_1 \\times 1 + \\beta_2 \\times 1 + \\beta_3 \\times 1 \\times 1\\\\\n& Aggression = \\beta_0 + \\beta_2 + \\beta_3\n\\end{align}\\]"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#pictures-please-3",
    "href": "content/08-interactions/slides/index.html#pictures-please-3",
    "title": "Interactions",
    "section": "Pictures, please",
    "text": "Pictures, please\n\n\\(\\beta_3\\) is the extra difference between a peaceful and violent game when the game is also difficult."
  },
  {
    "objectID": "content/08-interactions/slides/index.html#lets-put-that-into-numbers",
    "href": "content/08-interactions/slides/index.html#lets-put-that-into-numbers",
    "title": "Interactions",
    "section": "Let’s put that into numbers",
    "text": "Let’s put that into numbers\nLet’s say we have 50 people per group.\n\nd <- \n  data.frame(\n    Game = rep(0:1, times = 50*2),\n    Difficulty = rep(0:1, each = 50*2)\n  )\n\ntable(d$Game, d$Difficulty)\n\n   \n     0  1\n  0 50 50\n  1 50 50"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#lets-put-that-into-numbers-1",
    "href": "content/08-interactions/slides/index.html#lets-put-that-into-numbers-1",
    "title": "Interactions",
    "section": "Let’s put that into numbers",
    "text": "Let’s put that into numbers\nThen we imitate our regression equation (and add some error):\n\\(Aggression = \\beta_0 + \\beta_1Game + \\beta_2Difficulty + \\beta_3Game \\times Difficulty\\)\n\nb0 <- 0 # peaceful and easy\nb1 <- 2 # difference between b0 and violent and easy\nb2 <- 3 # difference between b0 and peaceful and hard\nb3 <- 1 # the \"extra\"\n\nset.seed(42)\n\nd$Aggression <- \n  b0 + b1*d$Game + b2*d$Difficulty + b3*d$Game*d$Difficulty + rnorm(200, 0, 1)"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#where-are-our-values",
    "href": "content/08-interactions/slides/index.html#where-are-our-values",
    "title": "Interactions",
    "section": "Where are our values?",
    "text": "Where are our values?\n\nb0 <- 0 # peaceful and easy\nb1 <- 2 # difference between b0 and violent and easy\nb2 <- 3 # difference between b0 and peaceful and hard\nb3 <- 1 # the \"extra\""
  },
  {
    "objectID": "content/08-interactions/slides/index.html#lets-recover-our-values",
    "href": "content/08-interactions/slides/index.html#lets-recover-our-values",
    "title": "Interactions",
    "section": "Let’s recover our values",
    "text": "Let’s recover our values\n\nsummary(lm(Aggression ~ Game*Difficulty, d))\n\n\nCall:\nlm(formula = Aggression ~ Game * Difficulty, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.02981 -0.58291  0.04749  0.61717  2.72220 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      0.03672    0.13845   0.265    0.791    \nGame             1.99159    0.19579  10.172  < 2e-16 ***\nDifficulty       2.80863    0.19579  14.345  < 2e-16 ***\nGame:Difficulty  1.14275    0.27689   4.127 5.43e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.979 on 196 degrees of freedom\nMultiple R-squared:  0.8298,    Adjusted R-squared:  0.8272 \nF-statistic: 318.6 on 3 and 196 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#might-as-well-go-for-means",
    "href": "content/08-interactions/slides/index.html#might-as-well-go-for-means",
    "title": "Interactions",
    "section": "Might as well go for means?",
    "text": "Might as well go for means?\n\n\n\n\n\n\n\n\n\n\n\nGame\nDifficulty\nBetas\nValues\nMeans\n\n\n\n\nPeaceful\nEasy\n\\(\\beta_0\\)\n0\n0\n\n\nViolent\nEasy\n\\(\\beta_0+\\beta_1\\)\n0+2\n2\n\n\nPeaceful\nHard\n\\(\\beta_0+\\beta_2\\)\n0+3\n3\n\n\nViolent\nHard\n\\(\\beta_0+\\beta_1+\\beta_2+\\beta_3\\)\n0+2+3+1\n6"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#directly-make-the-groups",
    "href": "content/08-interactions/slides/index.html#directly-make-the-groups",
    "title": "Interactions",
    "section": "Directly make the groups",
    "text": "Directly make the groups\n\nset.seed(42)\n\npeace_easy <- 0\nviolent_easy <- 2\npeace_hard <- 3\nviolent_hard <- 6\n\nd <- \n  data.frame(\n    Game = rep(c(0, 1), each = 50, times = 2),\n    Difficulty = rep(c(0, 1), each = 50*2),\n    Aggression = c(\n      rnorm(50, peace_easy, 1),\n      rnorm(50, violent_easy, 1),\n      rnorm(50, peace_hard, 1),\n      rnorm(50, violent_hard, 1)\n    )\n  )"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#same-result",
    "href": "content/08-interactions/slides/index.html#same-result",
    "title": "Interactions",
    "section": "Same result",
    "text": "Same result\n\nsummary(lm(Aggression ~ Game*Difficulty, d))\n\n\nCall:\nlm(formula = Aggression ~ Game * Difficulty, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-3.09379 -0.57416  0.02313  0.58649  2.85314 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)     -0.03567    0.13829  -0.258 0.796720    \nGame             2.13637    0.19557  10.924  < 2e-16 ***\nDifficulty       2.88442    0.19557  14.748  < 2e-16 ***\nGame:Difficulty  0.99116    0.27658   3.584 0.000428 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9779 on 196 degrees of freedom\nMultiple R-squared:  0.8323,    Adjusted R-squared:  0.8297 \nF-statistic: 324.1 on 3 and 196 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#a-more-extreme-example",
    "href": "content/08-interactions/slides/index.html#a-more-extreme-example",
    "title": "Interactions",
    "section": "A more extreme example",
    "text": "A more extreme example\n\npeace_easy <- 0\nviolent_easy <- 3\npeace_hard <- 3\nviolent_hard <- 0\n\nd <- \n  data.frame(\n    Game = rep(c(0, 1), each = 50, times = 2),\n    Difficulty = rep(c(0, 1), each = 50*2),\n    Aggression = c(\n      rnorm(50, peace_easy, 1),\n      rnorm(50, violent_easy, 1),\n      rnorm(50, peace_hard, 1),\n      rnorm(50, violent_hard, 1)\n    )\n  )"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#full-cross-over",
    "href": "content/08-interactions/slides/index.html#full-cross-over",
    "title": "Interactions",
    "section": "Full cross-over",
    "text": "Full cross-over"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#whats-our-correction",
    "href": "content/08-interactions/slides/index.html#whats-our-correction",
    "title": "Interactions",
    "section": "What’s our “correction”?",
    "text": "What’s our “correction”?\n\nsummary(lm(Aggression ~ Game*Difficulty, d))\n\n\nCall:\nlm(formula = Aggression ~ Game * Difficulty, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.67125 -0.66907 -0.00832  0.65231  2.48827 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)    \n(Intercept)      0.00794    0.13457   0.059    0.953    \nGame             2.96338    0.19031  15.571   <2e-16 ***\nDifficulty       2.93052    0.19031  15.398   <2e-16 ***\nGame:Difficulty -5.77443    0.26915 -21.455   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9516 on 196 degrees of freedom\nMultiple R-squared:  0.7016,    Adjusted R-squared:  0.697 \nF-statistic: 153.6 on 3 and 196 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#factor-vs.-contrasts",
    "href": "content/08-interactions/slides/index.html#factor-vs.-contrasts",
    "title": "Interactions",
    "section": "Factor vs. contrasts",
    "text": "Factor vs. contrasts\n\nIn the example above, we’re going straight for the contrasts in lm\nIn ANOVAs (aov) we usually calculate the Sums of Squares for an entire factor (grand mean vs. group mean)\nSame as comparing an lm model without the interaction to an lm model with the interaction"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#comparing-models",
    "href": "content/08-interactions/slides/index.html#comparing-models",
    "title": "Interactions",
    "section": "Comparing models",
    "text": "Comparing models\n\nm1 <- lm(Aggression ~ Game + Difficulty, d)\nm2 <- lm(Aggression ~ Game + Difficulty + Game:Difficulty, d)\nanova(m1, m2)\n\nAnalysis of Variance Table\n\nModel 1: Aggression ~ Game + Difficulty\nModel 2: Aggression ~ Game + Difficulty + Game:Difficulty\n  Res.Df    RSS Df Sum of Sq      F    Pr(>F)    \n1    197 594.28                                  \n2    196 177.48  1     416.8 460.31 < 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#main-effects-vs.-contrasts",
    "href": "content/08-interactions/slides/index.html#main-effects-vs.-contrasts",
    "title": "Interactions",
    "section": "Main effects vs. contrasts",
    "text": "Main effects vs. contrasts\n\nsummary(aov(Aggression ~ Game*Difficulty, d))\n\n                 Df Sum Sq Mean Sq F value Pr(>F)    \nGame              1    0.3     0.3   0.320  0.572    \nDifficulty        1    0.1     0.1   0.104  0.748    \nGame:Difficulty   1  416.8   416.8 460.305 <2e-16 ***\nResiduals       196  177.5     0.9                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAggregating over both conditions means no main effects of either factor. Both conditions have a mean of 1.5:\n\n\n\n\nPeaceful\nViolent\n\n\n\n\nEasy\n0\n3\n\n\nHard\n3\n0"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#it-gets-a-bit-complicated",
    "href": "content/08-interactions/slides/index.html#it-gets-a-bit-complicated",
    "title": "Interactions",
    "section": "It gets a bit complicated",
    "text": "It gets a bit complicated\n\nIf we don’t go for contrasts, then there’s different types of Sums of Squares for the terms\nAre we interested in only the interaction?\nDo we want to interpret main effects in the presence of an interaction?"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#what-about-here",
    "href": "content/08-interactions/slides/index.html#what-about-here",
    "title": "Interactions",
    "section": "What about here?",
    "text": "What about here?\n\nYes, there’s main effects: On average, peaceful is lower than violent and on average, easy is lower than difficult. But we know that’s not really the case."
  },
  {
    "objectID": "content/08-interactions/slides/index.html#type-i",
    "href": "content/08-interactions/slides/index.html#type-i",
    "title": "Interactions",
    "section": "Type I",
    "text": "Type I\nSequential sum of squares, where the order of factors matters in unbalanced designs:\n\n\\(SS(A)\\)\n\\(SS(B | A)\\)\n\\(SS(AB | B, A)\\)"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#type-ii",
    "href": "content/08-interactions/slides/index.html#type-ii",
    "title": "Interactions",
    "section": "Type II",
    "text": "Type II\nPresence of Interaction \\(SS(AB | A, B)\\)? Then no need to test for main effects. If there’s no interaction, then we test main effects.\n\n\\(SS(A |B)\\) for factor A\n\\(SS(B | A)\\) for factor B"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#type-iii",
    "href": "content/08-interactions/slides/index.html#type-iii",
    "title": "Interactions",
    "section": "Type III",
    "text": "Type III\nWe want everything: Main effects even in the presence of an interaction effect.\n\n\\(SS(A | B, AB)\\)\n\\(SS(B | A, AB)\\)"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#what-do-we-want",
    "href": "content/08-interactions/slides/index.html#what-do-we-want",
    "title": "Interactions",
    "section": "What do we want?",
    "text": "What do we want?\n\nWith balanced data (same number of observations per cell) none of this matters\nDo we believe main effects are meaningful when there’s an interaction present? If not, Type II are more powerful.\nBut our typical hypotheses are about main effects and interaction effects: Type III"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#type-iii-1",
    "href": "content/08-interactions/slides/index.html#type-iii-1",
    "title": "Interactions",
    "section": "Type III",
    "text": "Type III\n\nIf we choose to go for Type III, we can’t use dummy coding\nDummy coding isn’t adequate when there’s interactions present\nGo sum-to-zero instead\n\n\nGame <- as.factor(d$Game)\n\ncontrasts(Game)\n\n  1\n0 0\n1 1\n\ncontrasts(Game) <- contr.sum\n\ncontrasts(Game)\n\n  [,1]\n0    1\n1   -1"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#bottom-line",
    "href": "content/08-interactions/slides/index.html#bottom-line",
    "title": "Interactions",
    "section": "Bottom line",
    "text": "Bottom line\n\nIf you have balanced cells, none of this matters\nIf you have planned contrasts, none of this matters\nIf you have unbalanced cells and care about main effects, you must choose Type III with sum-to-zero contrasts\nEither compare models (anova command on two lm models) or go straight to an out-of-the-box ANOVA solutions (e.g., afex package)"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#creating-unbalanced-data",
    "href": "content/08-interactions/slides/index.html#creating-unbalanced-data",
    "title": "Interactions",
    "section": "Creating unbalanced data",
    "text": "Creating unbalanced data\nLet’s delete some rows.\n\nrows_to_delete <- sample(nrow(d), 15)\nd <- d[-rows_to_delete,]\n\ntable(d$Game, d$Difficulty)\n\n   \n     0  1\n  0 46 46\n  1 47 46"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#different-results",
    "href": "content/08-interactions/slides/index.html#different-results",
    "title": "Interactions",
    "section": "Different results",
    "text": "Different results\n\nsummary(aov(Aggression ~ Game*Difficulty, d))\n\n                 Df Sum Sq Mean Sq F value Pr(>F)    \nGame              1    0.9     0.9   1.078  0.301    \nDifficulty        1    0.3     0.3   0.306  0.581    \nGame:Difficulty   1  412.0   412.0 495.391 <2e-16 ***\nResiduals       181  150.5     0.8                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nsummary(afex::aov_car(Aggression ~ Game*Difficulty + Error(id), d %>% mutate(id=1:nrow(d)), type = 3))\n\nAnova Table (Type 3 tests)\n\nResponse: Aggression\n                num Df den Df     MSE        F     ges Pr(>F)    \nGame                 1    181 0.83162   0.8505 0.00468 0.3576    \nDifficulty           1    181 0.83162   0.4517 0.00249 0.5024    \nGame:Difficulty      1    181 0.83162 495.3912 0.73240 <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#the-lm-solution",
    "href": "content/08-interactions/slides/index.html#the-lm-solution",
    "title": "Interactions",
    "section": "The lm solution",
    "text": "The lm solution\n\nSpecify sum-to-zero contrasts\nRun a model with and one without the interaction\nCompare models and get a p-value for just the interaction\nDoesn’t give you the main effects, but fast"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#the-afex-solution",
    "href": "content/08-interactions/slides/index.html#the-afex-solution",
    "title": "Interactions",
    "section": "The afex solution",
    "text": "The afex solution\n\nNeeds an ID variable to identify a row\nYou can specify the type of tests\nAutomatically uses sum-to-zero coding\nAutomatically transforms character variables into factors\n\n\nafex::aov_car(DV ~ IV1*IV2 + Error(ID), data, type = 3)"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#power-for-interactions",
    "href": "content/08-interactions/slides/index.html#power-for-interactions",
    "title": "Interactions",
    "section": "Power for interactions",
    "text": "Power for interactions\n\nWhat do I need to power for: The literal interaction effect or will we follow up?\nIf we’re not interested in the pattern, then we can just focus on powering for the interaction term\nIf we’re interested in the pattern, we must power for simple effects (aka follow-ups)"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#same-effect-size",
    "href": "content/08-interactions/slides/index.html#same-effect-size",
    "title": "Interactions",
    "section": "Same effect size",
    "text": "Same effect size"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#full-reversal",
    "href": "content/08-interactions/slides/index.html#full-reversal",
    "title": "Interactions",
    "section": "Full reversal",
    "text": "Full reversal\n\nFor full details, see here."
  },
  {
    "objectID": "content/08-interactions/slides/index.html#attentuation",
    "href": "content/08-interactions/slides/index.html#attentuation",
    "title": "Interactions",
    "section": "Attentuation",
    "text": "Attentuation"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#back-to-our-example",
    "href": "content/08-interactions/slides/index.html#back-to-our-example",
    "title": "Interactions",
    "section": "Back to our example",
    "text": "Back to our example\nOverall results tells us whether the interaction provides extra information:\n\n# get id variables\nd$id <- 1:nrow(d)\n\n# give actual names to factor leves\nd$Game <- as.factor(d$Game)\nd$Difficulty <- as.factor(d$Difficulty)\nlevels(d$Game) <- c(\"Peaceful\", \"Violent\")\nlevels(d$Difficulty) <- c(\"easy\", \"hard\")\n\nm <- afex::aov_car(Aggression ~ Game*Difficulty + Error(id), d %>% mutate(id=1:nrow(d)), type = 3)"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#back-to-our-example-1",
    "href": "content/08-interactions/slides/index.html#back-to-our-example-1",
    "title": "Interactions",
    "section": "Back to our example",
    "text": "Back to our example\n\nsummary(m)\n\nAnova Table (Type 3 tests)\n\nResponse: Aggression\n                num Df den Df     MSE        F     ges Pr(>F)    \nGame                 1    181 0.83162   0.8505 0.00468 0.3576    \nDifficulty           1    181 0.83162   0.4517 0.00249 0.5024    \nGame:Difficulty      1    181 0.83162 495.3912 0.73240 <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#following-up",
    "href": "content/08-interactions/slides/index.html#following-up",
    "title": "Interactions",
    "section": "Following up",
    "text": "Following up\nPairwise comparisons tell us the pattern. Shotgun approach:\n\npairs(emmeans::emmeans(m, c(\"Game\", \"Difficulty\")))\n\n contrast                      estimate    SE  df t.ratio p.value\n Peaceful easy - Violent easy   -3.1084 0.189 181 -16.434  <.0001\n Peaceful easy - Peaceful hard  -3.0748 0.190 181 -16.170  <.0001\n Peaceful easy - Violent hard   -0.2138 0.190 181  -1.124  0.6750\n Violent easy - Peaceful hard    0.0335 0.189 181   0.177  0.9980\n Violent easy - Violent hard     2.8946 0.189 181  15.304  <.0001\n Peaceful hard - Violent hard    2.8610 0.190 181  15.046  <.0001\n\nP value adjustment: tukey method for comparing a family of 4 estimates"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#following-up-1",
    "href": "content/08-interactions/slides/index.html#following-up-1",
    "title": "Interactions",
    "section": "Following up",
    "text": "Following up\nPairwise comparisons tell us the pattern. By factor:\n\npairs(emmeans::emmeans(m, \"Game\", by = \"Difficulty\"))\n\nDifficulty = easy:\n contrast           estimate    SE  df t.ratio p.value\n Peaceful - Violent    -3.11 0.189 181 -16.434  <.0001\n\nDifficulty = hard:\n contrast           estimate    SE  df t.ratio p.value\n Peaceful - Violent     2.86 0.190 181  15.046  <.0001"
  },
  {
    "objectID": "content/08-interactions/slides/index.html#power-for-the-interaction-term",
    "href": "content/08-interactions/slides/index.html#power-for-the-interaction-term",
    "title": "Interactions",
    "section": "Power for the interaction term",
    "text": "Power for the interaction term\n\nn <- 50\nm1 <- 4\nm2 <- 4\nm3 <- 4\nm4 <- 4.5\nsd <- 1.5\ndraws <- 1e3\n\npvalues <- NULL\n\nfor (i in 1:n) {\n\n  group1 <- rnorm(n, m1, sd)\n  group2 <- rnorm(n, m2, sd)\n  group3 <- rnorm(n, m3, sd)\n  group4 <- rnorm(n, m4, sd)\n\n  d <- data.frame(\n    id = factor(1:c(4*n)),\n    scores = c(group1, group2, group3, group4),\n    group1 = factor(rep(c(\"a\", \"b\"), each = n, times = 2)),\n    group2 = factor(rep(c(\"a\", \"b\"), each = n*2))\n  )\n  \n  contrasts(d$group1) <- contr.sum\n  contrasts(d$group2) <- contr.sum\n  \n  m <- suppressMessages(afex::aov_car(scores ~ group1*group2 + Error(id), data = d, type = 3))\n  \n  pvalues[i] <- m$anova_table$`Pr(>F)`[3]\n}\n\nsum(pvalues < 0.05) / length(pvalues)\n\n[1] 0.2"
  },
  {
    "objectID": "content/09-continuous-predictors/09-exercise.html#exercise",
    "href": "content/09-continuous-predictors/09-exercise.html#exercise",
    "title": "Exercise VI",
    "section": "0.1 Exercise",
    "text": "0.1 Exercise\nYou’re interested in the effects of three predictors on an outcome. When all predictors are 0, the outcome (y) should be around 16. The first predictor, x1, causes a 1-point increase in y; the second predictor, x2 causes a 0.3-point increase in y; the third predictor, x3, causes a 1.4 increase in y. All predictors range from 0 to 7; for our purposes, we can assume that they’re uniformly distributed. The error term has a mean of 0 and an SD of 10.\nSimulate power with 500 runs. You could either power for the entire model (so for the p-value of the F-test for the full lm model) or for each individual predictor. Find out which has the most power: Store power for the full model and each individual predictor from the model. Start with 50 participants and go up in steps of 10 until you reach 200. Plot the power curves for the different power types. You can use the code below:\n\nlibrary(ggplot2)\n\nggplot(outcomes, aes(x = sample_size, y = power, color = type)) + geom_line() + theme_bw()\n\n\nset.seed(42)\n\nb0 <- 16\nb1 <- 1\nb2 <- 0.3\nb3 <- 1.4\nsizes <- seq(50, 200, 10)\nruns <- 500\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    type = NULL,\n    power = NULL\n  )\n\nfor (n in sizes) {\n  \n  pvalues_all <- NULL\n  pvalues_x1 <- NULL\n  pvalues_x2 <- NULL\n  pvalues_x3 <- NULL\n  \n  for (i in 1:runs) {\n    \n    error <- rnorm(n, 0, 10)\n    \n    x1 <- runif(n, 0, 7)\n    x2 <- runif(n, 0, 7)\n    x3 <- runif(n, 0, 7)\n    \n    d <- \n      data.frame(\n        x1 = x1,\n        x2 = x2,\n        x3 = x3,\n        y = b0 + b1*x1 + b2*x2 + b3*x3 + error\n      )\n    \n    m <- summary(lm(y ~ x1 + x2 + x3, d))\n    \n    pvalues_all[i] <- broom::glance(m)$p.value\n    pvalues_x1[i] <- broom::tidy(m)$p.value[2]\n    pvalues_x2[i] <- broom::tidy(m)$p.value[3]\n    pvalues_x3[i] <- broom::tidy(m)$p.value[4]\n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = rep(n, 4),\n        type = factor(c(\"all\", \"x1\", \"x2\", \"x3\")),\n        power = c(\n          sum(pvalues_all < 0.05) / length(pvalues_all),\n          sum(pvalues_x1 < 0.05) / length(pvalues_x1),\n          sum(pvalues_x2 < 0.05) / length(pvalues_x2),\n          sum(pvalues_x3 < 0.05) / length(pvalues_x3)\n        )\n      )\n    )\n}\n\nlibrary(ggplot2)\nggplot(outcomes, aes(x = sample_size, y = power, color = type)) + geom_line() + theme_bw()"
  },
  {
    "objectID": "content/09-continuous-predictors/09-exercise.html#exercise-1",
    "href": "content/09-continuous-predictors/09-exercise.html#exercise-1",
    "title": "Exercise VI",
    "section": "0.2 Exercise",
    "text": "0.2 Exercise\nCreate a correlation between two variables of 0.2. Use a correlation matrix. How many participants do you need for 95% power with an alpha of 0.01? Go about this as you think is best. Verify your estimate with GPower.\n\nlibrary(MASS)\n\nsigma <- \n  matrix(\n    c(1, 0.2, 0.2, 1),\n    ncol = 2\n  )\nalpha <- 0.01\npower <- 0\nn <- 100\nruns <- 1e3\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nwhile (power < 0.95) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    \n    d <- \n      mvrnorm(\n        n,\n        c(0,0),\n        sigma\n      )\n    \n    d <- as.data.frame(d)\n    \n    colnames(d) <- c(\"x1\", \"x2\")\n    \n    pvalues[i] <- cor.test(d$x1, d$x2)$p.value\n  }\n  \n  power <- sum(pvalues < alpha) / length(pvalues)\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        power = power\n      )\n    )\n  \n  n <- n + 10\n}\n\nplot(outcomes$sample_size, outcomes$power)"
  },
  {
    "objectID": "content/09-continuous-predictors/09-exercise.html#exercise-2",
    "href": "content/09-continuous-predictors/09-exercise.html#exercise-2",
    "title": "Exercise VI",
    "section": "0.3 Exercise",
    "text": "0.3 Exercise\nIn the correlation matrix, you specifically say how variables are related, but also when they aren’t related (i.e., when you assign zero for a correlation). For the data generating process, this is important, because it specifies the causal structure. Going simply by \\(R^2\\) will often be misleading. For exmaple, what if you have a third variable that creates a spurious effect? You can simulate that as well. Say you measure how much ice cream a person consumes on average and expect that eating a lot of sweet ice cream makes people crave savoury foods, which you measure as the portions of fries consumed in a beer garden. However, there truly is no effect of ice cream on eating fries; it’s just that both are influenced by the number of sun hours: More sun hours will lead people to eat more ice cream but also to spend more time in beer gardens and, inevitably, eat fries there.\nSimulate that and inspect the \\(R^2\\) of the models. Use a sample size of 10,000. First, create a sun hours variable. Then, predict ice cream eating with sun hours. Next, predict fries eating with sun hours. Now run two regression models:\n\none where you predict fries with ice cream alone\none where you predict fries with icre cream and sun hours\n\nWhich model is the correct causal structure? But which one has the higher \\(R^2\\)? If this tickled your interest, have a look here and here.\n\nn <- 1e4\n\nsun <- rnorm(n)\nice_cream <- 0.4*sun + rnorm(n)\nfries <- 0.5*sun + rnorm(n)\n\nsummary(lm(fries ~ ice_cream))\n\n\nCall:\nlm(formula = fries ~ ice_cream)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8340 -0.7412 -0.0039  0.7345  4.3659 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.009393   0.011036   0.851    0.395    \nice_cream   0.175206   0.010235  17.118   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.104 on 9998 degrees of freedom\nMultiple R-squared:  0.02847,   Adjusted R-squared:  0.02838 \nF-statistic:   293 on 1 and 9998 DF,  p-value: < 2.2e-16\n\nsummary(lm(fries ~ ice_cream + sun))\n\n\nCall:\nlm(formula = fries ~ ice_cream + sun)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6994 -0.6739 -0.0083  0.6723  3.6741 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.004700   0.010029   0.469    0.639    \nice_cream   -0.003296   0.010080  -0.327    0.744    \nsun          0.497472   0.010829  45.937   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.003 on 9997 degrees of freedom\nMultiple R-squared:  0.1978,    Adjusted R-squared:  0.1976 \nF-statistic:  1233 on 2 and 9997 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "content/09-continuous-predictors/09-exercise.html#exercise-3",
    "href": "content/09-continuous-predictors/09-exercise.html#exercise-3",
    "title": "Exercise VI",
    "section": "0.4 Exercise",
    "text": "0.4 Exercise\nYour colleague comes to you and tells you that they predicted satisfaction with a film with people’s enjoyment. However, they only had 20 people in their sample. Run a sensitivity analysis and check for what \\(r\\) 20 people give you 90% power, even if you allow a more liberal alpha of 0.10. Increase \\(r\\) in steps of 0.01 and go the full distance, checking all effect sizes from 0 to 1. Do 500 runs per step. Verify with GPower.\n\nn <- 20\nr <- 0\nalpha <- 0.10\neffects <- seq(0, 1, 0.01)\nruns <- 1e3\n\noutcomes <- \n  data.frame(\n    effect_size = NULL,\n    power = NULL\n  )\n\npvalues <- NULL\n\nfor (anr in effects) {\n  \n  for (i in 1:runs) {\n    sigma <- matrix(\n        c(1, anr, anr, 1),\n        ncol = 2\n      )\n      \n      d <- mvrnorm(\n        n,\n        c(0,0),\n        sigma\n      )\n      \n      d <- as.data.frame(d)\n      colnames(d) <- c(\"enjoyment\", \"satisfaction\")\n      \n      pvalues[i] <- cor.test(d$enjoyment, d$satisfaction)$p.value\n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        effect_size = anr,\n        power = sum(pvalues < alpha) / length(pvalues)\n      )\n    )\n}\n\nplot(outcomes$effect_size, outcomes$power)"
  },
  {
    "objectID": "content/09-continuous-predictors/09-exercise.html#exercise-4",
    "href": "content/09-continuous-predictors/09-exercise.html#exercise-4",
    "title": "Exercise VI",
    "section": "0.5 Exercise",
    "text": "0.5 Exercise\nYou design a follow-up study where you measure enjoyment and satisfaction with films, but this time you want to determine a SESOI and do a power analysis before-hand. You measure both variables on a 5-point index Likert-scale (by index I mean it’s the average of several items). You know enjoyment usually scores above the midpoint, say a mean of 3.9 sounds realistic. The SD will be narrow: 0.5. For satisfaction, you expect a score below the midpoint of the scale, at 2.1, but with a larger SD of 1. Your smallest effect size of interest is 0.7 points because a previous analysis shows that this is the point where satisfaction translates to higher well-being for the day.\nYou want to be strict, so you set your alpha to 0.005, but at the same time, you don’t mind missing a true effect just as much, which is why you set your power goal to 85%. Run the power analysis (1000 runs per combo). Start at 50 people and go up in steps of 1 until you reach your desired power level. (Tip: Use the raw effect and SDs to get \\(r\\) to use in a variance-covariance matrix). Verify with GPower.\n\ngoal <- 0.85\nalpha <- 0.005\nsesoi <- 0.7\nn <- 50\nmeans <- c(enjoyment = 3.9, satisfaction = 2.1)\nsd_enjoy <- 0.5\nsd_sat <- 1\nr <- sesoi * sd_enjoy/sd_sat\ncovariance <- r * sd_enjoy * sd_sat\nruns <- 1e3\n\nsigma <- \n  matrix(\n    c(\n      sd_enjoy**2, covariance,\n      covariance, sd_sat**2\n    ),\n    ncol = 2\n  )\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\npower <- 0\n\nwhile (power < goal) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    \n    d <- as.data.frame(\n      mvrnorm(\n        n,\n        means,\n        sigma\n      )\n    )\n    \n    pvalues[i] <- cor.test(d$enjoyment, d$satisfaction)$p.value\n  }\n  \n  power <- sum(pvalues < alpha) / length(pvalues)\n  \n  outcomes <- rbind(\n    outcomes,\n    data.frame(\n      sample_size = n,\n      power = power\n    )\n  )\n  \n  n <- n + 1\n}\n\nplot(outcomes$sample_size, outcomes$power)"
  },
  {
    "objectID": "content/09-continuous-predictors/09-exercise.html#exercise-5",
    "href": "content/09-continuous-predictors/09-exercise.html#exercise-5",
    "title": "Exercise VI",
    "section": "0.6 Exercise",
    "text": "0.6 Exercise\nA colleague has found that films with higher ratings on IMDB bring in more money at the box office. However, you think this effect is mostly due to genre: for comedies, there is an effect of quality on success, but for action films this doesn’t matter. In other words, you predict an interaction, such that the positive effect of quality (i.e., IMDB rating) is only present in one condition (genre: comedies), but not in the other condition.\nYou want to test that hypothesis and start your power simulation. Specifically, you want to power for the interaction effect. IMDB ratings range from 0-10. Success is measured in million dollar steps. For genre, action films will be your baseline. You’ll also center the rating so that 0 represents an average rating. Overall, when a film is an action flick and has an average rating, you expect it to bring in 20 million. You don’t expect a main effect of quality because the quality effect will depend on genre. You do expect, however, a main effect of genre, such that, at average quality, action flicks bring in 5 million more than comedies. Crucially, you expect an interaction effect: For comedies, each 1-point increase in quality will generate 2 million extra at the box office.\nAs for error: You expect a normally distributed error with a mean of 0 and an SD of 15. Simulate the data and see how many films you need to code to have enough power to detect the interaction effect with 95% power with an alpha of 0.05. Start at 10 and go up in steps of 10 to a maximum of 250. Do 1,000 runs per combo. (Tip: Generate a large data set with the linear model formula first and plot the means before getting into the simulation.)\n\nb0 <- 50\nb1 <- 0\nb2 <- 5\nb3 <- 2\nsizes <- seq(10, 300, 10)\nruns <- 1e3\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nfor (n in sizes) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    \n    error <- rnorm(n, 0, 15)\n    genre <-  rep(0:1, n/2)\n    quality <-  scale(runif(n, 0, 10), center = TRUE, scale = FALSE)\n    \n    d <- \n      data.frame(\n        genre = genre,\n        quality = quality,\n        success = b0 + b1*quality + b2*genre + b3*quality*genre + error\n      )\n    \n    # success can't be less than 0\n    d$success <- ifelse(d$success < 0, 0, d$success)\n    \n    m <- summary(lm(success ~ quality*genre, d))\n    \n    pvalues[i] <- broom::tidy(m)$p.value[4]\n    \n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        power = sum(pvalues < 0.05) / length(pvalues)\n      )\n    )\n}\n\nggplot(d, aes(x = quality, y = success, color = as.factor(genre))) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\") +\n  theme_bw()\n\n\n\nwith(outcomes, plot(sample_size, power))"
  },
  {
    "objectID": "content/09-continuous-predictors/09-exercise.html#exercise-6",
    "href": "content/09-continuous-predictors/09-exercise.html#exercise-6",
    "title": "Exercise VI",
    "section": "0.7 Exercise",
    "text": "0.7 Exercise\nSimulate the above again, but this time increase the main effects of both quality and genre to 10 million. Leave the interaction effect at 2. What do you think – will this have an effect on power? If so, how and why?\n\nb0 <- 50\nb1 <- 10\nb2 <- 10\nb3 <- 2\nsizes <- seq(10, 250, 10)\nruns <- 1e3\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nfor (n in sizes) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    \n    error <- rnorm(n, 0, 15)\n    genre <-  rep(0:1, n/2)\n    quality <-  scale(runif(n, 0, 10), center = TRUE, scale = FALSE)\n    \n    d <- \n      data.frame(\n        genre = genre,\n        quality = quality,\n        success = b0 + b1*quality + b2*genre + b3*quality*genre + error\n      )\n    \n    # success can't be less than 0\n    d$success <- ifelse(d$success < 0, 0, d$success)\n    \n    m <- summary(lm(success ~ quality*genre, d))\n    \n    pvalues[i] <- broom::tidy(m)$p.value[4]\n    \n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        power = sum(pvalues < 0.05) / length(pvalues)\n      )\n    )\n}\n\nggplot(d, aes(x = quality, y = success, color = as.factor(genre))) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\") +\n  theme_bw()\n\n\n\nwith(outcomes, plot(sample_size, power))"
  },
  {
    "objectID": "content/09-continuous-predictors/09-exercise.html#exercise-7",
    "href": "content/09-continuous-predictors/09-exercise.html#exercise-7",
    "title": "Exercise VI",
    "section": "0.8 Exercise",
    "text": "0.8 Exercise\nYou want to know how much listening to music leads to feelings of being relaxed. You expect a linear effect of the number of songs someone listens to on them feeling relaxed. However, you suspect that whether this effect occurs will strongly depend on how much people like these songs. You measure feelings of relaxation on a Likert-type index variable with a range of 1-7. The number of songs is measured as a count with a maximum of 30 songs–you counted songs over a 1h period. After that period, you also asked how much people enjoyed the songs they were listening to on a scale from 0-100.\nSimulate (uniformly) number of songs (use sample) and liking. Transform the number of songs so that 1 up on the variable means listening to 5 more songs. As for liking, you want to go in steps of 10. Center both. These transformations make it easier to get an intuition about the effect sizes. At average listening and liking, you think relaxation should be somewhat below the midpoint of the scale: 3.1. You don’t expect main effects of either predictor: Liking music shouldn’t relax you unless you listen to some of it, and listening to music alone shouldn’t do much unless you like the music. However, you expect a fairly sizeable interaction effect, such that the combination of liking and listening will increase relaxation by 0.2 points. Remember the transformations: We say that listening to 5 songs (going up 1 on listening) will increase relaxation 0.2 when liking also goes up by 10 points (going up 1 on liking).\nFor error, you expect a normally distributed error with mean 0 and an SD of 2. Simulate power for the interaction effect, starting at 10 and ending at a sample size of 100. Go in steps of 5. Do 1,000 runs per combo. Also use interactions::interaction_plot to plot a random model to check how the data look like.\n\nb0 <- 3.1\nb1 <- 0\nb2 <- 0\nb3 <- 0.2\nsizes <- seq(10, 100, 5)\nruns <- 1e3\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nfor (n in sizes) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    \n    error <- rnorm(n, 0, 2)\n    listening <- scale(sample(1:30, n, replace = TRUE) / 5, scale = FALSE)\n    liking <- scale(runif(n, 0, 100) / 10, scale = FALSE)\n    \n    d <- \n      data.frame(\n        listening = listening,\n        liking = liking,\n        relaxation = b0 + b1*listening + b2*liking + b3*listening*liking + error\n      )\n    \n    #trim the outcome\n    d$relaxation <- ifelse(d$relaxation < 0, 0, d$relaxation)\n    d$relaxation <- ifelse(d$relaxation > 7, 7, d$relaxation)\n    \n    m <- summary(lm(relaxation ~ listening*liking, d))\n    pvalues[i] <- broom::tidy(m)$p.value[4]\n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        power = sum(pvalues < 0.05) / length(pvalues)\n      )\n    )\n  \n}\n\nplot(outcomes$sample_size, outcomes$power)\n\n\n\ninteractions::interact_plot(lm(relaxation ~ listening*liking, d), pred = \"listening\", modx = \"liking\")"
  },
  {
    "objectID": "content/09-continuous-predictors/09-slides.html",
    "href": "content/09-continuous-predictors/09-slides.html",
    "title": "Slides",
    "section": "",
    "text": "Below you can find the slides for continuous predictors:\n\n\n\nIf you want to see the slides in full screen, you can click here. To download them as PDFs, hit the ‘e’ button when you got the presentation open and then Ctrl+P print to PDF. This will work best in Chrome (in Firefox, it didn’t print the headings for me)."
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#back-to-the-linear-model",
    "href": "content/09-continuous-predictors/slides/index.html#back-to-the-linear-model",
    "title": "Continuous predictors",
    "section": "Back to the linear model",
    "text": "Back to the linear model\nSo far our predictors have been categorical. If we have two groups predicting an outcome and our predictor is 0:\n\\[\\begin{align}\n& y = \\beta_0 + \\beta_1x\\\\\n& y = \\beta_0 + \\beta_1 \\times 0\\\\\n& y = \\beta_0\n\\end{align}\\]\nIf it’s 1:\n\\[\\begin{align}\n& y = \\beta_0 + \\beta_1x\\\\\n& y = \\beta_0 + \\beta_1 \\times 1\\\\\n& y = \\beta_0 + \\beta_1\n\\end{align}\\]"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#changing-x",
    "href": "content/09-continuous-predictors/slides/index.html#changing-x",
    "title": "Continuous predictors",
    "section": "Changing \\(x\\)",
    "text": "Changing \\(x\\)\nA continuous predictor is nothing new: We’re basically looking at what happens if \\(x\\) goes up by 1. So if our measure isn’t categorical (aka dummy coded), but continuous, we’re asking the same question. Only now, \\(x\\) can be more than 0 or 1.\n\\[\\begin{align}\ny &= \\beta_0 + \\beta_1x \\\\\ny &= \\beta_0 + \\beta_11 \\\\\ny &= \\beta_0 + \\beta_12 \\\\\ny &= \\beta_0 + \\beta_13 \\\\\ny &= \\dots\n\\end{align}\\]"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#full-range-of-x",
    "href": "content/09-continuous-predictors/slides/index.html#full-range-of-x",
    "title": "Continuous predictors",
    "section": "Full range of \\(x\\)",
    "text": "Full range of \\(x\\)\nIf we assume linearity, then getting \\(y\\) is easy. Assume we predict disagreeableness from age. With each year people grow older, they become 1 point more disagreeable on a 100-point scale. (And when they’re 0 years old, they’re also 0 disagreeable.)\n\\[\\begin{align}\n& disagreeableness = \\beta_0 + \\beta_1 \\times age\\\\\n& disagreeableness = 0 + 1 \\times age\n\\end{align}\\]\nNow getting the score for someone aged 50 is trivial:\n\\(disagreeableness = 0 + 1 \\times 50\\)"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#simulating-this-process-is-trivial",
    "href": "content/09-continuous-predictors/slides/index.html#simulating-this-process-is-trivial",
    "title": "Continuous predictors",
    "section": "Simulating this process is trivial",
    "text": "Simulating this process is trivial\nWe get our age, put it into the formula, and we have our outcome.\n\nage <- rnorm(100, 50, 20)\ndisagreeableness <- 0 + age\n\nsummary(lm(disagreeableness ~ age))\n\n\nCall:\nlm(formula = disagreeableness ~ age)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-1.213e-13  3.100e-16  1.174e-15  1.932e-15  1.425e-14 \n\nCoefficients:\n             Estimate Std. Error   t value Pr(>|t|)    \n(Intercept) 3.411e-14  3.096e-15 1.102e+01   <2e-16 ***\nage         1.000e+00  5.893e-17 1.697e+16   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.253e-14 on 98 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:      1 \nF-statistic: 2.88e+32 on 1 and 98 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#perfect-fit",
    "href": "content/09-continuous-predictors/slides/index.html#perfect-fit",
    "title": "Continuous predictors",
    "section": "Perfect fit",
    "text": "Perfect fit\n\nplot(age, disagreeableness)"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#adding-error",
    "href": "content/09-continuous-predictors/slides/index.html#adding-error",
    "title": "Continuous predictors",
    "section": "Adding error",
    "text": "Adding error\nIf there’s lots of error, it’ll be harder to separate signal (aka our true 1-point effect) from noise (the additional variation in our outcome).\n\nerror <- rnorm(100, 0, 100)\ndisagreeableness <- 0 + age + error\n\nsummary(lm(disagreeableness ~ age))\n\n\nCall:\nlm(formula = disagreeableness ~ age)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-182.39  -65.60  -12.45   61.45  300.97 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)  \n(Intercept)  -0.5999    25.4154  -0.024   0.9812  \nage           1.1264     0.4725   2.384   0.0191 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 98.13 on 98 degrees of freedom\nMultiple R-squared:  0.05481,   Adjusted R-squared:  0.04517 \nF-statistic: 5.683 on 1 and 98 DF,  p-value: 0.01906"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#psst-scale",
    "href": "content/09-continuous-predictors/slides/index.html#psst-scale",
    "title": "Continuous predictors",
    "section": "Psst, scale",
    "text": "Psst, scale\nAdding this much error will bring our outcome measure out of bounds. For a proper simulation where we’re interested in the data generating process, we need to deal with this (by truncating etc.)."
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#thats-it-really",
    "href": "content/09-continuous-predictors/slides/index.html#thats-it-really",
    "title": "Continuous predictors",
    "section": "That’s it, really",
    "text": "That’s it, really\nFor the linear model, it doesn’t matter on what level our predictor is. Categorical or continuous, we can simulate any outcome with this formula—including multiple predictors and interactions between different levels of predictors.\nLet’s assume number of relatives living close-by is also causing disagreeableness:\n\nage <- rnorm(100, 50, 20)\nrelatives <- rpois(100, 5)\nerror <- rnorm(100, 0, 10)\n\ndisagreeableness <- 0 + age + relatives + error"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#two-independent-effects",
    "href": "content/09-continuous-predictors/slides/index.html#two-independent-effects",
    "title": "Continuous predictors",
    "section": "Two independent effects",
    "text": "Two independent effects\n\nsummary(lm(disagreeableness ~ age + relatives))\n\n\nCall:\nlm(formula = disagreeableness ~ age + relatives)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-27.018  -6.560   0.326   6.535  23.319 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -2.4204     3.2527  -0.744   0.4586    \nage           1.0096     0.0470  21.480   <2e-16 ***\nrelatives     1.1264     0.4591   2.453   0.0159 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.882 on 97 degrees of freedom\nMultiple R-squared:  0.8301,    Adjusted R-squared:  0.8266 \nF-statistic:   237 on 2 and 97 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#varying-effect-sizes",
    "href": "content/09-continuous-predictors/slides/index.html#varying-effect-sizes",
    "title": "Continuous predictors",
    "section": "Varying effect sizes",
    "text": "Varying effect sizes\nWhat if we believe the effect of age is smaller, but that of number of relatives much larger. No problem, we just adjust our betas. Say each year only contributes 0.25 higher grumpiness, but each extra relative contributes 5 points on grumpiness.\n\\[\\begin{align}\n& disagreeableness = \\beta_0 + \\beta_1 \\times age + \\beta_2relatives\\\\\n& disagreeableness = 0 + 0.25 \\times age + 5 \\times relatives\n\\end{align}\\]"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#in-r",
    "href": "content/09-continuous-predictors/slides/index.html#in-r",
    "title": "Continuous predictors",
    "section": "In R",
    "text": "In R\n\ndisagreeableness <- 0 + 0.25*age + 5*relatives + error\nsummary(lm(disagreeableness ~ age + relatives))\n\n\nCall:\nlm(formula = disagreeableness ~ age + relatives)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-27.018  -6.560   0.326   6.535  23.319 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -2.4204     3.2527  -0.744    0.459    \nage           0.2596     0.0470   5.523 2.79e-07 ***\nrelatives     5.1264     0.4591  11.166  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.882 on 97 degrees of freedom\nMultiple R-squared:  0.6252,    Adjusted R-squared:  0.6175 \nF-statistic: 80.91 on 2 and 97 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#thats-the-data-generating-process",
    "href": "content/09-continuous-predictors/slides/index.html#thats-the-data-generating-process",
    "title": "Continuous predictors",
    "section": "That’s the data generating process",
    "text": "That’s the data generating process\nIn our simulation, we yet again make our assumptions explicit about how the data are generated: According to this linear model and our inputs (aka numbers).\nError adds uncertainty to our data generating process. It specifies that our linear model doesn’t 100% explain the causal structures and that there are influences on our outcome that we haven’t accounted for (or that an effect is heterogeneous)."
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#about-that-intercept",
    "href": "content/09-continuous-predictors/slides/index.html#about-that-intercept",
    "title": "Continuous predictors",
    "section": "About that intercept",
    "text": "About that intercept\n\\(disagreeableness = \\beta_0 + \\beta_1 \\times age + \\beta_2relatives\\)\nNow the intercept is disagreeableness when both age and number of relatives are 0. Maybe 0 age doesn’t make a lot of sense, so let’s center that variable.\nNow the meaning intercept changes: Disagreeableness at average age and 0 relatives living close-by. Easier to have an intuition for."
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#in-r-1",
    "href": "content/09-continuous-predictors/slides/index.html#in-r-1",
    "title": "Continuous predictors",
    "section": "In R",
    "text": "In R\nThe effect for age doesn’t change, but the interpretation of the intercept does: Now it’s the disagreeableness when there’s no relatives and age is at average.\n\ncentered_age <- scale(age, center = TRUE, scale = FALSE)\n\nsummary(lm(disagreeableness ~ centered_age + relatives))\n\n\nCall:\nlm(formula = disagreeableness ~ centered_age + relatives)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-27.018  -6.560   0.326   6.535  23.319 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   10.7744     2.3192   4.646 1.07e-05 ***\ncentered_age   0.2596     0.0470   5.523 2.79e-07 ***\nrelatives      5.1264     0.4591  11.166  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.882 on 97 degrees of freedom\nMultiple R-squared:  0.6252,    Adjusted R-squared:  0.6175 \nF-statistic: 80.91 on 2 and 97 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#simulate-power",
    "href": "content/09-continuous-predictors/slides/index.html#simulate-power",
    "title": "Continuous predictors",
    "section": "Simulate power",
    "text": "Simulate power\n\nn <- 100\neffect <- 0.10\nruns <- 1000\n\npvalues <- NULL\n\nfor (i in 1:runs) {\n  \n  age <- rnorm(n, 50, 20)\n  centered_age <- scale(age, center = TRUE, scale = FALSE)\n  error <- rnorm(n, 0, 10)\n  \n  disagreeableness <- 50 + effect*centered_age + error\n  \n  disagreeableness <- ifelse(disagreeableness > 100, 100, disagreeableness)\n  disagreeableness <- ifelse(disagreeableness < 0, 0, disagreeableness)\n  \n  m <- summary(lm(disagreeableness ~ centered_age))\n  \n  pvalues[i] <- broom::glance(m)$p.value\n}\n\nsum(pvalues < 0.05) / length(pvalues)\n\n[1] 0.485"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#standardized-effects",
    "href": "content/09-continuous-predictors/slides/index.html#standardized-effects",
    "title": "Continuous predictors",
    "section": "Standardized effects?",
    "text": "Standardized effects?\nWhat if we want to work with standardized effects? Remember that a standardized effect is just an expression of standard deviation units? So we can standardize our variables and voila: \\(r\\).\n\nage <- rnorm(100, 50, 20)\ndisagreeableness <- 0 + 0.25*age + rnorm(100, 0, 10)\n\n\nstan_age <- scale(age)\nstan_dis <- scale(disagreeableness)"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#lets-compare",
    "href": "content/09-continuous-predictors/slides/index.html#lets-compare",
    "title": "Continuous predictors",
    "section": "Let’s compare",
    "text": "Let’s compare\n\ncor(age, disagreeableness)\n\n[1] 0.4281227\n\nsummary(lm(stan_dis ~ stan_age))\n\n\nCall:\nlm(formula = stan_dis ~ stan_age)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3554 -0.5474 -0.1218  0.4755  2.5096 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 2.064e-17  9.083e-02    0.00        1    \nstan_age    4.281e-01  9.129e-02    4.69 8.86e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9083 on 98 degrees of freedom\nMultiple R-squared:  0.1833,    Adjusted R-squared:  0.175 \nF-statistic: 21.99 on 1 and 98 DF,  p-value: 8.86e-06"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#not-super-clean",
    "href": "content/09-continuous-predictors/slides/index.html#not-super-clean",
    "title": "Continuous predictors",
    "section": "Not super clean",
    "text": "Not super clean\nThis doesn’t give us a lot of control over the standardized effect size (our raw effect of 0.25 just happened to be 0.22 SDs). How about we just start off with standardized variables? If we want \\(r\\) = 0.20, we can do that as follows:\n\nn <- 1e4\n\nstan_age <- rnorm(n, 0, 1)\nstan_dis <- 0 + 0.2*stan_age + rnorm(n, 0, 0.3)"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#r-0.20",
    "href": "content/09-continuous-predictors/slides/index.html#r-0.20",
    "title": "Continuous predictors",
    "section": "\\(r\\) = 0.20?",
    "text": "\\(r\\) = 0.20?\n\nsummary(lm(stan_dis ~ stan_age))\n\n\nCall:\nlm(formula = stan_dis ~ stan_age)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.09330 -0.20679  0.00066  0.20381  1.02869 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept) -0.0005512  0.0030027  -0.184    0.854    \nstan_age     0.1957996  0.0029714  65.895   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3003 on 9998 degrees of freedom\nMultiple R-squared:  0.3028,    Adjusted R-squared:  0.3027 \nF-statistic:  4342 on 1 and 9998 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#wait-a-second",
    "href": "content/09-continuous-predictors/slides/index.html#wait-a-second",
    "title": "Continuous predictors",
    "section": "Wait a second",
    "text": "Wait a second\nWhy isn’t the effect as we specified?\n\ncor(stan_dis, stan_age)\n\n[1] 0.5502689"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#because-we-didnt-standardize",
    "href": "content/09-continuous-predictors/slides/index.html#because-we-didnt-standardize",
    "title": "Continuous predictors",
    "section": "Because we didn’t standardize",
    "text": "Because we didn’t standardize\nWe created the standardized version of disagreeableness with\n\nstan_dis <- 0 + 0.2*stan_age + rnorm(n, 0, 0.3)\n\nThat means the variable isn’t actually standardized:\n\nmean(stan_dis); sd(stan_dis)\n\n[1] 0.0005116522\n\n\n[1] 0.3595904"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#how-do-we-fix-this",
    "href": "content/09-continuous-predictors/slides/index.html#how-do-we-fix-this",
    "title": "Continuous predictors",
    "section": "How do we fix this?",
    "text": "How do we fix this?\nLuckily, we encountered a way to simulate variables, including their means, standard deviations, and correlations: The variance-covariance matrix. Now, we just make sure the means are 0 and standard deviations are 1.\n\nsd <- 1\ncorrelation <- 0.2\ncovariance <- correlation * sd * sd\n\nsigma <- \n  matrix(\n    c(\n      sd**2, covariance,\n      covariance, sd**2\n    )\n  )"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#correlation-matrix",
    "href": "content/09-continuous-predictors/slides/index.html#correlation-matrix",
    "title": "Continuous predictors",
    "section": "Correlation matrix",
    "text": "Correlation matrix\n\\[\n\\begin{bmatrix}\nvar  & cov \\\\\ncov & var \\\\\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix}\nsd^2  & r\\times sd \\times sd \\\\\nr\\times sd \\times sd & sd^2 \\\\\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix}\n1^2  & 0.2\\times 1 \\times 1 \\\\\n0.2\\times 1 \\times 1 & 1^2 \\\\\n\\end{bmatrix}\n\\]\n\\[\n\\begin{bmatrix}\n1 & 0.2 \\\\\n0.2 & 1 \\\\\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#lets-simulate-that",
    "href": "content/09-continuous-predictors/slides/index.html#lets-simulate-that",
    "title": "Continuous predictors",
    "section": "Let’s simulate that",
    "text": "Let’s simulate that\n\nlibrary(MASS)\n\nmeans <- c(age = 0, disagreeableness = 0)\n\nsigma <- \n  matrix(\n    c(1, correlation, \n      correlation, 1),\n    ncol = 2\n  )\n\nd <- mvrnorm(\n  n,\n  means,\n  sigma\n)\n\nd <- as.data.frame(d)\n\ncor(d$age, d$disagreeableness)\n\n[1] 0.1861377"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#what-if-we-want-both",
    "href": "content/09-continuous-predictors/slides/index.html#what-if-we-want-both",
    "title": "Continuous predictors",
    "section": "What if we want both?",
    "text": "What if we want both?\nSo now we know how to:\n\nSpecify an outcome on the raw scale, but we sort of eyeball the error\nSpecify both predictor and outcome on the standardized scale, full control over means, SDs, but we prefer unstandardized\n\nHow do we specify an effect on the raw scale, but use the multivariate normal distribution? Remember the formula for \\(r\\)?\n\\(r = B_{xy} \\frac{\\sigma_x}{\\sigma_y}\\)"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#just-plug-in-the-number-then",
    "href": "content/09-continuous-predictors/slides/index.html#just-plug-in-the-number-then",
    "title": "Continuous predictors",
    "section": "Just plug in the number then",
    "text": "Just plug in the number then\nSay we want a raw effect of age on disagreeableness of 0.5 points. We want age to have a mean of 50 and an SD of 20. We want disagreeableness to have a mean of 60 and an SD of 15. Let’s use the raw score and SDs first.\n\nsd_age <- 20\nsd_dis <- 15\neffect <- 0.5\n\nr <- effect * sd_age/sd_dis"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#then-we-get-the-covariances",
    "href": "content/09-continuous-predictors/slides/index.html#then-we-get-the-covariances",
    "title": "Continuous predictors",
    "section": "Then we get the covariances",
    "text": "Then we get the covariances\nNow that we have our correlation, we can get the covariates and fill everything into our variance covariance matrix.\n\ncovariance <- r * sd_age * sd_dis\n\nsigma <- \n  matrix(\n    c(\n      sd_age**2, covariance,\n      covariance, sd_dis**2\n    ),\n    ncol = 2\n  )"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#get-some-data",
    "href": "content/09-continuous-predictors/slides/index.html#get-some-data",
    "title": "Continuous predictors",
    "section": "Get some data",
    "text": "Get some data\nNow all we need are some means, and we’re good to go.\n\nmeans <- c(age = 50, disagreeableness = 60)\n\nd <- mvrnorm(\n  n,\n  means,\n  sigma\n)\n\nd <- as.data.frame(d)"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#lets-check-it-all-worked",
    "href": "content/09-continuous-predictors/slides/index.html#lets-check-it-all-worked",
    "title": "Continuous predictors",
    "section": "Let’s check it all worked",
    "text": "Let’s check it all worked\nCan we recover our raw effect of 0.5?\n\nsummary(lm(disagreeableness ~ age, d))\n\n\nCall:\nlm(formula = disagreeableness ~ age, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-39.376  -7.649  -0.091   7.637  39.706 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 34.870513   0.299804  116.31   <2e-16 ***\nage          0.502866   0.005571   90.27   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 11.2 on 9998 degrees of freedom\nMultiple R-squared:  0.449, Adjusted R-squared:  0.449 \nF-statistic:  8148 on 1 and 9998 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#standardized-effect-size",
    "href": "content/09-continuous-predictors/slides/index.html#standardized-effect-size",
    "title": "Continuous predictors",
    "section": "Standardized effect size",
    "text": "Standardized effect size\n\\(r = B_{xy} \\frac{\\sigma_x}{\\sigma_Y}\\)\n\nb <- coef(summary(lm(disagreeableness ~ age, d)))[2]\n\nb * sd(d$age)/sd(d$disagreeableness)\n\n[1] 0.6701006\n\ncor(d$age, d$disagreeableness)\n\n[1] 0.6701006"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#a-word-on-getting-that-r",
    "href": "content/09-continuous-predictors/slides/index.html#a-word-on-getting-that-r",
    "title": "Continuous predictors",
    "section": "A word on getting that \\(r\\)",
    "text": "A word on getting that \\(r\\)\n\\(r = B_{xy} \\frac{\\sigma_x}{\\sigma_Y}\\)\nWhat if we had specified an SD of 20 for age (\\(x\\)) and an SD of 5 for disagreeableness (\\(y\\))?\n\n0.5 * 20/5\n\n[1] 2\n\n\nBut \\(r\\) can’t be larger than 1–what’s going on??"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#data-generating-process-creates-limits",
    "href": "content/09-continuous-predictors/slides/index.html#data-generating-process-creates-limits",
    "title": "Continuous predictors",
    "section": "Data generating process creates limits",
    "text": "Data generating process creates limits\nWhen we determine the “raw” regression slope, we also determine the causal structure. In other words: If we say Y is caused by X, the SD of Y will be dependent on the SD of X and the effec size.\nPerfect correlation: \\(y\\) is ten times the size of \\(x\\).\n\nx <- rnorm(n, 50, 20)\ny <- 10*x\n\nsd(x); sd(y)\n\n[1] 19.56937\n\n\n[1] 195.6937"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#maximum-correlation",
    "href": "content/09-continuous-predictors/slides/index.html#maximum-correlation",
    "title": "Continuous predictors",
    "section": "Maximum correlation",
    "text": "Maximum correlation\n\nsummary(lm(y ~ x))\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-6.221e-11 -2.500e-14 -2.000e-15  2.000e-14  1.114e-10 \n\nCoefficients:\n              Estimate Std. Error    t value Pr(>|t|)    \n(Intercept) -5.093e-13  3.510e-14 -1.451e+01   <2e-16 ***\nx            1.000e+01  6.523e-16  1.533e+16   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.276e-12 on 9998 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:      1 \nF-statistic: 2.35e+32 on 1 and 9998 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#maximum-correlation-1",
    "href": "content/09-continuous-predictors/slides/index.html#maximum-correlation-1",
    "title": "Continuous predictors",
    "section": "Maximum correlation",
    "text": "Maximum correlation\nNow if we use the formula, we can get the maximum correlation of 1 because the outcome SD is exactly 10 times the predictor SD, which is exactly our effect:\n\nb <- coef(lm(y~x))[2]\nb\n\n x \n10 \n\n\n\nb * sd(x)/sd(y)\n\nx \n1 \n\ncor(x,y)\n\n[1] 1"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#lets-add-some-error",
    "href": "content/09-continuous-predictors/slides/index.html#lets-add-some-error",
    "title": "Continuous predictors",
    "section": "Let’s add some error",
    "text": "Let’s add some error\n\nx <- rnorm(n, 50, 20)\ny <- 10*x + rnorm(n, 0, 50)\n\nsd(x); sd(y)\n\n[1] 19.97822\n\n\n[1] 206.2865"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#now-our-correlation-is-lower",
    "href": "content/09-continuous-predictors/slides/index.html#now-our-correlation-is-lower",
    "title": "Continuous predictors",
    "section": "Now our correlation is lower",
    "text": "Now our correlation is lower\n\nb <- coef(lm(y~x))[2]\nb\n\n      x \n10.0238 \n\n\n\nb * sd(x)/sd(y)\n\n        x \n0.9707744 \n\ncor(x,y)\n\n[1] 0.9707744"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#bottom-line",
    "href": "content/09-continuous-predictors/slides/index.html#bottom-line",
    "title": "Continuous predictors",
    "section": "Bottom line",
    "text": "Bottom line\nIf you’re saying that one variable is caused by another, you’re not free to choose the variation of the outcome variable. The variation is a result of the data generating process and you’re determining what the data generating process is with your linear model. For our example, with a raw effect of 10, the smallest SD we can choose for the outcome is 10 times that of the cause.\nBack to our example: If age causes disagreeableness with an effect of 0.5, then the SD of disagreeableness can only be as small as half the SD of age."
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#bottom-line-1",
    "href": "content/09-continuous-predictors/slides/index.html#bottom-line-1",
    "title": "Continuous predictors",
    "section": "Bottom line",
    "text": "Bottom line\nIn other words, by determining the effect size and SD of the cause, you’re setting bounds on the range and SD of the outcome. You need to take that into account when simulating data: What’s a sensible raw effect size in relation to both the scale of the cause and the effect?"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#likert-setup",
    "href": "content/09-continuous-predictors/slides/index.html#likert-setup",
    "title": "Continuous predictors",
    "section": "Likert setup",
    "text": "Likert setup\nLet’s say both predictor and outcome are on a 7-point Likert-scale. You think that both should be roughly on the mid-point with a 1.2-point SD for predictor and 0.9 for outcome. As a raw effect, you assume 0.3 points as your SESOI. Do those SDs make sense? Would they produce a sensible \\(r\\)?\n\n0.3 * 1.2/0.9\n\n[1] 0.4"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#power",
    "href": "content/09-continuous-predictors/slides/index.html#power",
    "title": "Continuous predictors",
    "section": "Power",
    "text": "Power\n\nmeans <- c(x = 4, y = 4)\nsd_x <- 1.2\nsd_y <- 0.9\nsesoi <- 0.3\nr <- sesoi * sd_x/sd_y\ncovariance <- r * sd_x * sd_y\nn <- 50\nruns <- 500\n\nsigma <- \n  matrix(\n    c(sd_x**2, covariance,\n      covariance, sd_y),\n    ncol = 2\n  )\n\npvalues <- NULL\n\nfor (i in 1:runs) {\n  \n  d <- mvrnorm(\n    n,\n    means,\n    sigma\n  )\n  \n  d <- as.data.frame(d)\n  \n  pvalues[i] <- broom::glance(summary((lm(y ~ x, d))))$p.value\n}\n\nsum(pvalues < 0.05) / length(pvalues)\n\n[1] 0.778"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#more-than-one-cause",
    "href": "content/09-continuous-predictors/slides/index.html#more-than-one-cause",
    "title": "Continuous predictors",
    "section": "More than one cause",
    "text": "More than one cause\nIf we have more than one predictor, we need to specify effect sizes for each. Also, we need to be clear what our causal model is: We’re saying that both predictors are independently influencing our outcome. Otherwise we commit the Table II fallacy (Westreich and Greenland 2013). Simulating data also means simulating the causal structure."
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#what-power",
    "href": "content/09-continuous-predictors/slides/index.html#what-power",
    "title": "Continuous predictors",
    "section": "What power?",
    "text": "What power?\nIf we want to simulate power for multiple predictors, powering for \\(R^2\\) is possible, but strange: It means powering for the total effect of all predictors. Just like with group differences, powering for a total effect can mean many underlying patterns:\n\n\\(x_1\\) explaining everything, but \\(x_2\\) and \\(x_3\\) explaining nothing\n\\(x_1\\) and \\(x_2\\) both explaining a moderate amount, but \\(x_3\\) explaining nothing\nAll 3 explaining a little\nEtc."
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#what-we-need",
    "href": "content/09-continuous-predictors/slides/index.html#what-we-need",
    "title": "Continuous predictors",
    "section": "What we need",
    "text": "What we need\nIdeally, we power for all effectts meaning the smallest independent effect:\n\nSlope for each predictor\nCorrelation between each predictor\nCorrelation between each predictor and the outcome\nAll means and SDs\n\nHow to simulate power for multiple causes? Next exercise."
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#categorical-by-continuous-interactions",
    "href": "content/09-continuous-predictors/slides/index.html#categorical-by-continuous-interactions",
    "title": "Continuous predictors",
    "section": "Categorical by continuous interactions",
    "text": "Categorical by continuous interactions\nAn interaction between a categorical and a continuous variable states that the effect of one depends on the other. Back to the linear model:\n\\[\ny = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_1x_2\n\\]\nIn effect, we ask what information increasing both predictors adds to increasing them individually."
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#example",
    "href": "content/09-continuous-predictors/slides/index.html#example",
    "title": "Continuous predictors",
    "section": "Example",
    "text": "Example\nSay we want to know the effect of framing of a message (positive vs. neutral) on how much people agree with it, but we expect the effect to depend on how much people like positive framing."
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#pictures-plase",
    "href": "content/09-continuous-predictors/slides/index.html#pictures-plase",
    "title": "Continuous predictors",
    "section": "Pictures, plase",
    "text": "Pictures, plase\nTherefore, we expect the effect to look something like this, if we were to plot means per group and liking:"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#how-to-translate",
    "href": "content/09-continuous-predictors/slides/index.html#how-to-translate",
    "title": "Continuous predictors",
    "section": "How to translate",
    "text": "How to translate\n\\[\ny = \\beta_0 + \\beta_1condition + \\beta_2liking + \\beta_3 \\times condition \\times liking\n\\]\n\n\\(\\beta_0\\): The outcome when condition is neutral and liking is 0\n\\(\\beta_1\\): Difference between condition neutral and liking 0 and condition positive and liking 0 (aka main effect of condition)\n\\(\\beta_2\\): Difference between condition neutral and liking 0 and condition neutral and liking 1 (aka main effect liking)\n\\(\\beta_3\\): In addition to the condition effect, what does going up in liking by 1 add (or subtract) from the outcome?"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#lets-create-those-scores",
    "href": "content/09-continuous-predictors/slides/index.html#lets-create-those-scores",
    "title": "Continuous predictors",
    "section": "Let’s create those scores",
    "text": "Let’s create those scores\nLet’s say we measure agreement on a 100-point scale. We make several assumptions:\n\nThere’s probably no main effect of liking: Why would how much you like a positive message influence the effect of any message? It should only enhance the positive one.\nWe’ll center liking: It makes it easier to think about what our coefficients mean.\nLet’s say at average liking (0 = centered), agreement is on the mid-point of the scale: 50\nWe expect a positive message to “work” at average liking, so we put down a main effect of 5 points as our SESOI\nWe expect that going up 1 point on liking will enhance our framing effect by 1 point"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#put-into-numbers",
    "href": "content/09-continuous-predictors/slides/index.html#put-into-numbers",
    "title": "Continuous predictors",
    "section": "Put into numbers",
    "text": "Put into numbers\n\\[\\begin{align}\n& y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_1x_2\\\\\n& y = 50 + 5 \\times condition + 0 \\times liking + 1 \\times condition \\times liking\n\\end{align}\\]\n\n\\(\\beta_0\\): The outcome when condition is neutral and liking is 0 (i.e., average): 50\n\\(\\beta_1\\): Difference between condition neutral and liking 0 and condition positive and liking 0 (aka main effect of condition): 5\n\\(\\beta_2\\): Difference between condition neutral and liking 0 and condition neutral and liking 1 (aka main effect liking): 0\n\\(\\beta_3\\): In addition to the condition effect, what does going up in liking by 1 add (or subtract) from the outcome?: 1"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#in-r-2",
    "href": "content/09-continuous-predictors/slides/index.html#in-r-2",
    "title": "Continuous predictors",
    "section": "In R",
    "text": "In R\n\nset.seed(42)\n\nb0 <- 50\nb1 <- 5\nb2 <- 0\nb3 <- 1\nn <- 1e4\nerror <- rnorm(n*2, 0, 20)\n\ncondition <- rep(0:1, n)\nliking <- runif(n*2, min = 0, max = 7)\nliking <- scale(liking, center = TRUE, scale = FALSE)\n\nd <- \n  data.frame(\n    condition = condition,\n    liking = liking,\n    agree = b0 + b1 * condition + b2 * liking + b3 * condition * liking + error\n  )\n\nd$condition <- as.factor(ifelse(d$condition == 0, \"neutral\", \"positive\"))\nd$agree <- ifelse(d$agree > 100, 100, d$agree)\nd$agree <- ifelse(d$agree < 0, 0, d$agree)"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#lets-find-our-numbers",
    "href": "content/09-continuous-predictors/slides/index.html#lets-find-our-numbers",
    "title": "Continuous predictors",
    "section": "Let’s find our numbers",
    "text": "Let’s find our numbers\n\nsummary(lm(agree ~ condition*liking, d))\n\n\nCall:\nlm(formula = agree ~ condition * liking, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-57.891 -13.504   0.112  13.581  50.237 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n(Intercept)              49.772322   0.198910 250.226  < 2e-16 ***\nconditionpositive         5.177188   0.281301  18.404  < 2e-16 ***\nliking                    0.002632   0.097403   0.027    0.978    \nconditionpositive:liking  1.001518   0.138376   7.238 4.73e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.89 on 19996 degrees of freedom\nMultiple R-squared:  0.02175,   Adjusted R-squared:  0.0216 \nF-statistic: 148.2 on 3 and 19996 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#pictures-please",
    "href": "content/09-continuous-predictors/slides/index.html#pictures-please",
    "title": "Continuous predictors",
    "section": "Pictures, please",
    "text": "Pictures, please"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#two-way-interaction",
    "href": "content/09-continuous-predictors/slides/index.html#two-way-interaction",
    "title": "Continuous predictors",
    "section": "Two-way interaction",
    "text": "Two-way interaction\nThe interaction can also go way the other way around: The effect of A under B or the effect of B under A. In our case, we could also ask how the effect of liking a message is modified through framing. Same data, different plot:"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#lets-redo-the-logic",
    "href": "content/09-continuous-predictors/slides/index.html#lets-redo-the-logic",
    "title": "Continuous predictors",
    "section": "Let’s redo the logic",
    "text": "Let’s redo the logic\n\nset.seed(42)\n\nb0 <- 50\nb1 <- 0 # no main effect of condition\nb2 <- 2 # main effect of liking\nb3 <- 3 # interaction\nn <- 1e4\nerror <- rnorm(n*2, 0, 5)\n\ncondition <- rep(0:1, n)\nliking <- runif(n*2, min = 0, max = 7)\nliking <- scale(liking, center = TRUE, scale = FALSE)\n\nd <- \n  data.frame(\n    condition = condition,\n    liking = liking,\n    agree = b0 + b1 * condition + b2 * liking + b3 * condition * liking + error\n  )\n\nd$condition <- as.factor(ifelse(d$condition == 0, \"neutral\", \"positive\"))\nd$agree <- ifelse(d$agree > 100, 100, d$agree)\nd$agree <- ifelse(d$agree < 0, 0, d$agree)"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#now-were-looking-at-slopes",
    "href": "content/09-continuous-predictors/slides/index.html#now-were-looking-at-slopes",
    "title": "Continuous predictors",
    "section": "Now we’re looking at slopes",
    "text": "Now we’re looking at slopes"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#alternative-way-of-thinking-about-it",
    "href": "content/09-continuous-predictors/slides/index.html#alternative-way-of-thinking-about-it",
    "title": "Continuous predictors",
    "section": "Alternative way of thinking about it",
    "text": "Alternative way of thinking about it\nLet’s go back to our original example. At average liking, there’ll be a 5-point difference. What’s the maximum effect we can expect? If someone scores 7/7 on liking. Because we centered (and the mean was 3.5), that means someone who scores 3.5 above 0.\nSo the maximum effect is 5 (main effect) + 3.5 * 1 (interaction effect) = 8.5 points difference. The minimum effect is 5 - 3.5 = 1.5 points. The maximum range, therefore, is 7 points."
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#continuous-interactions",
    "href": "content/09-continuous-predictors/slides/index.html#continuous-interactions",
    "title": "Continuous predictors",
    "section": "Continuous interactions",
    "text": "Continuous interactions\nSame logic as before: What extra information do we gain if we go up on both variables compared to going up on them individually?\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_1x_2\\)"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#example-1",
    "href": "content/09-continuous-predictors/slides/index.html#example-1",
    "title": "Continuous predictors",
    "section": "Example",
    "text": "Example\nSay we want to know the effect of liking a message sender on agreeing with the sender’s message. However, we expect the effect to depend on how trustworthy the sender is."
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#pictures-plase-1",
    "href": "content/09-continuous-predictors/slides/index.html#pictures-plase-1",
    "title": "Continuous predictors",
    "section": "Pictures, plase",
    "text": "Pictures, plase\nWe expect the effect to look something like this, if we were to plot a line per trustworthiness rating:"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#how-to-translate-1",
    "href": "content/09-continuous-predictors/slides/index.html#how-to-translate-1",
    "title": "Continuous predictors",
    "section": "How to translate",
    "text": "How to translate\n\\[\ny = \\beta_0 + \\beta_1liking + \\beta_2trust + \\beta_3 \\times liking \\times trust\n\\] Both predictors are centered, so 0 is their mean:\n\n\\(\\beta_0\\): The outcome when both liking and trustworthiness are 0 (at their means)\n\\(\\beta_1\\): Outcome when liking goes 1 up, but trustworthiness remains at average (= 0) (aka main effect of liking)\n\\(\\beta_2\\): Outcome when trustworthiness goes 1 up, but liking remains at average (= 0) (aka the main effect of trustworthiness)\n\\(\\beta_3\\): In addition to the effect of liking going up 1, what does going up in trustworthiness by 1 add (or subtract) from the outcome?"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#lets-create-those-scores-1",
    "href": "content/09-continuous-predictors/slides/index.html#lets-create-those-scores-1",
    "title": "Continuous predictors",
    "section": "Let’s create those scores",
    "text": "Let’s create those scores\nOnce more, we measure agreement on a 100-point scale. We make several assumptions:\n\nWe’ll center both predictors: It makes it easier to think about the interaction.\nAt average liking and trustworthiness (0 = centered), agreement is on the mid-point of the scale: 50\nThere’s a main effect of liking: With each extra point (at average trustworthiness), agreement should go up by 3 points.\nThere’s a main effect of trustworthiness: With each extra point (at average liking), agreement should go up by 2 points.\nWe expect that going up on trustworthiness will enhance our liking effect by 2 points"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#put-into-numbers-1",
    "href": "content/09-continuous-predictors/slides/index.html#put-into-numbers-1",
    "title": "Continuous predictors",
    "section": "Put into numbers",
    "text": "Put into numbers\n\\[\\begin{align}\n& y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_1x_2\\\\\n& y = 50 + 3 \\times liking + 2 \\times trust + 2 \\times liking \\times trust\n\\end{align}\\]\n\n\\(\\beta_0\\): The outcome when both liking and trustworthiness are 0: 50\n\\(\\beta_1\\): Outcome when liking goes 1 up, but trustworthiness remains at average: +3\n\\(\\beta_2\\): Outcome when trustworthiness goes 1 up, but liking remains at average: +2\n\\(\\beta_3\\): In addition to the effect of liking going up 1, also going up in trustworthiness 1 adds to the outcome: 2"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#in-r-3",
    "href": "content/09-continuous-predictors/slides/index.html#in-r-3",
    "title": "Continuous predictors",
    "section": "In R",
    "text": "In R\n\nset.seed(42)\n\nb0 <- 50\nb1 <- 3\nb2 <- 2\nb3 <- 2\nn <- 1e4\nerror <- rnorm(n, 0, 20)\n\nliking <- runif(n, 0, 7)\ntrustworthiness <- runif(n, 0, 7)\nliking <- scale(liking, center = TRUE, scale = FALSE)\ntrustworthiness <- scale(trustworthiness, center = TRUE, scale = FALSE)\n\nd <- \n  data.frame(\n    liking = liking,\n    trustworthiness = trustworthiness,\n    agree = b0 + b1 * liking + b2 * trustworthiness + b3 * liking * trustworthiness + error\n  )\n\nd$agree <- ifelse(d$agree > 100, 100, d$agree)\nd$agree <- ifelse(d$agree < 0, 0, d$agree)"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#lets-find-our-numbers-1",
    "href": "content/09-continuous-predictors/slides/index.html#lets-find-our-numbers-1",
    "title": "Continuous predictors",
    "section": "Let’s find our numbers",
    "text": "Let’s find our numbers\n\nsummary(lm(agree ~ liking*trustworthiness, d))\n\n\nCall:\nlm(formula = agree ~ liking * trustworthiness, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-59.451 -13.609   0.126  13.583  66.108 \n\nCoefficients:\n                       Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            49.68043    0.19581  253.72   <2e-16 ***\nliking                  2.97717    0.09649   30.85   <2e-16 ***\ntrustworthiness         1.83232    0.09605   19.08   <2e-16 ***\nliking:trustworthiness  1.92430    0.04730   40.69   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.58 on 9996 degrees of freedom\nMultiple R-squared:  0.2276,    Adjusted R-squared:  0.2274 \nF-statistic:   982 on 3 and 9996 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#pictures-plesase",
    "href": "content/09-continuous-predictors/slides/index.html#pictures-plesase",
    "title": "Continuous predictors",
    "section": "Pictures, plesase",
    "text": "Pictures, plesase"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#interaction-plots",
    "href": "content/09-continuous-predictors/slides/index.html#interaction-plots",
    "title": "Continuous predictors",
    "section": "Interaction plots",
    "text": "Interaction plots\nIn the above plot, I cheated a bit by rounding trustworthiness so that we can get only 7 “levels”. With truly continuous variables, we usually operate with standard deviations: What’s the effect of liking on agreeing when trustworthiness is 1 SD above or below its mean? You can create those plots yourself or rely on other packages."
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#prediction-plots",
    "href": "content/09-continuous-predictors/slides/index.html#prediction-plots",
    "title": "Continuous predictors",
    "section": "Prediction plots",
    "text": "Prediction plots\n\nm <- lm(agree ~ liking*trustworthiness, d)\n\ninteractions::interact_plot(m, pred = \"liking\", modx = \"trustworthiness\")"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#from-the-matrix",
    "href": "content/09-continuous-predictors/slides/index.html#from-the-matrix",
    "title": "Continuous predictors",
    "section": "From the matrix",
    "text": "From the matrix\n\\(y = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + \\beta_3x_1x_2\\)\nThe interaction term is really just a product, so we can treat it as its own variable. We can simply create a correlation matrix where we specify how the product of two variables (aka \\(x_1 \\times x_2\\)) should correlate with our outcome.\n\\[\n\\begin{bmatrix}\n& x_1 & x_2 & x_1x_2 & y \\\\\nx_1 & 1 & & & \\\\\nx_2 & r & 1 & &\\\\\nx_1x_2 & r & r & 1 &\\\\\ny & r & r& r& 1\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#putting-that-into-numbers",
    "href": "content/09-continuous-predictors/slides/index.html#putting-that-into-numbers",
    "title": "Continuous predictors",
    "section": "Putting that into numbers",
    "text": "Putting that into numbers\nLet’s say liking is correlated to agreeing with 0.2, trustworthiness with 0.25, and the interaction “adds” 0.1 standard deviations. The interaction is correlated to liking and trustworthiness with 0. Liking and trustworthiness are correlated at 0.4.\n\\[\n\\begin{bmatrix}\n& x_1 & x_2 & x_1x_2 & y \\\\\nx_1 & 1 & & & \\\\\nx_2 & 0.4 & 1 & &\\\\\nx_1x_2 & 0 & 0 & 1 &\\\\\ny & 0.2 & 0.25 & 0.1 & 1\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#in-r-4",
    "href": "content/09-continuous-predictors/slides/index.html#in-r-4",
    "title": "Continuous predictors",
    "section": "In R",
    "text": "In R\n\nlibrary(MASS)\n\nmeans <- c(x1 = 0, x2 = 0, x1x2 = 0, y = 0)\n\nsigma <- \n  matrix(\n    c(\n    1, 0.4, 0, 0.2,\n    0.4, 1, 0, 0.25,\n    0, 0, 1, 0.1,\n    0.2, 0.25, 0.1, 1\n  ),\n  ncol = 4\n  )\n\nd <- \n  mvrnorm(\n    1e4,\n    means,\n    sigma\n  )\n\nd <- as.data.frame(d)"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#lets-find-our-numbers-2",
    "href": "content/09-continuous-predictors/slides/index.html#lets-find-our-numbers-2",
    "title": "Continuous predictors",
    "section": "Let’s find our numbers",
    "text": "Let’s find our numbers\n\ncor(d)\n\n              x1           x2         x1x2         y\nx1   1.000000000  0.422176276  0.002282137 0.2097402\nx2   0.422176276  1.000000000 -0.002203798 0.2535390\nx1x2 0.002282137 -0.002203798  1.000000000 0.1082762\ny    0.209740212  0.253539037  0.108276236 1.0000000"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#summary",
    "href": "content/09-continuous-predictors/slides/index.html#summary",
    "title": "Continuous predictors",
    "section": "Summary",
    "text": "Summary\nRemember: Those are conditional effects, not just 1-to-1 correlations.\n\nsummary(lm(y ~ x1 + x2 + x1x2, d))\n\n\nCall:\nlm(formula = y ~ x1 + x2 + x1x2, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3061 -0.6368  0.0043  0.6319  3.4221 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.006035   0.009414   0.641    0.521    \nx1          0.120853   0.010219  11.826   <2e-16 ***\nx2          0.196707   0.010297  19.103   <2e-16 ***\nx1x2        0.106056   0.009338  11.358   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9414 on 9996 degrees of freedom\nMultiple R-squared:  0.08888,   Adjusted R-squared:  0.0886 \nF-statistic:   325 on 3 and 9996 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#what-about-raw",
    "href": "content/09-continuous-predictors/slides/index.html#what-about-raw",
    "title": "Continuous predictors",
    "section": "What about raw?",
    "text": "What about raw?\nIf we want to work on the raw scale, we once more need the standard deviations. We could do the transformation per variable variable pairing, but that gets very unwieldy. At this point, you’d need to do some matrix multiplication, see here for a starter."
  },
  {
    "objectID": "content/10-goodbye/10-slides.html",
    "href": "content/10-goodbye/10-slides.html",
    "title": "Slides",
    "section": "",
    "text": "If you want to see the slides in full screen, you can click here. To download them as PDFs, hit the ‘e’ button when you got the presentation open and then Ctrl+P print to PDF. This will work best in Chrome (in Firefox, it didn’t print the headings for me)."
  },
  {
    "objectID": "content/10-goodbye/slides/index.html#why-this-workshop",
    "href": "content/10-goodbye/slides/index.html#why-this-workshop",
    "title": "Goodbye",
    "section": "Why this workshop",
    "text": "Why this workshop\n\nDesigning an informative study is a key skill\nA study is rarely informative if it can’t detect what you’re after\nNeglecting power means not knowing what our results mean"
  },
  {
    "objectID": "content/10-goodbye/slides/index.html#so-why-are-we-here",
    "href": "content/10-goodbye/slides/index.html#so-why-are-we-here",
    "title": "Goodbye",
    "section": "So why are we here?",
    "text": "So why are we here?\nThe goal of the workshop is for you to (1) have an understanding of the philosophy behind using data to test claims, (2) get an intuition of how data generation processes work, (3) learn the technical skills to turn these processes into data, and (4) use these skills to simulate power for an informative study."
  },
  {
    "objectID": "content/10-goodbye/slides/index.html#whats-power",
    "href": "content/10-goodbye/slides/index.html#whats-power",
    "title": "Goodbye",
    "section": "What’s power",
    "text": "What’s power\n\nUnderstanding of the logic behind NHST\nIntuition about what power is\nSee why power, perhaps, potentially isn’t just a hoop to jump through"
  },
  {
    "objectID": "content/10-goodbye/slides/index.html#simulations-in-r",
    "href": "content/10-goodbye/slides/index.html#simulations-in-r",
    "title": "Goodbye",
    "section": "Simulations in R",
    "text": "Simulations in R\n\nUnderstand why simulations are useful\nLogic of Monte Carlo Simulations\nBasic tools"
  },
  {
    "objectID": "content/10-goodbye/slides/index.html#effect-sizes",
    "href": "content/10-goodbye/slides/index.html#effect-sizes",
    "title": "Goodbye",
    "section": "Effect sizes",
    "text": "Effect sizes\n\nUnderstand the importance of effect sizes\nHow to formulate a smallest effect size of interest\nKnow when you don’t have enough information"
  },
  {
    "objectID": "content/10-goodbye/slides/index.html#alpha-beta-sensitivity",
    "href": "content/10-goodbye/slides/index.html#alpha-beta-sensitivity",
    "title": "Goodbye",
    "section": "Alpha, beta, sensitivity",
    "text": "Alpha, beta, sensitivity\n\nQuestion the default of \\(\\alpha\\) = 0.05 and power = 80%\nUnderstand how terribly complex designing an informative study is\nKnow where to turn when you don’t have enough information"
  },
  {
    "objectID": "content/10-goodbye/slides/index.html#categorical-predictors",
    "href": "content/10-goodbye/slides/index.html#categorical-predictors",
    "title": "Goodbye",
    "section": "Categorical predictors",
    "text": "Categorical predictors\n\nUnderstand the logic behind the data generating process\nSee how the linear model is our data generating process\nApply this to a setting with multiple categories in a predictor"
  },
  {
    "objectID": "content/10-goodbye/slides/index.html#interactions",
    "href": "content/10-goodbye/slides/index.html#interactions",
    "title": "Goodbye",
    "section": "Interactions",
    "text": "Interactions\n\nUnderstand what an interaction is from the perspective of the linear model\nMake yourself think in more detail about the form of interactions\nBe able to translate that detail to generating data"
  },
  {
    "objectID": "content/10-goodbye/slides/index.html#continuous-predictors",
    "href": "content/10-goodbye/slides/index.html#continuous-predictors",
    "title": "Goodbye",
    "section": "Continuous predictors",
    "text": "Continuous predictors\n\nUnderstand that continuous predictors are just another case of the linear model\nExtend this understanding to continuous (by categorical) interactions\nBe able to translate that extension to generating data"
  },
  {
    "objectID": "content/10-goodbye/slides/index.html#other-better-solutions",
    "href": "content/10-goodbye/slides/index.html#other-better-solutions",
    "title": "Goodbye",
    "section": "Other (better) solutions",
    "text": "Other (better) solutions\n\nfaux\nsimstudy\nSuperpower\nPrimer for mixed-effects models\nPrimer and paper for SEM models"
  },
  {
    "objectID": "index.html#this-website",
    "href": "index.html#this-website",
    "title": "Welcome",
    "section": "This website",
    "text": "This website\nThis website collects the materials for a 2-day workshop on simulating power with R that I’ve given at the Universities of Vienna, Zurich, and Mainz (to thundering applause and very few boos). Shoot me an email if you’re interested in giving the workshop at your uni/company.\nRather than dumping the materials in zip folder somewhere, I thought this format will be better for a) participants to revisit, b) those who might be interested in the content but couldn’t make it. That’s also the reason most of the materials are so annoyingly verbose: I wanted readers to be able to follow along by themselves and re-visit."
  },
  {
    "objectID": "index.html#goal",
    "href": "index.html#goal",
    "title": "Welcome",
    "section": "Goal",
    "text": "Goal\nThe goal of the workshop is for you to (1) have an understanding of the philosophy behind using data to test claims, (2) get an intuition of how data generation processes work, (3) learn the technical skills in R to turn these processes into data, and (4) use these skills to simulate power for an informative study."
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Welcome",
    "section": "Schedule",
    "text": "Schedule\nBelow is the 2-day schedule. From experience, there’s quite a lot of variability on how far in the schedule we can get in 2 days. It’ll mostly depend on how much time you need to go through the exercises (and most likely we won’t do all of them). Technically, after Exercise 2, you’ll have everything you need, but I aim to at least make it to interactions. The rest you can also do at home in case we don’t make it all the way.\nDay 1\n\n\n\nWhat?\nWhen?\n\n\n\n\n9:00-9:45\nIntro\n\n\n10:00-10:45\nWhat’s power?\n\n\n11:00-11:45\nSimulations in R\n\n\n12:00-13:00\nExercise 1\n\n\n14:00-14:45\nEffect sizes\n\n\n15:00-15:45\nExercise 2\n\n\n16:00-16:45\nAlpha, beta, sensitivity\n\n\n17:00-17:45\nExercise 3\n\n\n17:45-18:00\nRecap\n\n\n\nDay 2\n\n\n\nWhat?\nWhen?\n\n\n\n\n9:00-9:45\nRecap\n\n\n10:00-10:45\nCategorical predictors\n\n\n11:00-11:45\nExercise 4\n\n\n12:00-13:00\nInteractions\n\n\n14:00-14:45\nExercise 5\n\n\n15:00-15:45\nContinuous predictors\n\n\n16:00-16:45\nExercise 6\n\n\n17:00-17:45\nBuffer\n\n\n17:45-18:00\nRecap"
  },
  {
    "objectID": "index.html#takeaways",
    "href": "index.html#takeaways",
    "title": "Welcome",
    "section": "Takeaways",
    "text": "Takeaways\nEach of these blocks has a several learning outcomes. They are:\n\nWhat’s power\n\nUnderstanding of the logic behind NHST\nIntuition about what power is\nSee why power, perhaps, potentially isn’t just a hoop to jump through\n\n\n\nSimulations in R\n\nUnderstand why simulations are useful\nLogic of Monte Carlo Simulations\nBasic tools\n\n\n\nEffect sizes\n\nUnderstand the importance of effect sizes\nHow to formulate a smallest effect size of interest\nKnow when you don’t have enough information\n\n\n\nAlpha, beta, sensitivity\n\nQuestion the default of \\(\\alpha\\) = 0.05 and power = 80%\nUnderstand how terribly complex designing an informative study is\nKnow where to turn when you don’t have enough information\n\n\n\nCategorical predictors\n\nUnderstand the logic behind the data generating process\nSee how the linear model is our data generating process\nApply this to a setting with multiple categories in a predictor\n\n\n\nInteractions\n\nUnderstand what an interaction is from the perspective of the linear model\nMake yourself think in more detail about the form of interactions\nBe able to translate that detail to generating data\n\n\n\nContinuous predictors\n\nUnderstand that continuous predictors are just another case of the linear model\nExtend this understanding to continuous (by categorical) interactions\nBe able to translate that extension to generating data"
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Welcome",
    "section": "Prerequisites",
    "text": "Prerequisites\nI expect that you’re somewhat familiar (though not deeply) with R and vaguely remember your undergrad stats classes. Other than that, you should be good to go. That said, if you only have a couple of hours of R experience or have only done a short introduction, you’ll struggle. It’ll be difficult to learn both programming and the concepts you’re supposed to implement.\nWhat you’ll need for the workshop:\n\nA laptop\nA working installation of R and preferably RStudio\nGPower installed\nFor communication, please join this discord channel that we’ll use throughout the 2 days:"
  },
  {
    "objectID": "index.html#structure",
    "href": "index.html#structure",
    "title": "Welcome",
    "section": "Structure",
    "text": "Structure\nThe workshop will follow a structure of learn, do, recall. So typically, for each topic, I’ll introduce theoretical concepts; then you’ll do lots of exercise where you try to apply and extend these concepts; afterwards, there will be a short quiz.\nOn this website, only the first two are present: On the left-hand side, you see the slides per topic and the exercises."
  },
  {
    "objectID": "index.html#getting-a-local-copy",
    "href": "index.html#getting-a-local-copy",
    "title": "Welcome",
    "section": "Getting a local copy",
    "text": "Getting a local copy\nIf you want a local copy of this website, just download the entire project from Github: https://github.com/niklasjohannes/power-workshop (Code –> Download Zip). Unzip and then go to the docs folder, where you can double click on index. Alternatively, you can select individual slides/exercises within the docs folder. If you want to look at the source code, check the content folder. (Here you’ll also find the code for the solutions for the exercises.)"
  },
  {
    "objectID": "index.html#copyright",
    "href": "index.html#copyright",
    "title": "Welcome",
    "section": "Copyright",
    "text": "Copyright\nThis content is licensed under a GPL-3.0 License, so feel free to use the materials as you please (just not for commercial purposes). I’d appreciate attribution, though; making this workshop was a lot of work."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome",
    "section": "",
    "text": "Hello there. Internet etiquette dictates that any website that deals with stats starts with a GIF to signal that this’ll be fun. I’m not one to breach etiquette, so here we go:"
  },
  {
    "objectID": "content/01-intro/slides/index.html#nothing-wrong-with-gpower",
    "href": "content/01-intro/slides/index.html#nothing-wrong-with-gpower",
    "title": "Welcome",
    "section": "Nothing wrong with GPower",
    "text": "Nothing wrong with GPower\n\nGPower works great!\nRuns the risk of treating power just as a hoop to jump through\nSimulating data instead forces us to be explicit about many more features than GPower asks for"
  },
  {
    "objectID": "content/01-intro/slides/index.html#about-that-schedule",
    "href": "content/01-intro/slides/index.html#about-that-schedule",
    "title": "Welcome",
    "section": "About that schedule",
    "text": "About that schedule\n\nTiming will be way off: Schedule depends heavily on how much time is needed for exercises\nGood news is: After half a day you’ll know everything you need–everything else is just bonus\nThat’s why these slides are so full: I wrote the entire thing so you can go and revisit"
  },
  {
    "objectID": "content/02-whats-power/slides/index.html#put-differently",
    "href": "content/02-whats-power/slides/index.html#put-differently",
    "title": "What’s power?",
    "section": "Put differently",
    "text": "Put differently\nTu put it differently: Small studies are only sensitive to large effects. But if the effect is truly small, we’ll only get a significant result for the rare massive overestimate.\nLet’s have a look again: Preview"
  },
  {
    "objectID": "content/03-simulations-in-r/03-exercise.html",
    "href": "content/03-simulations-in-r/03-exercise.html",
    "title": "Exercise I",
    "section": "",
    "text": "This is the first set of exercises where you get familiar with basic commands that you’ll need for data simulation. It’ll make it easier later on when you do more complicated designs to have a good intuition how these commands work.\nThis part might be boring to you if you’re already a seasoned R user. In that case, I recommend you get a coffee, tea, and/or nap.\nAfter some feedback from a previous iteration of this workshop, I’ll give quite a lot of tips. They’ll come in the form below, and I recommend you ignore them and try it out on your own first:\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nTips will appear here, such as:\n\nsample(\n  x = ?,\n  size = how many samples?\n)\n\n\n\n\n\n\n\n\nYou have three groups. Name the groups, randomly sample ten cases (in total, so uneven numbers per group), and then create a simple data frame that contains a variable for the group called condition.\n\ngroups <- letters[1:3]\nmy_sample <- sample(groups, 10, TRUE)\nd <- data.frame(condition = as.factor(my_sample))\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nFirst create an object that contains three names (can be anything, but I recommend using letters). Then:\n\nsample(\n  x = your groups,\n  size = ten cases,\n  replace = ?\n)\n\nThen use data.frame.\n\n\n\n\n\n\nSame three groups. This time you want each case to have a 70% to be in the first group, a 20% to be in the second group, and a 10% to be in the third group. Get 100 participants (no need for a data frame). Use set.seed(1). How many are in the first group?\n\nset.seed(1)\ngroups <- letters[1:3]\nmy_sample <- \n  sample(\n    groups,\n    100,\n    TRUE,\n    prob = c(0.7, 0.2, 0.1)\n  )\nsum(my_sample==groups[1])\n\n[1] 68\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nCheck ?sample and have a look at the prob argument. You can count how many are in the first group by summing up (sum) the groups or using a table (table).\n\n\n\n\n\n\nShow that sample with assigned probability (prob = argument) is the same as rbinom. Conduct 10 coin flips with a an unfair coin that has a 60% of landing heads. Remember to set a seed (tip: twice).\n\nset.seed(1)\nsample(0:1, 10, replace = TRUE, prob = c(0.4, 0.6))\n\n [1] 1 1 1 0 1 0 0 0 0 1\n\nset.seed(1)\nrbinom(10, 1, 0.6)\n\n [1] 1 1 1 0 1 0 0 0 0 1\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nSet the seed, then call sample:\n\nsample(\n  x = head or tails,\n  size = how many samples?,\n  replace = ?,\n  prob = c(0.5, 0.5)\n)\n\nThen set the seed again and call rbinom. You need 10 “experiments” with only one outcome. Check ?rbinom for the prob argument. 0.5 would give you a 50% chance of getting the outcome.\n\n\n\n\n\n\nDraw random letters from the alphabet until the alphabet is empty.\n\nsample(letters, length(letters))\n\n [1] \"s\" \"a\" \"u\" \"x\" \"j\" \"n\" \"v\" \"g\" \"i\" \"o\" \"e\" \"p\" \"r\" \"w\" \"q\" \"l\" \"b\" \"m\" \"d\"\n[20] \"z\" \"y\" \"h\" \"c\" \"t\" \"k\" \"f\"\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nTell sample to draw as many letters as there are in the alphabet. Use letters.\n\n\n\n\n\n\nDraw all letters from the alphabet and explicitly assign the same probability for each letter (tip: repeat the same probability).\n\nprobs <- rep(1/length(letters), length(letters))\nsample(letters, prob = probs)\n\n [1] \"r\" \"v\" \"o\" \"n\" \"w\" \"b\" \"l\" \"t\" \"s\" \"k\" \"y\" \"i\" \"f\" \"c\" \"e\" \"j\" \"q\" \"u\" \"m\"\n[20] \"a\" \"g\" \"p\" \"h\" \"x\" \"d\" \"z\"\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nUse the prob argument. You need one probability for each letter. That probability is the same for each letter, so in effect, you can just repeat the probability. If only there were an R command to repeat things.\n\n\n\n\n\n\nCreate a data set. In the data set, each participant has an identifier (called id), a group identifier (condition), and an identifier of what number of measurement we have for this participant (trial). There are 3 participants in each of three groups with 5 trials in each group.\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nUltimately, you want to have something of the following form:\n\n\n   id condition trial\n1   1         a     1\n2   1         a     2\n3   1         a     3\n4   1         a     4\n5   1         a     5\n6   2         b     1\n7   2         b     2\n8   2         b     3\n9   2         b     4\n10  2         b     5\n11  3         c     1\n12  3         c     2\n13  3         c     3\n14  3         c     4\n15  3         c     5\n16  4         a     1\n17  4         a     2\n18  4         a     3\n19  4         a     4\n20  4         a     5\n21  5         b     1\n22  5         b     2\n23  5         b     3\n24  5         b     4\n25  5         b     5\n26  6         c     1\n27  6         c     2\n28  6         c     3\n29  6         c     4\n30  6         c     5\n31  7         a     1\n32  7         a     2\n33  7         a     3\n34  7         a     4\n35  7         a     5\n36  8         b     1\n37  8         b     2\n38  8         b     3\n39  8         b     4\n40  8         b     5\n41  9         c     1\n42  9         c     2\n43  9         c     3\n44  9         c     4\n45  9         c     5\n\n\nSo first create the groups/conditions. Then decide how many participants you want and how many trials you want. Next, create a data frame:\n\ndata.frame(\n  id = # it should go id number for as many trials as there are: 11111 22222 etc.,\n  condition = # each id above needs to only have one condition as many times as there are trials: aaaaa bbbbb ccccc aaaaa etc.,\n  trial = # repeat 12345 as many times as there are participants\n)\n\n\n\n\n\n\n\nYou have two groups, a control and a treatment group. In each group, there are 10 participants. Each participant flips a coin 10 times. The control group has a fair coin: 50% heads. The treatment group has an unfair coin: 70% heads. Create a data frame with a participant identifier (id), group membership (condition), and a total head count for that participant (heads). Check that the two groups indeed have different means of how often they get heads (roughly corresponding to the two probabilities)\n\ngroups <- c(\"control\", \"treatment\")\nn <- 20\nflips <- 10\nfair <- 0.5\nunfair <- 0.7\nprobs <- rep(c(fair, unfair), each = n/2)\n\n\nd <- \n  data.frame(\n    id = 1:20,\n    condition = rep(groups, each = n/2),\n    heads = rbinom(n, flips, prob = probs)\n  )\n\naggregate(d$heads, by = list(d$condition), mean)\n\n    Group.1   x\n1   control 5.4\n2 treatment 6.5\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nI recommend to define your parameters at the beginning (sometimes called declaring your variables or assignment block):\n\ngroups <- #?\nn <- #?\nflips <- #?\nfair <- #?\nunfair <- #?\nprobs <- #? (Check how you assigned probabilities for the alphabet exercise above)\n\nThen you can create a data frame like you did above, with IDs running from minimum to maximum, the first half having control assignment, the second half having treatment assignment, and a head count with rbinom (which takes the number of experiments, how many flips there are per experiments, and the probability of heads for that experiments; the experiment is each participant).\nThen use aggregate or table.\n\n\n\n\n\n\nYou have 100 participants. Each participants reports their age which lies uniformly between 20 and 40. They also report grumpiness on a 100-point scale (with a 10-point SD). Each extra year predicts 0.5 higher grumpiness (Tip: Check the lecture slides; you’ll need to add some error). Create the two variables (no need for a data frame) and conduct a correlation with cor. What’s the correlation?\n\nn <- 100\nage <- runif(n, 20, 40)\ngrumpiness <- age*0.5 + rnorm(n, 100, 10)\ncor(age, grumpiness)\n\n[1] 0.2127516\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nGo to slide 29 here and replace the error (norm) with the one specified.\n\n\n\n\n\n\nWe track how many calls you get during this exercise. Nobody calls anymore, so there’ll be very few. Create a data frame with 20 participants, a participant number and the number of calls per participant. Plot the calls and show that 0 is the most common value.\n\nn <- 20\n\nd <- data.frame(\n  id = 1:n,\n  calls = rpois(n, 0.5)\n)\n\nplot(density(d$calls)) \n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nCreate a data frame with an id variable running from 1 to 20, then call rpois and play around with the lambda parameter until plot(density(dataframe$calls)) shows a spike at zero.\n\n\n\n\n\n\nProfessors get more calls. Add 20 more participants who have much higher call numbers. Also include a condition variable that marks whether participants are students (first 20 people) or professors (new 20 people). Conduct an independent sample t-test (t.test) and also plot the different groups as a boxplot.\n\nd <- rbind(\n  d,\n    data.frame(\n    id = n+1:2*n,\n    calls = rpois(n, 3)\n  )\n)\n\nd$condition <- \n  rep(c(\"students\", \"profs\"), each = 20)\n\nt.test(d$calls ~ d$condition)\n\n\n    Welch Two Sample t-test\n\ndata:  d$calls by d$condition\nt = 5.8023, df = 26.202, p-value = 3.989e-06\nalternative hypothesis: true difference in means between group profs and group students is not equal to 0\n95 percent confidence interval:\n 1.679272 3.520728\nsample estimates:\n   mean in group profs mean in group students \n                  3.25                   0.65 \n\nboxplot(d$calls ~ d$condition)\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nCreate a second data frame that has an id variables that runs from 21 to 40 as well as a calls variable that indicates the number of calls like you did before. Then use rbind to combine the data frame with the students with the data frame you just created. Add a condition variable:\n\nd$condition <- \n  rep(c(\"students\", \"profs\"), ?)\n\nNow run a t-test and a boxplot (boxplot).\n\n\n\n\n\n\n\nIn this section, you’ll use the basics from above to perform your first power analysis. You’ll apply repeated simulations over a range of values and extract and store results to summarize them.\n\n\nThere are four groups. Each group comes from a different normal distribution. The means are c(100, 105, 107, 109). The SDs are c(9, 12, 10, 17). Each group should be 20 cases. Store the scores in a data frame and have a variable that indicates the group. Tip: Remember that R uses vectors, even for arguments in a function.\n\nmeans <- c(100, 105, 107, 109)\nsds <- c(9, 12, 10, 17)\nn <- 20\n\nd <- \n  data.frame(\n    group = rep(c(letters[1:4]), times = n),\n    score = rnorm(n*4, means, sds)\n  )\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\n\nmeans <- ?\nsds <- ?\nn <- ?\n\nd <- \n  data.frame(\n    group = rep(c(letters[1:4]), ?), # make sure condition matches how the score below is drawn\n    score = rnorm(n*4, means, sds)\n  )\n\n\n\n\n\n\n\nYou need 5 samples. Each sample contains 10 unique letters from the alphabet. (Use replicate.)\n\nreplicate(5, sample(letters, 10))\n\n      [,1] [,2] [,3] [,4] [,5]\n [1,] \"o\"  \"u\"  \"p\"  \"l\"  \"e\" \n [2,] \"p\"  \"e\"  \"g\"  \"j\"  \"w\" \n [3,] \"q\"  \"l\"  \"z\"  \"a\"  \"f\" \n [4,] \"s\"  \"c\"  \"q\"  \"f\"  \"s\" \n [5,] \"n\"  \"r\"  \"k\"  \"q\"  \"t\" \n [6,] \"f\"  \"n\"  \"i\"  \"m\"  \"l\" \n [7,] \"k\"  \"m\"  \"h\"  \"b\"  \"c\" \n [8,] \"h\"  \"o\"  \"m\"  \"o\"  \"d\" \n [9,] \"e\"  \"b\"  \"f\"  \"d\"  \"o\" \n[10,] \"b\"  \"t\"  \"x\"  \"z\"  \"n\" \n\n\n\n\n\nSame as before, but this time you need 10 cases from a normal distribution with a mean of 10 and an SD of 2. Use replicate first, then a for loop.\n\nreplicate(5, rnorm(10, 10, 2))\n\n           [,1]      [,2]      [,3]      [,4]      [,5]\n [1,] 10.629034 10.077856 10.916251 11.087316  8.782663\n [2,] 10.444436 10.029294  7.095943 12.082121  9.397606\n [3,]  8.312769  9.627366 10.154685 10.395012 11.952405\n [4,] 10.887611 12.801182 11.119791  6.740843 10.912017\n [5,] 10.111594 10.036971  9.850107 10.242080 12.588816\n [6,] 10.135945 10.498392 11.565395  6.725156  7.733596\n [7,]  9.596077 10.298441  9.654663  8.937914  8.261079\n [8,]  7.683969  8.073534  7.897412 11.907360  8.490059\n [9,]  8.814513  9.867066 11.458903  6.558699  9.740729\n[10,] 11.532131 12.573844 10.525330 10.212641  7.996397\n\nfor (i in 1:5) {\n  print(rnorm(10, 10, 2))\n}\n\n [1]  8.360263  8.050895 11.208619 11.097575 11.832865 15.323133  9.639486\n [8] 11.370030 16.532829 11.121201\n [1]  9.861965  8.055114  8.906827  6.622615  6.855255  9.190026 10.638573\n [8] 10.080855  9.219981  6.361556\n [1] 11.318361 10.919243 13.233253  6.287619  9.426352 13.500644 10.232827\n [8] 12.768506 11.148442 10.272982\n [1] 11.828432  6.398347  9.320239 11.212529 12.682261 11.534575 10.387451\n [8] 12.281133 10.027730  7.789388\n [1]  9.949675  9.672653 10.740119  9.238351 11.305905 14.122684  6.406710\n [8] 11.168154  8.554494  8.741671\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\n\nreplicate(?, rnorm(?, ?, ?))\n\nfor (i in 1:5) {\n  print(rnorm(?, ?, ?))\n}\n\n\n\n\n\n\n\nAssume we know the population mean in height (168cm) and its standard deviation (20). Assume we draw 10,000 samples from this distribution. Each sample has 50 participants. The standard deviation of these 10,000 sample means is the standard error.\nSimulate the standard error and compare it to the theoretical value: \\(SE = \\frac{\\sigma}{\\sqrt{n}}\\). (\\(\\sigma\\) is the standard deviation of the population.)\n\nn <- 50\nmu <- 168\nsd <- 20\nse <- sd / sqrt(n)\nmeans <- NULL \ndraws <- 1e5\n\nfor (i in 1:draws) {\n  means[i] <- mean(rnorm(n, mu, sd))\n}\n\ncat(\"The real SE is:\", round(se, digits = 2), \". The simulated SE is:\", round(sd(means), digits = 2), \".\")\n\nThe real SE is: 2.83 . The simulated SE is: 2.83 .\n\n# with replicate\nmeans <- \n  replicate(\n    draws,\n    rnorm(n, mu, sd)\n  )\n\nsd(colMeans(means))\n\n[1] 2.827195\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nLike we did earlier, first declare your variables:\n\nn <- ?\nmu <- ? # mu is the greek value for the true mean in the population\nsd <- ?\nse <- sd / sqrt(n) # calculating standard error\nmeans <- NULL # somewhere to store the means in\ndraws <- 1e5 # how many samples?\n\nThen we iterate over the number of samples, generate that sample, and take its mean.\n\nfor (i in 1:draws) {\n  current_sample <- rnorm(?, ?, ?)\n  current_mean <- mean(current_sample)\n  \n  # store\n  means[?] <- current_mean\n}\n\nNow you can compare the SD of means with the standard error.\n\n\n\n\n\n\nSame population. Draw 1,000 observations for each sample size between 20 and 100. Calculate the standard error for each sample size (like you did above by taking the SD of sample means) and plot it against the sample size. (Tip: You’ll need to iterate over two things.)\n\nmu <- 168\nsd <- 20\ndraws <- 1e3\nmin_sample <- 20\nmax_sample <- 100\n\nresults <- \n  data.frame(\n    sample_size = NULL,\n    se = NULL\n  )\n\nfor (i in min_sample:max_sample) {\n  \n  sample_means <- NULL\n  \n  for (j in 1:draws) {\n    sample_means[j] <- mean(rnorm(i, mu, sd))\n  }\n  \n  se <- sd(sample_means)\n  \n  results <- rbind(\n    results,\n    data.frame(\n      sample_size = i,\n      se = se\n    )\n  )\n}\n\nplot(results$sample_size, results$se)\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nLike we did earlier, first declare your variables and generate an object in which to store the results:\n\nmu <- ?\nsd <- ?\ndraws <- ? # how many draws are we doing?\nmin_sample <- ?# minimum sample size\nmax_sample <- ? # maximum sample size\n  \n# container\nresults <- \ndata.frame(\n  sample_size = NULL,\n  se = NULL\n)\n\nThen we iterate over the sample size, and for each sample size do 1,000 “draws”, then store the results for that sample size.\n\nfor (i in min_sample:max_sample) { # the sample sizes we want to iterate over\n  \n  sample_means <- NULL # somewhere to store the means for each of the 1,000 samples we'll do for this sample size\n  \n  for (j in 1:draws) { # second loop where we draw 1,000 samples for each sample size\n    sample_means[?] <- mean(rnorm(?, ?, ?)) # store the mean of this sample\n  }\n  \n  se <- sd(?) # once we're done with 1,000 draws, we need to get the SD of the sample means of this sample size\n  \n  # store the results\n  results <- rbind(\n    results, # the previous results\n    data.frame( # a new data row\n      sample_size = ?, # what sample size did we do in this iteration of the loop? Where is it stored?\n      se = ? # you calculated this above\n    )\n  )\n}\n\nThat’s it, you can plot the results with plot(results$sample_size, results$se).\n\n\n\n\n\n\nTurn the above into a function so that you can change the population effect size, SD, number of simulations, and sample size range. The function should also return the plot from above.\n\nsample_against_se <- \n  function(\n    mu = 168,\n    sd = 20,\n    draws = 1e3,\n    min_sample = 20,\n    max_sample = 100\n  ) {\n    results <- \n      data.frame(\n        sample_size = NULL,\n        se = NULL\n      )\n    \n    for (i in min_sample:max_sample) {\n      \n      sample_means <- NULL\n      \n      for (j in 1:draws) {\n        sample_means[j] <- mean(rnorm(i, mu, sd))\n      }\n      \n      se <- sd(sample_means)\n      \n      results <- rbind(\n        results,\n        data.frame(\n          sample_size = i,\n          se = se\n        )\n      )\n    }\n    \n    return(plot(results$sample_size, results$se))\n  }\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nYou basically do the variable declaration inside function(), then copy-paste everything else into the body of the function:\n\nsample_against_se <- # or whatever function name you want\n  function( # this should look familiar\n    mu = ?,\n    sd = ?,\n    draws = ?,\n    min_sample = ?,\n    max_sample = ?\n  ) {\n    # here you put everything from above in here\n    results <- \n      data.frame(\n        sample_size = NULL,\n        se = NULL\n      ) #etc.\n    \n    # then don't forget to return\n    return(plot(results$sample_size, results$se))\n  }\n\n\n\n\n\n\n\nTry out the function with two plots: when the population SD is 5 and when you do 10 draws. What changes?\n\nsample_against_se(sd = 5)\n\n\n\nsample_against_se(draws = 10)\n\n\n\n\n\n\n\nThe average height of men in Spain is 173cm (says Wikipedia). The population standard deviation is probablay around 7cm (source).\nYou draw a sample of men and want to test whether they’re significantly different from that mean (our H0). In fact, these men you have sampled are truly French (175.6cm, our true “effect size”). In other words, can we reject the null hypothesis that these men we sampled come from the Spanish distribution in favor of our alternative hypothesis that the true population value is greater than the Spanish population mean?\nYou calculate the z-statistic, which is calculated as follows: \\(\\frac{\\bar{X} - \\mu_0}{\\sigma/\\sqrt{N}}\\) This simply tells us how far from the population mean (well, the suspected population mean under the null hypothesis) our sample mean is in terms of standard errors. \\(\\bar{X}\\) is the sample mean, \\(\\mu_0\\) is the population mean under H0, \\(\\sigma\\) is the population standard deviation, and \\(N\\) is the sample size.\nThen we can look up the z-score to see what the probability is to score this high or higher (does that definition ring a bell?). In R, you can simply do that with a built-in function: pnorm(). For example, if we have a z-score of 1.645, our probability of obtaining such a value (or higher) is pnorm(1.645, lower.tail = FALSE) = 0.0499849 – our p-value for a one-sided test.\nWe can simulate the power of our statistical test (in this case, the z-statistic). Take a sample of 30 people from the French population, calculate the z-statistic, its p-value, and store the p-value. Do this 1,000 times. Plot the distribution of p-values. What can we conclude about the sample size?\n\n# sample size\nn <- 30\nh0 <- 173 # test against this null\nh1 <- 175.6 # true population  effect\nsd <- 7\nalpha <- 0.05\ndraws <- 1e3\n\npvalues <- NULL\n\nfor (i in 1:draws) {\n  d <- rnorm(n, h1, sd)\n  z <- (mean(d) - h0) / (sd / sqrt(n))\n  pvalues[i] <- pnorm(z, lower.tail = FALSE)\n}\n\nplot(density(pvalues))\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nThis all is much easier than it appears at first sight. Start with declaring your variables:\n\nn <- ? # sample size\nh0 <- ? # test against this null (read the description again to know which height is the null)\nh1 <- ? # true population effect (true height)\nsd <- ? # population SD\ndraws <- ? # how many draws?\n\nNow you do what you’ve been doing this whole time: Use a loop to draw a sample, then calculate the z statistic (this could be any statistical test), extract the p-value for that z-statistic (or p-value for any statistical test), and store the p-value somewhere.\n\n# somewhere to store pvalues\npvalues <- NULL\n\n# then our loop\nfor (i in 1:draws) {\n  d <- rnorm(?, ?, ?) # get a sample with the variables you declared above\n  z <- # translate the formula above into R code (?sqrt)\n  pvalues[?] <- ? # get the p-value with the code from the exercise instructions, but replace 1.645 with the z-value you calculated\n}\n\nThat’s it. Have a look at how the p-values are distributed with plot(density(pvalues)).\n\n\n\n\n\n\nNow calculate the proportion of p-values that are below 0.05. That’s your power: The proportion of tests that will detect that there’s a true effect. In our case, that effect is a difference of 2.6cm. (Tip: Count or sum how many p-values are below 0.05 and divide by total number of p-values).\n\nsum(pvalues < 0.05)/length(pvalues)\n\n[1] 0.668\n\n\n\n\n\nNow let’s do what we did before: Put the loop from above inside another loop that iterates over different sample sizes. Then put that all into a function that let’s you set the parameters of interest (sample size range, h0, h1, etc.). Then simulate power (1,000 simulations each) for samples between 30 and 100. Plot the sample size against power.\n\npower_function <- \n  function(\n    h0 = 173,\n    h1 = 175.6,\n    sd = 7,\n    alpha = 0.05,\n    draws = 1e3,\n    min_sample = 30,\n    max_sample = 100\n  ) {\n    results <- \n      data.frame(\n        sample_size = NULL,\n        power = NULL\n      )\n    \n    for (i in min_sample:max_sample) {\n      \n      pvalues <- NULL\n      \n      for (j in 1:draws) {\n        d <- rnorm(i, h1, sd)\n        z <- (mean(d) - h0) / (sd / sqrt(i))\n        pvalues[j] <- pnorm(z, lower.tail = FALSE)\n      }\n      \n      results <- rbind(\n        results,\n        data.frame(\n          sample_size = i,\n          power = sum(pvalues < 0.05)/length(pvalues)\n        )\n      )\n    }\n    \n    return(plot(results$sample_size, results$power, type = \"l\"))\n  }\n\npower_function()\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nYou basically do the variable declaration inside function() like you’ve done before, then copy-paste everything else into the body of the function:\n\npower_function <- # name as you please\n  function(\n    h0 = ?, # assign default values or scratch the \"= ?\" part\n    h1 = ?,\n    sd = ?,\n    alpha = ?,\n    draws = ?,\n    min_sample = ?, # the minimum sample size\n    max_sample = ? #  the maximum sample size\n  ) {\n    \n    # create a data frame to store results in\n    results <- \n      data.frame(\n        sample_size = NULL,\n        power = NULL\n      )\n    \n    # first loop\n    for (i in min_sample:max_sample) {\n      \n      # store pvalues for each sample size\n      pvalues <- NULL\n      \n      # second loop\n      for (j in 1:draws) {\n        # do what you did above and store all pvalues for each draw in `pvalues`\n      }\n      \n      # add the results for this sample size (aka pvalues)\n      results <- rbind(\n        results,\n        data.frame(\n          sample_size = ?,\n          power = ? # proportion of pvalues under 0.05\n        )\n      )\n    }\n    \n    # don't forget to return the plot\n    return(plot(results$sample_size, results$power, type = \"l\"))\n  }\n\n# call the function\npower_function()\n\n\n\n\n\n\n\nNow do the same thing with a one-sample t-test. (Tip: You only need to replace the z-scores with a t.test from which you can extract the p-value). (Another tip: Use the $ sign on where you stored the t-test results.)\n\npower_function_t <- \n  function(\n    h0 = 173,\n    h1 = 175.6,\n    sd = 7,\n    alpha = 0.05,\n    draws = 1e3,\n    min_sample = 30,\n    max_sample = 100\n  ) {\n    results <- \n      data.frame(\n        sample_size = NULL,\n        power = NULL\n      )\n    \n    for (i in min_sample:max_sample) {\n      \n      pvalues <- NULL\n      \n      for (j in 1:draws) {\n        d <- rnorm(i, h1, sd)\n        t <- t.test(d, mu = h0,alternative = \"greater\")\n        pvalues[j] <- t$p.value\n      }\n      \n      results <- rbind(\n        results,\n        data.frame(\n          sample_size = i,\n          power = sum(pvalues < 0.05)/length(pvalues)\n        )\n      )\n    }\n    \n    return(plot(results$sample_size, results$power, type = \"l\"))\n  }\n\npower_function()\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nInstead of calculating the z-value yourself, you run t.test(data, mu = null hypothesis value, alternative = \"greater\"), then extract the p-value with t.test$p.value. All you need is to change the code from above by two lines.\n\n\n\n\n\n\nJust for funsies (and for our next session), see what happens when the true effect is only 1cm in difference.\n\npower_function_t(h1 = h0+1)"
  },
  {
    "objectID": "content/04-effect-sizes/04-exercise.html",
    "href": "content/04-effect-sizes/04-exercise.html",
    "title": "Exercise II",
    "section": "",
    "text": "Welcome to the next set of exercises. At this point, I hope you feel somewhat comfortable with the logic of simulations as well as the commands that get you there. Now we’ll try to use these commands to add to your understanding of effect sizes and add an extra layer (aka another loop level). That’s what you’ll do in the first block.\nIn the second block, we’ll add a tad more complexity. You’ll get going with correlated measures (aka paired-sampled t-test) and get to know the while command."
  },
  {
    "objectID": "content/05-alpha-beta-sensitivity/05-exercise.html",
    "href": "content/05-alpha-beta-sensitivity/05-exercise.html",
    "title": "Exercise III",
    "section": "",
    "text": "This set of exercises won’t introduce new technical skills. Instead, you’ll apply your simulation skills to different concepts to get a better feeling for the false positive rate, the role of alpha, and how a sensitivity analysis works.\n\n\nThe False Positive Rate is the proportion of false positive findings among all positive (aka signifiant) findings. It’s defined as follows:\n\\[\\begin{align}\nFalse \\ positive \\ rate = \\frac{False \\ positives}{False \\ positives + True \\ positives}\\\\\\\\\nFalse \\ positives = \\phi * \\alpha\\\\\nTrue \\ positives = power * (1 - \\phi)\\\\\\\\\nFalse \\ positive \\ rate = \\frac{\\phi * \\alpha}{\\phi * \\alpha + power * (1 - \\phi)}\n\\end{align}\\]\n\\(\\phi\\) is the proportion of null hypotheses, in general in a field, that are true, \\(\\alpha\\) your false positive error rate, and power is \\((1-\\beta)\\).\nPlot how the false positive rate develops as \\(\\phi\\) goes from 0 to 1 for two \\(\\alpha\\) levels (.05 and .01.) and two levels of power (80% and 95%). No need for a simulation here. You can just straight up use the formula above to calculate the false positive rate. For that, it’s probably easiest to create a data frame. Try out the expand.grid command which creates a data frame of all combinations of several variables. For example:\n\niq_scores <- seq(100, 105, 1)\nsample_size <- c(10, 20)\nd <- expand.grid(iq_scores, sample_size)\n\nd\n\n   Var1 Var2\n1   100   10\n2   101   10\n3   102   10\n4   103   10\n5   104   10\n6   105   10\n7   100   20\n8   101   20\n9   102   20\n10  103   20\n11  104   20\n12  105   20\n\n\nYou can use the following code (make sure the variables are named accordingly):\n\nlibrary(ggplot2)\nggplot(d, aes(x = phis, y = fpr, color = as.factor(alphas))) + geom_line() + facet_wrap(~ power) + theme_bw()\n\n\nphis <- seq(0, 1, 0.01)\nalphas <- c(standard = .05, low = .01)\npower <- c(0.80, 0.95)\n\nd <- expand.grid(phis, alphas, power)\nnames(d) <- c(\"phis\", \"alphas\", \"power\")\nd$fpr <- (d$phis * d$alphas) / ((d$phis * d$alphas) + (d$power * (1 - d$phis)))\n\nlibrary(ggplot2)\nggplot(d, aes(x = phis, y = fpr, color = as.factor(alphas))) + geom_line() + facet_wrap(~ power) + theme_bw()\n\n\n\n\nIn physics, they use a five sigma rule. That means their alpha is \\(3*10^{-7}\\) or 1 in 3.5 million. Do the above again, but this time plot “our” 0.05 against five sigma and compare false positive rates.\n\nphis <- seq(0, 1, 0.01)\nalphas <- c(standard = .05, physics = 3*10^-7)\npower <- c(0.80, 0.95)\n\nd <- expand.grid(phis, alphas, power)\nnames(d) <- c(\"phis\", \"alphas\", \"power\")\nd$fpr <- (d$phis * d$alphas) / ((d$phis * d$alphas) + (d$power * (1 - d$phis)))\n\nggplot(d, aes(x = phis, y = fpr, color = as.factor(alphas))) + geom_line() + facet_wrap(~ power) + theme_bw()\n\n\n\n\n\n\n\nHow does the alpha level influence your power? Simulate two correlated scores. The means of the scores are 4 and 4.2; their SDs are 0.4 and 0.7. Their correlation is 0.65. Simulate power (500 runs) for sample sizes starting at 30 and going to a maximum of 110. Stop whenever you reach 95% power (so use while). Do that for 5 different alpha levels: c(0.005, 0.001, 0.01, 0.05, 0.10). Plot the results. As always, you can use the code below. What’s the influence of the alpha compared to the sample size?\n\nfor (analpha in alphas) {\n  \n  n <- ?\n  power <- ?\n  \n  while (power < ? & n <= ?) {\n    \n    for (i in 1:runs) {\n    }\n    n <- n + 1\n  }\n}\n\n\nlibrary(ggplot2)\n\nggplot(outcomes, aes(x = sample_size, y = power, color = as.factor(alpha))) + geom_line() + geom_hline(yintercept = 0.95) %>%  theme_classic()\n\n\nlibrary(MASS)\n\nmeans <- c(pre = 4, post = 4.3)\npre_sd <- 0.4\npost_sd <- 0.7\ncorrelation <- 0.65\nalphas <- c(0.005, 0.001, 0.01, 0.05, 0.10)\nruns <- 500\nn_max <- 110\n\nsigma <- \n    matrix(\n      c(\n        pre_sd**2,\n        correlation * pre_sd * post_sd,\n        correlation * pre_sd * post_sd,\n        post_sd**2\n      ),\n      ncol = 2\n    )\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    alpha = NULL,\n    power = NULL\n  )\n\nfor (analpha in alphas) {\n  \n  n <- 30\n  power <- 0\n  \n  while (power < 0.95 & n <= n_max) {\n    \n    pvalues <- NULL\n    \n    for (i in 1:runs) {\n      \n      d <- as.data.frame(\n        mvrnorm(\n          n,\n          means,\n          sigma\n        )\n      )\n      \n      t <- t.test(d$pre, d$post, paired = TRUE)\n      \n      pvalues[i] <- t$p.value\n    }\n    \n    power <- sum(pvalues < analpha) / length(pvalues)\n    \n    outcomes <- rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        alpha = analpha,\n        power = power\n      )\n    )\n    \n    n <- n + 1\n  }\n}\n\nggplot(outcomes, aes(x = sample_size, y = power, color = as.factor(alpha))) + geom_line() + geom_hline(yintercept = 0.95) + theme_classic()\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nFor this exercise, you’ll need to loop over different alpha levels. For each alpha level, you run a while command. Inside the while command you run simulations to determine power. Once you reach enough power, the while command will stop and we’ll go to the next alpha level in the first-level loop.\nFirst, we declare our variables.\n\nlibrary(MASS)\n\nmeans <- c(pre = ?, post = ?)\npre_sd <- ?\npost_sd <- ?\ncorrelation <- ?\nalphas <- c(0.005, 0.001, 0.01, 0.05, 0.10)\nruns <- ?\nn_max <- ? # what's our maximum sample size\n\n# make an outcome data frame\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    alpha = NULL,\n    power = NULL\n  )\n\nOur variables are correlated, so we need to specify a variance-covariance matrix. Thankfully, we only need to do this once because the means, SDs, and the correlation are independent of the alpha level and therefore constant.\n\nsigma <- \n    matrix(\n      c(\n        pre_sd**2,\n        correlation * pre_sd * post_sd,\n        correlation * pre_sd * post_sd,\n        post_sd**2\n      ),\n      ncol = 2\n    )\n\nSweet, then we just fill our variables into the structure provided in the exercise instructions.\n\nfor (analpha in alphas) { # iterate over the alphas\n  \n  n <- 30 # inititate sample size and power\n  power <- 0\n  \n  while (power < ? & n <= ?) { # we stop either when we reach power OR when we reach the maximum sample size\n    \n    pvalues <- NULL # store our p-values for this alpha \n    \n    for (i in 1:runs) {\n      \n      d <- as.data.frame(\n        mvrnorm(\n          ?,\n          ?,\n          ?\n        )\n      )\n      \n      # conduct paired samples t-test\n      # store p-value in pvalues\n    }\n    \n    # now we calculate and store power for this alpha for all these runs\n    power <- ?\n    \n    # add that power, our sample size, and the alpha to the outcome data frame\n    \n    n <- ? # update our sample size for the next chunk of runs\n  }\n}\n\n\n\n\n\n\n\nYou have a large sample (2,000 people) from a public cohort study. You’re interested in comparing two groups on their intelligence. Your smallest effect effect size of interest is 3 IQ points. You know of Lindley’s paradox where even small p-values are actually evidence for H0 if the test has a lot of power. Therefore, you decide to conduct a compromise analysis in GPower for an independent, one-tailed t-test. You think that type 2 errors in this case are twice as bad as Type I errors. (Tip: IQ scores are standardized with a mean of 100 and an SD of 15–this should help you get a standardized effect size).\nObtain the new alpha from GPower. Then check whether it helps with Lindley’s paradox. Simulate drawing 10,000 samples with exactly your SESOI and that sample size; run a t-test oon this sample. Also do 10,000 samples where there is 0 difference. What proportion of the p-values of the simulation with an effect are below your new alpha? (Aka: Does your power estimate align with GPower’s power output?). Then plot the p-values of both simulations between 0 and alpha. Have you taken care of Lindley’s paradox?\nYou can use this code (if your data d are in the long format where the variable type indicates whether we have the effect distribution or the null distribution):\n\nlibrary(ggplot2)\n\nggplot(d, aes(x = pvalue, color = type)) + geom_density() + xlim(c(0, 0.02)) + ylim(c(0, draws/10)) + geom_vline(xintercept = alpha) + theme_bw()\n\n\nset.seed(42)\n\nn <- 1e3\nm <- 100\nsd <- 15\nsesoi <- 3\ndraws <- 1e4\nalpha <- 0.008925552\n\npvalues <- NULL\nnulls <- NULL\n\nfor (i in 1:draws) {\n  \n  control <- rnorm(n, m, sd)\n  treatment <- rnorm(n, m + sesoi, sd)\n  \n  pvalues[i] <- t.test(control, treatment, alternative = \"less\")$p.value\n  \n  control <- rnorm(n, m, sd)\n  treatment <- rnorm(n, m, sd)\n  \n  nulls[i] <- t.test(control, treatment, alternative = \"less\")$p.value\n}\n\npower <- sum(pvalues < alpha) / length(pvalues)\n\nd <- data.frame(\n  pvalue = c(pvalues, nulls),\n  type = rep(c(\"effect\", \"no effect\"), each = draws)\n)\n\nggplot(d, aes(x = pvalue, color = type)) + geom_density() + xlim(c(0, 0.02)) + ylim(c(0, draws/10)) + geom_vline(xintercept = alpha) + theme_bw()\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nFor this exercise, you’ll we simply need to conduct two t-tests per run: one with a difference and one without a difference. Let’s first declare our variables.\n\nn <- ?\nm <- ?\nsd <- ?\nsesoi <- ?\ndraws <- ?\nalpha <- # from GPower\n\nThen we run the simulation, for each draw generating two p-values and storing them.\n\n# somewhere to store the outcomes\npvalues <- NULL\nnulls <- NULL\n\nfor (i in 1:?) {\n  \n  # here we have a difference\n  control <- rnorm(?, ?, ?)\n  treatment <- rnorm(?, ?, ?)\n  \n  # run the first test and store the p-value\n  ? <- t.test(control, treatment, alternative = \"less\")$p.value\n  \n  # then we run a second t-test (without a group difference) and store the p-value\n  control <- rnorm(n, m, sd)\n  treatment <- rnorm(n, ?, sd)\n  \n  # store it\n  nulls[i] <- t.test(control, treatment, alternative = \"less\")$p.value\n}\n\nNow we have everything we need. First, we can calculate power like we always do with sum on pvalues. Then let’s store both types of p-values (effect and no effect) in a data frame. Afterwards, we can use the code from the instructions for plotting.\n\nd <- data.frame(\n  pvalue = c(pvalues, nulls),\n  type = rep(c(\"effect\", \"no effect\"), each = draws)\n)\n\n\n\n\n\n\n\nFor your master thesis, you ran a study where you conducted a paired-samples t-test. At the time, you didn’t know about power analysis. Now as you write the paper up for publication, you state that you didn’t conduct a power analysis, but you want to at least report the sensitivity of the test. Your sample size was 27 and you conducted a two-tailed test. Your alpha was 0.05. Simulate the sensitivity of your study (1,000 runs) for standardized effects ranging from 0 to 1 for 95% power. Verify with GPower. (Tip: Remember that the test is just on the difference of the two scores, so you can directly draw the difference. So rnorm(n, difference_score, sd), then t.test(differences, mu = 0)).\n\nset.seed(42)\n\nn <- 27\neffects <- seq(0, 1, 0.01)\ndraws <- 1e3\n\n\noutcomes <- \n  data.frame(\n    effect_size = NULL,\n    power = NULL\n  )\n\nfor (aneffect in effects) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:draws) {\n    \n    differences <- rnorm(n, aneffect)\n    \n    t <- t.test(differences, mu = 0)\n    \n    pvalues[i] <- t$p.value\n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        effect_size = aneffect,\n        power = sum(pvalues < 0.05) / length(pvalues)\n      )\n    )\n}\n\nwith(outcomes, plot(effect_size, power))\nabline(h = 0.95)\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nThe trick here is to change the way we think about simulations. At this point, we always iterated over sample sizes. But this time, sample size is fixed. So we need to iterate over effect sizes, run a t-test with the fixed sample size, and store the power for this effect size. We get all effect sizes (range 0 to 1 SD) with the seq command.\n\n# declare our variables and get all effect sizes\nn <- ?\neffects <- seq(?, ?, 0.01)\ndraws <- ?\n\n# somewhere to store our results\noutcomes <- \n  data.frame(\n    effect_size = NULL,\n    power = NULL\n  )\n\n# iterate over each effect\nfor (aneffect in effects) {\n  \n  # somewhere to store p-values for this effect size\n  pvalues <- NULL\n  \n  # now do a number of draws for this effect size to get its power\n  for (i in 1:draws) {\n    \n    # directly on the difference score\n    differences <- rnorm(n, ?)\n    \n    t <- t.test(differences, mu = 0)\n    \n    pvalues[?] <- t$p.value\n  }\n  \n  # then get power for this effect size like we always do\n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        effect_size = ?,\n        power = sum(? < 0.05) / ?\n      )\n    )\n}\n\nYou can plot with the following code.\n\nwith(outcomes, plot(effect_size, power))\nabline(h = 0.95)\n\n\n\n\n\n\n\nRun a sensitivity analysis on a paired samples t-test (one-tailed). You had 47 participants; the means were 56 and 60; the SDs were 16 and 13; the correlation between the measures was 0.4. Get sensitivity for three different alpha levels: c(0.005, 0.01, 0.05). As for effect sizes: Increase the effect size by 1 (on the raw scale) until you have 90% power (so use while). For each combination, do 1,000 simulations. Plot the results with ggplot like you did earlier.\n\nggplot(outcomes, aes(x = effect_size, y = power, color = as.factor(alpha))) + geom_line() + geom_hline(yintercept = 0.9) + theme_bw()\n\nYou could go about this with the following structure:\n\nfor (analpha in alphas) {\n  \n  while (power < 0.90) {\n    \n    for (i in 1:runs) {\n    }\n  }\n}\n\n\nn <- 32\nsd_control <- 16\nsd_treatment <- 13\ncorrelation <- 0.4\nalphas <- c(0.005, 0.01, 0.05)\nruns <- 1e3\n\nlibrary(MASS)\n\noutcomes <- \n  data.frame(\n    effect_size = NULL,\n    alpha = NULL,\n    power = NULL\n  )\n\nsigma <- \n  matrix(\n    c(\n      sd_control**2,\n      correlation * sd_control * sd_treatment,\n      correlation * sd_control * sd_treatment,\n      sd_treatment**2\n    ),\n    ncol = 2\n  )\n\nfor (analpha in alphas) {\n  \n  effect_size <- 0\n  power <- 0\n  \n  while (power < 0.90) {\n    \n    pvalues <- NULL\n    \n    means <- c(control = 56, treatment = 56 + effect_size)\n    \n    for (i in 1:runs) {\n      \n      d <- as.data.frame(\n        mvrnorm(\n          n,\n          means,\n          sigma\n        )\n      )\n      \n      t <- t.test(d$control, d$treatment, paired = TRUE, alternative = \"less\")\n      \n      pvalues[i] <- t$p.value\n    }\n    \n    power <- sum(pvalues < analpha) / length(pvalues)\n    \n    outcomes <- rbind(\n      outcomes,\n      data.frame(\n        effect_size = effect_size,\n        alpha = analpha,\n        power = power\n      )\n    )\n    \n    effect_size <- effect_size + 1\n  }\n}\n\nggplot(outcomes, aes(x = effect_size, y = power, color = as.factor(alpha))) + geom_line() + geom_hline(yintercept = 0.9) + theme_bw()\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nThis one can be tricky at first sight. Let’s first declare our variables.\n\nn <- ?\nsd_control <- ? # SD for control group\nsd_treatment <- ? # SD for treatment group\ncorrelation <- ? # correlation between the two scores\nalphas <- c(0.005, 0.01, 0.05) # our alphas we iterate over\nruns <- ? # number of draws\n  \n# somewhere to store the results\noutcomes <- \ndata.frame(\n  effect_size = NULL,\n  alpha = NULL,\n  power = NULL\n)\n\nOur variables are correlated, so we need to specify a variance-covariance matrix. The question is: When? Because the SDs are stable, we can do that right before the actual simulations. Then we iterate over each alpha level, followed by the power calculation.\n\n# \nsigma <- \n  matrix(\n    c(\n      sd_control**2,\n      correlation * sd_control * sd_treatment,\n      correlation * sd_control * sd_treatment,\n      sd_treatment**2\n    ),\n    ncol = 2\n  )\n\nfor (analpha in alphas) { # iterate over each alpha level\n  \n  # inititate zero effect size and power before we do simulations\n  effect_size <- 0\n  power <- 0\n  \n  # here is our while statement: we'll keep looking at power until we reach 90% for this alpha level\n  while (power < 0.90) {\n    \n    # store p-values\n    pvalues <- NULL\n    \n    # here is where we need to create the means (aka set the effect size)\n    means <- c(control = ?, treatment = ? + ?)\n    \n    # then we need to calculate power for this alpha level and effect size\n    for (i in 1:?) {\n      \n      # create correlated scores\n      d <- as.data.frame(\n        mvrnorm(\n          ?,\n          ?,\n          ?\n        )\n      )\n      \n      # run t-test\n      t <- t.test(d$control, d$treatment, paired = TRUE, alternative = \"less\")\n      \n      # store\n      pvalues[i] <- t$p.value\n    }\n    \n    # after the draws, we calculate power like we always do, except this time the alpha will change\n    power <- sum(pvalues < ?) / length(pvalues)\n    \n    # store it all\n    outcomes <- rbind(\n      outcomes,\n      data.frame(\n        effect_size = effect_size,\n        alpha = ?,\n        power = power\n      )\n    )\n    \n    # now we need to increase the effect size in case we didn't reach 90% power in the while statement above\n    effect_size <- effect_size + 1\n  }\n}\n\n\n\n\n\n\n\nDo the above again, but this time on the standardized scale. Verify your results with GPower. With this exercise, you can’t plot the raw effect size anymore; you’ll you need to plot the standardized one. However, remember the formula for Cohe’s \\(d\\) for correlated samples?\n\\(Cohen's \\ d = \\frac{M_{diff}-\\mu_o}{SD_{diff}}\\)\nThe standardized effect in this simulation will depend on the SD of the difference–and SDs will vary for each simulation. Therefore, you need to calculate the empirical Cohen’s \\(d\\) (i.e., calculate it with the formula above) for each run, store it, and then take the mean of all \\(d\\)s per combination. Ultimately, you only need to add a couple of lines (calculating and storing Cohen’s \\(d\\)) to the code from the exercise above.\n\nset.seed(42)\n\nn <- 32\nsd_control <- 16\nsd_treatment <- 13\ncorrelation <- 0.4\nalphas <- c(0.005, 0.01, 0.05)\nruns <- 1e3\n\nlibrary(MASS)\n\noutcomes <- \n  data.frame(\n    effect_size = NULL,\n    alpha = NULL,\n    power = NULL\n  )\n\nsigma <- \n  matrix(\n    c(\n      sd_control**2,\n      correlation * sd_control * sd_treatment,\n      correlation * sd_control * sd_treatment,\n      sd_treatment**2\n    ),\n    ncol = 2\n  )\n\nfor (analpha in alphas) {\n  \n  effect_size <- 0\n  power <- 0\n  \n  while (power < 0.90) {\n    \n    pvalues <- NULL\n    ds <- NULL\n    means <- c(control = 56, treatment = 56 + effect_size)\n    \n    for (i in 1:runs) {\n      \n      d <- as.data.frame(\n        mvrnorm(\n          n,\n          means,\n          sigma\n        )\n      )\n      \n      t <- t.test(d$control, d$treatment, paired = TRUE, alternative = \"less\")\n      \n      pvalues[i] <- t$p.value\n      \n      ds[i] <- (effect_size - 0) / sd(d$control-d$treatment)\n    }\n    \n    power <- sum(pvalues < analpha) / length(pvalues)\n    \n    outcomes <- rbind(\n      outcomes,\n      data.frame(\n        effect_size = mean(ds),\n        alpha = analpha,\n        power = power\n      )\n    )\n    \n    effect_size <- effect_size + 1\n  }\n}\n\nggplot(outcomes, aes(x = effect_size, y = power, color = as.factor(alpha))) + geom_line() + geom_hline(yintercept = 0.9) + theme_bw()\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nThis exercise just comes down to you calculating Cohen’s \\(d\\) for each run. So in the code from the previous exercise (somewhere in a loop; you need to find out yourself where), you add:\n\nd[i] <- (effect_size - null_hypothesis) / sd(control-treatment)\n\nThen, you store the mean effect size in our outcomes:\n\noutcomes <- rbind(\n      outcomes,\n      data.frame(\n        effect_size = mean(ds),\n        alpha = ?,\n        power = ?\n      )\n    )\n\nNote, you could also do the above by just sampling from a difference score, like we did earlier. But I wanted you to think about, once more, the differences between raw and standardized effect sizes."
  },
  {
    "objectID": "content/07-categorical-predictors/07-exercise.html",
    "href": "content/07-categorical-predictors/07-exercise.html",
    "title": "Exercise IV",
    "section": "",
    "text": "Once more, this set of exercises won’t introduce new technical skills: You got everything you need for the most common designs. Here, you’ll dive deeper into simulating multiple groups and getting a better feel for how these comparisons work.\n\n\nANOVAs are just linear models. There’s nothing wrong with using R’s aov command. In fact, it’s just a wrapper for lm. Type ?aov and convince yourself by reading the (short) description. There’s another reason why directly calling lm can be an advantage: efficiency. So far, we haven’t really cared much about the time our simulations take, but with more complicated designs (or more runs) it can easily take several hours, days, or even weeks.\nLet’s see whether lm gives us an advantage. For that, we’ll need the Sys.time() command. It does exactly what it says: tells you the time of your computer system. When we store the time before and after a simulation, we can check how long it took and compare different functions.\nRun the following to get familiar with how this works:\n\nt1 <- Sys.time()\n# wait a couple of seconds\nt2 <- Sys.time()\n\nt2-t1\n\nNow create a data frame with 4 (independent) groups whose means are c(4.2, 4.5, 4.6, 4.6) and their SDs are c(0.7, 0.8, 0.6, 0.5). The sample size is 240 (equal size in the groups). Check for an effect of condition on the outcome in 10,000 simulations. Do that once with aov and once with lm. For each type of model (lm or aov), clock the system time before and after the simulation with Sys.time. Are there differences? (Tip: Remember vectorization for rnorm to create the groups.)\n\nset.seed(42)\n\nmeans <- c(4.2, 4.5, 4.6, 4.6)\nsd <- c(0.7, 0.8, 0.6, 0.5)\nn <- 240\nruns <- 1e4\n\n# aov run\nt1_aov <- Sys.time()\n\nfor (i in 1:runs) {\n  \n  d <- \n    data.frame(\n      scores = rnorm(n, means, sd),\n      condition = rep(letters[1:4], n/4)\n    )\n  \n  aov(scores ~ condition, d)\n}\n\nt2_aov <- Sys.time()\n\n# lm run\nt1_lm <- Sys.time()\n\nfor (i in 1:runs) {\n  \n  d <- \n    data.frame(\n      scores = rnorm(n, means, sd),\n      condition = rep(letters[1:4], n/4)\n    )\n  \n  lm(scores ~ condition, d)\n}\n\nt2_lm <- Sys.time()\n\naov_time <- t2_aov-t1_aov\nlm_time <- t2_lm-t1_lm\n\naov_time-lm_time\n\nTime difference of 1.797944 secs\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nDid you remember the vectorization? It just means that rnorm will go through vectors in order, such as:\n\nmeans <- c(4.2, 4.5, 4.6, 4.6)\nsd <- c(0.7, 0.8, 0.6, 0.5)\nn <- 240\nruns <- 1e4\n\nrnorm(n, means, sd\n\nNow all you need to do is put that into a loop (no need to store the results). Just remember to save the time right before and after the loop. Do that once for aov and once for lm; then compare the time differences.\n\nt1_aov <- ?\n\nfor (i in 1:?) {\n  \n  d <- \n    data.frame(\n      scores = ?,\n      condition = ?\n    )\n  \n  aov(scores ~ condition, d)\n}\n\nt2_aov <- ?\n  \nt2_aov - t1_aov\n\n\n\n\n\n\n\nTime to run a power analysis. You have 3 independent groups. Their means are c(5, 5.2, 5.6); the SD is constant: 1. Your minimum sample size is 120; your maximum sample size is 300. How large does your sample have to be for 95% power? Do 1,000 per run. Go in steps of 6 for the sample size (so increase 2 per group). (Tip: You can use seq and remember this link). This simulation can take a minute (well, several actually).\n\nmeans <- c(5, 5.2, 5.6)\nsd <- 1\nsizes <- seq(120, 300, 6)\nruns <- 1e3\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nfor (n in sizes) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    d <- \n      data.frame(\n        scores = rnorm(n, means, sd),\n        condition = rep(letters[1:3], n/3)\n      )\n    \n    m <- summary(lm(scores ~ condition, d))\n    pvalues[i] <- broom::glance(m)$p.value\n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        power = sum(pvalues < 0.05) / length(pvalues)\n      )\n    )\n}\n\nplot(outcomes$sample_size, outcomes$power)\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nAt this point, you’ll know what comes here. We first declare our parameters:\n\nmeans <- c(5, 5.2, 5.6)\nsd <- 1\nsizes <- seq(?, ?, ?) # the steps we go up in sample size\nruns <- ?\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nThen we do two loops: On the first level, we iterate over the different sample sizes; on the second level, we iterate over the number of runs. There’s really nothing new here; you can copy-paste code from t-tests and just add another group.\n\n# iterate over the sample sizes\nfor (? in ?) {\n  \n  # set up storage for the p-values\n  \n  # then do the test as many timmes as we decided to do runs\n  for (? in 1:?) {\n    \n    d <- \n      data.frame(\n        scores = rnorm(?, ?, ?),\n        condition = rep(?, ?)\n      )\n    \n    m <- summary(lm(scores ~ condition, d))\n    \n    ? <- broom::glance(m)$p.value\n  }\n  \n  # now calculate power and add it to our data frame as always\n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = ?,\n        power = ?\n      )\n    )\n}\n\nYou can plot the power curve with plot(outcomes$sample_size, outcomes$power).\n\n\n\n\n\n\nDo the above again, but this time there’s no difference between the first two groups: c(5, 5, 5.6). What’s the effect on power if there’s a null effect for one contrast? Go in steps of 12 to speed it up.\n\nmeans <- c(5, 5, 5.6)\nsd <- 1\nsizes <- seq(120, 300, 12)\nruns <- 1e3\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nfor (n in sizes) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    d <- \n      data.frame(\n        scores = rnorm(n, means, sd),\n        condition = rep(letters[1:3], n/3)\n      )\n    \n    m <- summary(lm(scores ~ condition, d))\n    pvalues[i] <- broom::glance(m)$p.value\n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        power = sum(pvalues < 0.05) / length(pvalues)\n      )\n    )\n}\n\nplot(outcomes$sample_size, outcomes$power)\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nAll you need to do is change one number, namely the mean for the second group. The rest of the code is identical to the previous exercise.\n\n\n\n\n\n\nLet’s try to work on the standardized scale. You again have 3 groups. Their SDs are c(2, 2.2, 1.8). Choose means such that the Cohen’s \\(d\\)s are as follows:\n\nGroup 1 vs. Group 2: \\(d\\) = 0.2\nGroup 1 vs. Group 3: \\(d\\) = 0.6\nGroup 2 vs. Group 3: ?\n\nRemember the formulate for Cohen’s \\(d\\): \\(d = \\frac{M_1-M_2}{\\sqrt{\\frac{(sd_1^2 + sd_2^2)}{2}}}\\)\nHave a look at earlier exercises when you dealt with Cohen’s \\(d\\). It might (wink, wink) help to get the pooled SD for each group comparison. Once you have the pooled SD, the absolute means don’t matter as much. You can start with a random mean for the control group, then have the mean for group 2 be the control group mean plus 0.2 pooled SDs (so that we have our Cohen’s \\(d\\)). Have a look at earlier exercises on how to calculate the pooled SD. Create a data frame and calculate power for samples ranging from 20 per group to 60 per group. Go in steps of 3 per group. Do 500 runs for each combination. Run effectsize::cohens_d() for each run (and contrast). Store not only the p-values to calculate power, but also the Cohen’s \\(d\\) for each contrast. Plot the power first. Then take the mean of Cohen’s \\(d\\) for each contrast to check whether you actually got the correct numbers.\n\npooled_sd <- \n  function(sd1, sd2){\n    pooled_sd <- sqrt((sd1**2 + sd2**2) / 2)\n    \n    return(pooled_sd)\n  }\n\nsds <- c(2, 2.2, 1.8)\n\n# pooled sd for first two combos (the third is implied when we determine the means)\ng1_g2_sd <- pooled_sd(sds[1], sds[2])\ng1_g3_sd <- pooled_sd(sds[1], sds[3])\n\n# choose random mean for first group and other groups as proportion of the pooled SD. the last one, logically, has to be 0.4\nm1 <- 10\nm2 <- m1 + 0.2*g1_g2_sd\nm3 <- m1 + 0.6*g1_g3_sd\n\nmeans <- c(m1, m2, m3)\nsizes <- seq(20*3, 60*3, 3*3)\nruns <- 500\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL,\n    d12 = NULL,\n    d13 = NULL,\n    d23 = NULL\n  )\n\nfor (n in sizes) {\n  \n  pvalues <- NULL\n  d12 <- NULL\n  d13 <- NULL\n  d23 <- NULL\n  \n  for (i in 1:runs) {\n    d <- \n      data.frame(\n        scores = rnorm(n, means, sds),\n        condition = rep(letters[1:3], n/3)\n      )\n    \n    m <- summary(lm(scores ~ condition, d))\n    pvalues[i] <- broom::glance(m)$p.value\n    \n    # get cohen's for each contrast\n    d12[i] <- effectsize::cohens_d(d$scores[d$condition==letters[1]], d$scores[d$condition==letters[2]])$Cohens_d\n    d13[i] <- effectsize::cohens_d(d$scores[d$condition==letters[1]], d$scores[d$condition==letters[3]])$Cohens_d\n    d23[i] <- effectsize::cohens_d(d$scores[d$condition==letters[2]], d$scores[d$condition==letters[3]])$Cohens_d\n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        power = sum(pvalues < 0.05) / length(pvalues),\n        d12 = mean(d12),\n        d13 = mean(d13),\n        d23 = mean(d23)\n      )\n    )\n}\n\nplot(outcomes$sample_size, outcomes$power)\n\n\n\nmean(outcomes$d12);mean(outcomes$d13);mean(outcomes$d23)\n\n[1] -0.2022801\n\n\n[1] -0.6103061\n\n\n[1] -0.3666429\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nAlright, first we need a function to get the pooled SD. Afterwards, we get the pooled SD for each contrast.\n\npooled_sd <- \n  function(sd1, sd2){\n    pooled_sd <- ?\n    \n    return(pooled_sd)\n  }\n\ncontrast1 <- pooled_sd(2, 2.2)\ncontrast2 <- pooled_sd(2.2, 3)\n\nThat’s really all we need. Now we can pick a random mean and construct the mean of the other groups in relation to that mean plus however many pooled SDs we specified (e.g., 0.2, 0.6).\n\n# choose random mean for first group and other groups as proportion of the pooled SD. the last one, logically, has to be 0.4\nmean1 <- ?\nmean2 <- m1 + 0.2*?\nmean3 <- m1 + 0.6*?\n\n# store the means\nmeans <- c(mean1, mean2, mean3)\n\n# declare the other variables\nsizes <- seq(?, ?, ?)\nruns <- ?\n  \n# somewhere to store the results\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL,\n    d12 = NULL, # cohen's D for first contrast\n    d13 = NULL, # second contrast\n    d23 = NULL # third\n  )\n\nThat’s it. The rest is just lots of typing.\n\n# iterate over sample sizes\nfor (? in sizes) {\n  \n  # store p-values and Cohen's ds\n  pvalues <- NULL\n  d12 <- NULL\n  d13 <- NULL\n  d23 <- NULL\n  \n  for (i in 1:runs) {\n    d <- \n      data.frame(\n        scores = ?,\n        condition = ?\n      )\n    \n    m <- summary(lm(? ~ ?, ?))\n    pvalues[?] <- broom::glance(?)$?\n    \n    # get cohen's for each contrast\n    d12[i] <- effectsize::cohens_d(?, ?)$Cohens_d\n    # etc.\n  }\n  \n  # store it all\n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = ?,\n        power = ?,\n        d12 = mean(d12),\n        d13 = ?,\n        d23 = ?\n      )\n    )\n}\n\n\n\n\n\n\n\nSo far, we’ve worked with dummy coding for our explanatory factor. That presumes that we’re interested in those contrasts where the second and third condition are compared to the first. Sometimes, however, we’re not interested in these comparisons and would rather like to know the difference between our conditions and the grand mean. In that case, we should use sum to zero coding. Take the data set below:\n\nset.seed(42)\nn <- 100\nd <- \n  data.frame(\n    condition = as.factor(rep(letters[1:3], times = n)),\n    scores = rnorm(n*3, c(100, 150, 200), c(7, 10, 12))\n  )\n\nLet’s have a look at the contrasts:\n\ncontrasts(d$condition)\n\n  b c\na 0 0\nb 1 0\nc 0 1\n\n\nIt’s the familiar dummy coding. This means, going row by row, that, to get the mean for a, both b and c estimates will be zero. When everything is zero, that’s our intercept. In other words, the intercept will be the mean of condition a. To get b, we take the intercept plus 1 times the b estimate (mean of condition b in comparison to a), and to get c, we take the intercept plus 1 time the estimate for c (aka the mean of c in comparison to a).\nLet’s have a look at contrast coding instead:\n\ncontrasts(d$condition) <- contr.sum\ncontrasts(d$condition)\n\n  [,1] [,2]\na    1    0\nb    0    1\nc   -1   -1\n\n\nNow the intercept is our grand mean (the overall mean of the outcome), the first contrast compares the score in condition a against the grand mean and the second contrast compares the mean of condition b against the grand mean. To get c, we need to take the intercept take the estimate of a times -1 plus the estimate of c times -1 (so grand mean minus a and minus b).\n\nsummary(lm(scores~condition, d))\n\n\nCall:\nlm(formula = scores ~ condition, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-32.364  -6.417  -0.009   6.081  26.953 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 149.8348     0.5700 262.872   <2e-16 ***\ncondition1  -50.0140     0.8061 -62.045   <2e-16 ***\ncondition2   -0.6376     0.8061  -0.791     0.43    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 9.873 on 297 degrees of freedom\nMultiple R-squared:  0.946, Adjusted R-squared:  0.9456 \nF-statistic:  2600 on 2 and 297 DF,  p-value: < 2.2e-16\n\n\nIf we want a different comparison, we can reorder the factor levels:\n\nd$condition <- factor(d$condition, levels = c(\"b\", \"c\", \"a\")) \ncontrasts(d$condition) <- contr.sum\ncontrasts(d$condition)\n\n  [,1] [,2]\nb    1    0\nc    0    1\na   -1   -1\n\n\nNow the first contrasts compares condition b against the grand mean, and the second contrast compares condition c against the grand mean.\nFor later sessions on interactions, we’ll rely on this kind of sum-to-zero coding (also called effect coding), so let’s get familiar with it here. Create four groups: a placebo group, a low dose group, a medium dose group, and a high dose group.\nYou want to know whether the low dose, the medium dose, and the high dose are significantly different from the grand mean. Simulate a data set, set the contrasts, and run a linear model. Choose mean values as you like, but make sure you can find them again in the model output. Speaking of the model output: How would you get the mean of the placebo group now? How would you get the mean of any group? (Tip: Look at the contrasts (aka the estimates from the model), take the grand mean, and apply the contrasts to each estimate.)\n\nn <- 50\nd <- \n  data.frame(\n    condition = factor(rep(c(\"placebo\", \"low dose\", \"medium dose\", \"high dose\"), times = n)),\n    scores = rnorm(n*4, c(10, 20, 30, 40), 2)\n  )\n\nd$condition <- factor(d$condition, levels = c(\"low dose\", \"medium dose\", \"high dose\", \"placebo\"))\ncontrasts(d$condition) <- contr.sum\ncontrasts(d$condition)\n\n            [,1] [,2] [,3]\nlow dose       1    0    0\nmedium dose    0    1    0\nhigh dose      0    0    1\nplacebo       -1   -1   -1\n\nsummary(lm(scores ~ condition, d))\n\n\nCall:\nlm(formula = scores ~ condition, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-4.3991 -1.2198 -0.1447  1.3646  5.5612 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  24.9151     0.1334  186.74   <2e-16 ***\ncondition1   -5.4787     0.2311  -23.71   <2e-16 ***\ncondition2    5.4555     0.2311   23.61   <2e-16 ***\ncondition3   14.9170     0.2311   64.55   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.887 on 196 degrees of freedom\nMultiple R-squared:  0.9731,    Adjusted R-squared:  0.9726 \nF-statistic:  2360 on 3 and 196 DF,  p-value: < 2.2e-16\n\n# get placebo mean\ncoefs <- coef(lm(scores ~ condition, d))\ncoefs[1] + sum(coefs[-1] * contrasts(d$condition)[\"placebo\", ])\n\n(Intercept) \n   10.02139 \n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nThere’s nothing complicated here. We’re only doing one run. So first you create a data frame and choose a sample size and some means (feel free to choose any):\n\nn <- 50\nd <- \n  data.frame(\n    condition = factor(rep(c(\"placebo\", \"low dose\", \"medium dose\", \"high dose\"), times = n)),\n    scores = rnorm(n*4, c(?, ?, ?, ?), 2)\n  )\n\nThen let’s code the contrasts:\n\nd$condition <- factor(d$condition, levels = c(\"low dose\", \"medium dose\", \"high dose\", \"placebo\")) # rearrange the levels\ncontrasts(d$condition) <- contr.sum # apply sum-to-zero contrasts\ncontrasts(d$condition) # have a look\n\nThen all that’s left is to run an lm model. To get the placebo mean you need to take the grand mean and then subtract each estimate from it (remember that minus a negative estimate means adding).\n\n\n\n\n\n\nYou plan an experiment where you want to test the effect of framing on agreeing with a message. In your experiment people read a text with either neutral, positive, or negative framing. Each person reads each text (so we have a within-subjects design) and rates their level of agreeing on a 7-point Likert-scale.\nYou determine that the smallest contrast you care about is 0.25 points between the neutral and the negative condition and between the neutral and the positive condition. You’ve found in previous testing that only such a difference on agreeing actually has an effect on behavior. That’s your SESOI.\nYou decide that the midpoint of the scale is a decent starting point for the neutral condition. The negative condition should be 0.25 points lower and the positive condition 0.25 points higher. You generally expect that the variation should be somewhere around 1 Likert-point; this way, most scores are within -2 and +2 from the mean. However, you know that negative framing usually increases variation, so you’ll set the SD for that score 30% higher than that of the other two conditions.\nThe experiment takes place within 20 minutes, so you expect a decent amount of consistency in answers per participant: a person who generally agrees more should also agree more to all conditions. In other words, you set the correlation between scores to 0.6.\nLast, you determined that you’re early in the research process, so you definitely don’t want to miss any effects (aka commit a Type II error). You also consider that a false positive won’t have any negative consequences. Therefore, you set your \\(\\alpha\\) to 0.10.\nYou can afford to collect 100 participants in total (that’s where you’ll stop with the simulation). Simulate power (500 runs each); go in steps of 5 (start with 20). How many participants do you need for 95% power? Will 100 even get you there? Use contrast coding.\nSeveral tips:\n\nFor the variance-covariance matrix, you’ll need to get different covariances based on the groups\nRemember the data format the data have to be in. You’ll need to transform them from wide to long format. Have a look at the slides or good old Google\nYou might have to Google how to get the p-value out of aov (one method that worked for me was unlist(model)[\"Error: Within.Pr(>F)1\"])\n\n\nlibrary(MASS)\nlibrary(tidyr)\n\nmeans <- c(neutral = 4, negative = 3.75, positive = 4.25)\nsd_neutral_po <- 1\nsd_negative <- 1.3\ncorrelation <- 0.6\nsizes <- seq(10, 100, 5)\nruns <- 500\nalpha <- 0.10\n\n# covariance for negative with the two others is identical because the sd for the other two is identical\ncov_negative <- correlation * sd_neutral_po * sd_negative\ncov_other <- correlation * sd_neutral_po * sd_neutral_po\n\nour_matrix <- matrix(\n  c(\n    sd_neutral_po**2, cov_negative, cov_other,\n    cov_negative, sd_negative**2, cov_negative,\n    cov_other, cov_negative, sd_neutral_po**2\n  ),\n  ncol = 3\n)\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nfor (n in sizes) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    d <- mvrnorm(\n      n,\n      means,\n      our_matrix\n    )\n    \n    d <- as.data.frame(d)\n    d$id <- factor(1:n)\n    \n    d <- pivot_longer(\n      d,\n      cols = -id,\n      values_to = \"scores\",\n      names_to = \"condition\"\n    )\n    \n    d$condition <- as.factor(d$condition)\n    \n    contrasts(d$condition) <- contr.sum\n    \n    m <- summary(aov(scores ~ condition + Error(id), d))\n    \n    pvalues[i] <- unlist(m)[\"Error: Within.Pr(>F)1\"]\n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        power = sum(pvalues < alpha) / length(pvalues)\n      )\n    )\n}\n\nplot(outcomes$sample_size, outcomes$power)\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nThe complication here really comes from the variance-covariance matrix. This time, you can’t use the same covariance between groups because the variances differ per group. But before we get there, let’s first declare our variables. That should help:\n\nlibrary(MASS)\nlibrary(tidyr)\n\nmeans <- c(neutral = ?, negative = ?, positive = ?)\nsd_neutral_po <- ?  # the SD for the neutral and positive conditions\nsd_negative <- ? # the SD for the negative conditions\ncorrelation <- ? # the correlation between conditions\nsizes <- seq(?, ?, ?) # in what steps will we go up in sample size?\nruns <- ?\nalpha <- ? # our Type I error rate\n\nOnce we have all of that, we need to construct the variance-covariance matrix. Because negative condition is different from the positive and neutral conditions, we’ll need to create a covariance especially covariances between negative and the other two, as well as a covariance between neutral and positive\n\n# covariance for negative with the two others is identical because the sd for the other two is identical\ncov_negative <- correlation * ? * ? # correlation times SD for neutral/positive times SD for negative\ncov_other <- correlation * ? * ? # correlation times SD for neutral/positive times SD for neutral/positive again\n\n# then we create the matrix\nour_matrix <- matrix(\n  c(\n    sd_neutral_po**2, cov_negative, cov_other,\n    cov_negative, sd_negative**2, cov_negative,\n    cov_other, cov_negative, sd_neutral_po**2\n  ),\n  ncol = 3\n)\n\n# somewhere to store our outcomes\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nNow that we have out variance-covariance matrix, we can go to the actual simulations. We iterate over sample sizes, create normal scores, transform those into the long format (see the slides for code), and run the Repeated-Measures ANOVA.\n\n# iterate over sample sizes\nfor (? in ?) {\n  \n  # somewhere to store the p-values\n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    d <- mvrnorm(\n      ?,\n      ?,\n      ?\n    )\n    \n    # turn d into the long format\n    \n    # then assign sum-to-zero contrasts\n    contrasts(d$condition) <- contr.sum\n    \n    # get the condition effect\n    m <- summary(aov(scores ~ condition + Error(id), d))\n    \n    # and store it\n    pvalues[i] <- unlist(m)[\"Error: Within.Pr(>F)1\"]\n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = ?,\n        power = ?\n      )\n    )\n}\n\n\n\n\nYou can plot the results with plot(outcomes$sample_size, outcomes$power).\n\n\n\nThe previous study worked. Nice. Now you want to know whether positive framing scales: Can you reduce positivity and agreeing reduces proportionally? You run a follow-up experiment where you now have a neutral condition, a low positivity condition (half the positivity of your previous positive condition), and a high positivity condition (the positivity of your original positive condition). Your effect size remains: 0.25 for the comparison between neutral and high positivity (the original positive condition). You expect, therefore, that half the positivity should lead to half the effect size: 0.125. That’s your new SESOI.\nThe previous study also showed that you weren’t too far off with an SD of 1, but this time you’ll use 0.8 for all scores.\nHowever, you expect that, because the two positivity conditions are so similar, they’ll correlate higher than the correlation between neutral and positivity. So you expect a correlation between neutral and either positivity condition to be 0.3, but 0.6 between the two positivity conditions.\nBecause your department was so happy with how well you designed the previous study, they gave you more money. Now you can collect a maximum of 500 participants. Last time, the power simulation took quite some time, so this time you want to stop whenever you reach 95% power (while). Also, because this time you want to be more stringent, you set \\(\\alpha\\) to 0.01.\nDo 500 runs, start with 60 people and go in in steps of 10. Use treatment contrasts. This can take a while.\n\nlibrary(MASS)\nlibrary(tidyr)\n\nset.seed(42)\n\nsesoi <- 0.125\nmeans <- c(neutral = 4, low = 4+sesoi, high = 4+2*sesoi)\nsd <- 0.8\ncor_positive <- 0.6\ncor_neutral <- 0.3\nruns <- 500\nalpha <- 0.01\nmax_n <- 500\nsteps <- 10\n\n# covariance for neutral with the two others is identical because the SD and correlation for the other two is identical\ncov_positive <- cor_positive * sd * sd\ncov_neutral <- cor_neutral * sd * sd\n\nour_matrix <- matrix(\n  c(\n    sd**2, cov_neutral, cov_neutral,\n    cov_neutral, sd**2, cov_positive,\n    cov_neutral, cov_positive, sd**2\n  ),\n  ncol = 3\n)\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\npower <- 0\nn <- 60\n\nwhile (power < 0.95 & n <= max_n) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    \n    d <- mvrnorm(\n      n,\n      means,\n      our_matrix\n    )\n    \n    d <- as.data.frame(d)\n    \n    d$id <- factor(1:n)\n    \n    d <- pivot_longer(\n      d,\n      cols = -id,\n      values_to = \"scores\",\n      names_to = \"condition\"\n    )\n    \n    d$condition <- as.factor(d$condition)\n    \n    m <- summary(aov(scores ~ condition + Error(id), d))\n    \n    pvalues[i] <- unlist(m)[\"Error: Within.Pr(>F)1\"]\n  }\n  \n  power <- sum(pvalues < alpha) / length(pvalues)\n  \n  outcomes <- \n  rbind(\n    outcomes,\n    data.frame(\n      sample_size = n,\n      power = power\n    )\n  )\n  \n  n <- n + steps\n}\n  \nplot(outcomes$sample_size, outcomes$power)\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nOkay, nothing new here, just a lot. Let’s do the easy stuff first: Declare variables.\n\nlibrary(MASS)\nlibrary(tidyr)\n\nsesoi <- ?\nmeans <- c(neutral = 4, low = 4+sesoi, high = 4+2*sesoi)\nsd <- ?\ncor_positive <- ? # the correlation between positive conditions\ncor_neutral <- ? # the correlation between neutral and either positive condition\nruns <- ?\nalpha <- ?\nmax_n <- ? # maximum number of participants\nsteps <- ? # in what steps are you going up?\n\nOkay, this time when constructing the variance-covariance matrix, the SD will be the same, but the correlation will differ. Therefore, we need to create a covariance for neutral with either positivity condition\n\n# covariance for negative with the two others is identical because the sd for the other two is identical\ncov_positive <- ? * sd * sd # covariance of positive with the other positive\ncov_neutral <- ? * sd * sd # covariance of neutral with either positive\n\nour_matrix <- matrix(\n  c(\n    sd**2, cov_neutral, cov_neutral,\n    cov_neutral, sd**2, cov_positive,\n    cov_neutral, cov_positive, sd**2\n  ),\n  ncol = 3\n)\n\n# somewhere to store our outcomes\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nNow that we have out variance-covariance matrix, we can go to the actual simulations once more. We continue running 500 simulations for a given sample size step unless we reach 95% power or our maximum number of participants.\n\npower <- ? # starting power\nn <- ? # starting sample size\n\nwhile (power < ? & n <= ?) {\n  \n  # store p-values somewhere\n  pvalues <- NULL\n  \n  # do the 500 simulations for this sample size\n  for (i in 1:runs) {\n    \n    d <- mvrnorm(\n      ?,\n      ?,\n      ?\n    )\n    \n    # transform to long form\n    d <- as.data.frame(d)\n    \n    d$id <- factor(1:n)\n    \n    d <- pivot_longer(\n      d,\n      cols = -id,\n      values_to = \"scores\",\n      names_to = \"condition\"\n    )\n    \n    d$condition <- as.factor(d$condition)\n    \n    # run model\n    m <- summary(aov(? ~ ? + Error(?), ?))\n    \n    # get p-values\n  }\n  \n  power <- ?\n  \n  # add to results\n  outcomes <- \n  rbind(\n    outcomes,\n    data.frame(\n      sample_size = n,\n      power = power\n    )\n  )\n  \n  # increase sample size\n  n <- ?\n}\n\n\n\n\n\n\n\nUsually when you run an ANOVA, you specify your contrasts before-hand so you know which groups you want to compare beyond just the overall effect of your condition. Let’s take the data set below. The overall effect of condition is significant:\n\nlibrary(MASS)\nlibrary(tidyr)\n\nset.seed(42)\n\nmeans <- c(neutral = 10, low = 12, high = 14)\nsd <- 8\ncorrelation <- 0.6\nn <- 80\n\ncovariance <- correlation * sd * sd\n\nsigma <- matrix(\n  c(\n    sd**2, covariance, covariance,\n    covariance, sd**2, covariance,\n    covariance, covariance, sd**2\n  ),\n  ncol = 3\n)\n\nd <- data.frame(\n  mvrnorm(\n    n,\n    means,\n    sigma\n  )\n)\n\nd$id <- factor(1:n)\n    \nd <- pivot_longer(\n  d,\n  cols = -id,\n  values_to = \"scores\",\n  names_to = \"condition\"\n)\n\nd$condition <- as.factor(d$condition)\n\nm <- aov(scores ~ condition + Error(id), d)\n\nsummary(m)\n\n\nError: id\n          Df Sum Sq Mean Sq F value Pr(>F)\nResiduals 79  12853   162.7               \n\nError: Within\n           Df Sum Sq Mean Sq F value   Pr(>F)    \ncondition   2    606  303.22   14.23 2.07e-06 ***\nResiduals 158   3366   21.31                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLet’s inspect the post-hoc comparisons between the groups with the emmeans package. Here, not every comparison is significant.\n\nposthocs <- emmeans::emmeans(m, ~ condition)\nposthocs\n\n condition emmean    SE  df lower.CL upper.CL\n high       13.55 0.925 122     11.7     15.4\n low        12.30 0.925 122     10.5     14.1\n neutral     9.73 0.925 122      7.9     11.6\n\nWarning: EMMs are biased unless design is perfectly balanced \nConfidence level used: 0.95 \n\npairs(posthocs)\n\n contrast       estimate   SE  df t.ratio p.value\n high - low         1.25 0.73 158   1.713  0.2035\n high - neutral     3.82 0.73 158   5.232  <.0001\n low - neutral      2.57 0.73 158   3.519  0.0016\n\nP value adjustment: tukey method for comparing a family of 3 estimates \n\n\nRun a power simulation that finds the sample size for which we have 90% power for the smallest post-hoc contrast (aka the largest of the three p-values.) Go about it as you think is best, but save both the main effect p-value and the largest p-value of the posthoc contrasts to be able to compare power. In effect, you’re powering for a t-test, which only emphasizes that you should work on the raw scale and specify each comparison. (Tip: summary turns the pairs(postdocs) into a data frame.)\n\nmeans <- c(neutral = 10, low = 12, high = 14)\nsd <- 8\ncorrelation <- 0.6\nn <- 20\ndraws <- 1e3\n\ncovariance <- correlation * sd * sd\n\nsigma <- matrix(\n  c(\n    sd**2, covariance, covariance,\n    covariance, sd**2, covariance,\n    covariance, covariance, sd**2\n  ),\n  ncol = 3\n)\n\noutcomes <- data.frame(\n  sample_size = NULL,\n  power = NULL,\n  power_posthoc = NULL\n)\n\npower_posthoc <- 0\n\nwhile (power_posthoc < .90) {\n  \n  pvalues_posthoc <- NULL\n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    \n    d <- data.frame(\n      mvrnorm(\n        n,\n        means,\n        sigma\n      )\n    )\n    \n    d$id <- factor(1:n)\n        \n    d <- pivot_longer(\n      d,\n      cols = -id,\n      values_to = \"scores\",\n      names_to = \"condition\"\n    )\n    \n    d$condition <- as.factor(d$condition)\n    \n    m <- aov(scores ~ condition + Error(id), d)\n    \n    pvalues[i] <- unlist(summary(m))[\"Error: Within.Pr(>F)1\"]\n    \n    posthocs <- suppressMessages(emmeans::emmeans(m, ~ condition))\n    outputs <- summary(pairs(posthocs))\n    \n    pvalues_posthoc[i] <- max(outputs$p.value)\n  }\n  \n  power <- sum(pvalues < 0.05) / length(pvalues)\n  power_posthoc <- sum(pvalues_posthoc < 0.05) / length(pvalues_posthoc)\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        power = power,\n        power_posthoc = power_posthoc\n      )\n    )\n  \n  n <- n + 10\n}\n\nlibrary(ggplot2)\n\noutcomes %>% \n  pivot_longer(\n    cols = starts_with(\"power\"),\n    names_to = \"Power Type\"\n  ) %>% \n  ggplot(\n    aes(x = sample_size, y = value, color = `Power Type`)\n  ) +\n  geom_line() + theme_bw()\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nOkay, nothing new here, just a lot. Let’s do the easy stuff first: Declare variables.\n\nlibrary(MASS)\nlibrary(tidyr)\n\nsesoi <- ?\nmeans <- c(neutral = 4, low = 4+sesoi, high = 4+2*sesoi)\nsd <- ?\ncor_positive <- ? # the correlation between positive conditions\ncor_neutral <- ? # the correlation between neutral and either positive condition\nruns <- ?\nalpha <- ?\nmax_n <- ? # maximum number of participants\nsteps <- ? # in what steps are you going up?\n\nThis time when constructing the variance-covariance matrix, the SD will be the same, but the correlation will differ. Therefore, we need to create a covariance for neutral with either positivity condition\n\n# covariance for negative with the two others is identical because the sd for the other two is identical\ncov_positive <- ? * sd * sd # covariance of positive with the other positive\ncov_neutral <- ? * sd * sd # covariance of neutral with either positive\n\nour_matrix <- matrix(\n  c(\n    sd**2, cov_neutral, cov_neutral,\n    cov_neutral, sd**2, cov_positive,\n    cov_neutral, cov_positive, sd**2\n  ),\n  ncol = 3\n)\n\n# somewhere to store our outcomes\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nNow that we have out variance-covariance matrix, we can go to the actual simulations once more. We continue running 500 simulations for a given sample size step unless we reach 95% power or our maximum number of participants.\n\npower <- ? # starting power\nn <- ? # starting sample size\n\nwhile (power < ? & n <= ?) {\n  \n  # store p-values somewhere\n  pvalues <- NULL\n  \n  # do the 500 simulations for this sample size\n  for (i in 1:runs) {\n    \n    d <- mvrnorm(\n      ?,\n      ?,\n      ?\n    )\n    \n    # transform to long form\n    d <- as.data.frame(d)\n    \n    d$id <- factor(1:n)\n    \n    d <- pivot_longer(\n      d,\n      cols = -id,\n      values_to = \"scores\",\n      names_to = \"condition\"\n    )\n    \n    d$condition <- as.factor(d$condition)\n    \n    # run model\n    m <- summary(aov(? ~ ? + Error(?), ?))\n    \n    # get p-values\n    pvalues[i] <- ?\n    \n      # get the posthoc comparisons\n    posthocs <- suppressMessages(emmeans::emmeans(m, ~ condition))\n    \n    # turn into a data frame we can access\n    outputs <- summary(pairs(posthocs))\n    \n    # then get the largest p-value of the contrasts\n    pvalues_posthoc[i] <- ?\n  }\n  \n  power <- ?\n  \n  # add to results\n  outcomes <- \n  rbind(\n    outcomes,\n    data.frame(\n      sample_size = n,\n      power = power\n    )\n  )\n  \n  # increase sample size for the next round of the while statement\n  n <- ?\n}\n\n\n\n\n\n\n\nRather than doing posthoc comparisons between each group, we can rely on planned contrasts. If that’s completely new to you, have a read here. The blog post does a really nice job explaining planned contrasts.\nIn effect, rather than relying on sum to zero or treatment coding, we create our own comparisons. Let’s say we want to compare neutral to both low positivity and high positivity (as a general test that framing works). Then, our second contrast compares the two framing conditions. For simplicity, let’s go three independent groups with means of c(4, 4.3, 4.4) and a common SD of 1.1. Custom contrasts will allow us to directly make the two comparisons we’re interested in like such:\n\nd <- data.frame(\n  scores = rnorm(150, c(4, 4.3, 4.4), 1.1),\n  condition = factor(rep(c(\"neutral\", \"low\", \"high\"), 50))\n)\n\ncontrasts(d$condition)\n\n        low neutral\nhigh      0       0\nlow       1       0\nneutral   0       1\n\nhighlow_vs_neutral <- c(1, 1, -2)\nhigh_vs_low <- c(1, -1, 0)\n\ncontrasts(d$condition) <- cbind(highlow_vs_neutral, high_vs_low)\ncontrasts(d$condition)\n\n        highlow_vs_neutral high_vs_low\nhigh                     1           1\nlow                      1          -1\nneutral                 -2           0\n\nsummary.lm(aov(scores ~ condition, d))\n\n\nCall:\naov(formula = scores ~ condition, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.97137 -0.80870 -0.03995  0.77749  2.70410 \n\nCoefficients:\n                            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)                  4.26182    0.08776  48.562   <2e-16 ***\nconditionhighlow_vs_neutral  0.13517    0.06206   2.178    0.031 *  \nconditionhigh_vs_low         0.09555    0.10748   0.889    0.375    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.075 on 147 degrees of freedom\nMultiple R-squared:  0.03629,   Adjusted R-squared:  0.02318 \nF-statistic: 2.768 on 2 and 147 DF,  p-value: 0.06609\n\n\nRun a power simulation where you first run standard treatment contrasts. Then, run posthoc tests (emmeans::emmeans) on that test. For simplicity, focus on the comparison of high vs. low. Select the p-value for that posthoc contrast. In the same run, change the contrasts to custom from above, run the model again, and extract the p-value for the high vs. low comparison. Do that for samples ranging from 50 to 200 in steps of 10. Do 200 runs to save a bit of time, then plot the two types of power against each other. Which approach is more “powerful”? No tips here–see this as a little challenge.\n\nset.seed(42)\n\nmeans <- c(4., 4.3, 4.6)\nsd <- 1.1\nruns <- 200\nsizes <- seq(50, 200, 10)\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    type = NULL,\n    power = NULL\n  )\n\nfor (n in sizes) {\n  pvalues_treatment <- NULL\n  pvalues_contrasts <- NULL\n  \n  for (i in 1:runs) {\n    d <- data.frame(\n      scores = rnorm(n*3, means, 1.1),\n      condition = factor(rep(c(\"neutral\", \"low\", \"high\"), n))\n    )\n    posthocs <- suppressMessages(emmeans::emmeans(lm(scores~condition, d), ~ condition))\n    outputs <- summary(pairs(posthocs))\n    \n    pvalues_treatment[i] <- outputs[1,6]\n    \n    highlow_vs_neutral <- c(1, 1, -2)\n    high_vs_low <- c(1, -1, 0)\n    contrasts(d$condition) <- cbind(highlow_vs_neutral, high_vs_low)\n    \n    pvalues_contrasts[i] <- coefficients(summary(lm(scores~condition, d)))[3,4]\n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = c(n, n),\n        type = c(\"treatment\", \"contrasts\"),\n        power = c(\n          sum(pvalues_treatment < 0.05) / length(pvalues_treatment),\n          sum(pvalues_contrasts < 0.05) / length(pvalues_contrasts)\n        )\n      )\n    )\n}\n\nggplot(outcomes, aes(x = sample_size, y = power, color = type)) + geom_line() + theme_bw()"
  },
  {
    "objectID": "content/08-interactions/08-exercise.html",
    "href": "content/08-interactions/08-exercise.html",
    "title": "Exercise V",
    "section": "",
    "text": "In this set of exercises, you’ll test your understanding of interactions. You’ll start with simulating power for an interaction, before going into more detail of what that actually means. You play around with different effect sizes (and ways to approach interactions) to find out how that affects power. Please note: All experiments and previous research below are completely fictional–not like I have any idea what I’m talking about.\n\n\nYou plan to analyze data from a content analysis. You’re interested in what predicts the number of Twitter followers for celebrities (including C-list celebrities, whatever that means). You believe that blue check mark definitely means having more followers compared to not having a check mark. Your dependent measure is, well, just the number of followers. You also expect that this difference will be stronger if a celebrity is more active, say tweets at least once a week.\nNow you want to run a power analysis to test that idea. First draw (on a piece of paper or digitally) your interaction. You can use the function below to try out some “drawings”. Start with putting in some of the means to roughly get the pattern described above.\n\nlibrary(tidyverse)\n\ntwobytwo <-\n  function(\n    means = c(0, 0, 0, 0),\n    factors = c(\"Factor 1\", \"Factor 2\"),\n    levels1 = c(\"Level 1\", \"Level 2\"),\n    levels2 = c(\"Level 1\", \"Level 2\"),\n    outcome = \"Outcome\"\n  ){\n    d <- \n      data.frame(\n        f1 = rep(levels1, times = 2),\n        f2 = rep(levels2, each = 2),\n        outcome = means\n      )\n    \n    names(d) <- c(factors, outcome)\n    \n    p <- \n      ggplot(d, aes(x = .data[[factors[1]]], y = .data[[outcome]], shape = .data[[factors[2]]], group = .data[[factors[2]]])) +\n      geom_point(size = 3) +\n      geom_line(aes(linetype = .data[[factors[2]]]), size = 1) +\n      theme_bw() +\n      theme(\n        axis.ticks.y = element_blank(),\n        axis.text.y = element_blank()\n      )\n    \n    return(p)\n  }\n\nFor example, the below would be a complete reversal:\n\ntwobytwo(\n  means = c(0, 2, 2, 0),\n  factors = c(\"Verified\", \"Activity\"),\n  levels1 = c(\"No\", \"Yes\"),\n  levels2 = c(\"Lurker\", \"Active\"),\n  outcome = \"Followers\"\n)\n\n\n\n\nFor our case, we’re interested in an attenuation effect: There’s an effect of verification (well, it’s not a causal effect in that sense, but let’s not get into that for now) regardless of the activity type, but it’ll be stronger with higher activity. How much stronger? And what follower counts should you expect for the different groups? This is where common sense and thinking about effect sizes on the raw scale comes in. Let’s go back to the linear model:\n\\(Characters = \\beta_0 + \\beta_1Verification + \\beta_2Activity + \\beta_3Verification \\times Activity + \\epsilon\\)\nGo back to the slides if you need a refresher what each beta represents. Let’s talk about our expectations. Let’s say we expect those without verification and low activity to have something like 20,000 followers. We expect that verification makes a big difference, such that verified celebrities, even when they’re not active, have, on average, 50,000 followers. Now comes the tricky bit: How many more followers will an unverified, but active person have? Let’s say it’s 25,000. As for the interaction: How much does activity add to the already massive difference between unverified and verified lurkers? Let’s say it adds 1,000 followers.\n\n\n\n\n\nRun a power analysis with these values where you figure out how many participants you need per group. “Recruiting” people won’t be difficult, so all you need is a rough estimate. Start at 500 celebrities and then go up in steps of 20 until 1,500. Use the linear model from above to create the scores. As for error: You’re really not sure about your estimates, so you add a lot of error. Use a normally distributed error term of 5,000 with a standard deviation of 5,000. For now, you’re not interested in the actual contrasts, only in whether an interaction is present. So run an lm model with and one without the interaction term and compare them with anova. Do 1,000 runs per combo. Also, you really don’t want to commit a Type I error, so you set your alpha to 0.001.\n\nset.seed(42)\n\nb0 <- 2e4\nb1 <- 3e4\nb2 <- 5e3\nb3 <- 1e3\nsizes <- seq(500, 1500, 20)\nruns <- 1e3\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nfor (n in sizes) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    \n    d <- \n      data.frame(\n        Verification = rep(0:1, times = n*2),\n        Activity = rep(0:1, each = n*2)\n      )\n    \n    d$Followers <- b0 + b1*d$Verification + b2*d$Activity + b3*d$Verification*d$Activity + rnorm(n, 5e3, 5e3)\n    \n    mains <- lm(Followers ~ Verification + Activity, d)\n    interactions <- lm(Followers ~ Verification*Activity, d)\n    \n    pvalues[i] <- anova(mains, interactions)$`Pr(>F)`[2]\n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        power = sum(pvalues < .001) / length(pvalues)\n      )\n    )\n}\n\nplot(outcomes$sample_size, outcomes$power)\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nAs always, begin with declaring your variables:\n\nb0 <- ? # unverified lurkers\nb1 <- ? # verified active\nb2 <- ? # unverified, but active\nb3 <- ? # the extra for being both verified and active\nsizes <- seq(?, ?, ?) # the steps in which we go up\nruns <- ? # how many simulations per combo\n\n# somewhere to store\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nAlright, if we’re working with the linear model, we just need a data frame that has two variables (Activity and Verification); each one should either be 0 or 1. So you’ll need to play around with rep to get the right combination. Then, you just fill in the linear model to create a third variable that holds the number of followers. Everything after that is just doing two lm models and comparing them to obtain a p-value.\n\n# iterate over sample sizes\nfor (? in ?) {\n  \n  # store p-values\n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    \n    # create the data frame\n    d <- \n      data.frame(\n        Verification = ?,\n        Activity = ?\n      )\n    \n    # get our dependent variable (don't forget to add error in the end)\n    d$Followers <- ? + b1*? + b2*d$Activity + ?*d$Verification*? + rnorm(?, ?, ?)\n    \n    # construct the two models\n    mains <- lm(? ~ ? + ?, d)\n    interactions <- lm(? ~ ?, d)\n    \n    pvalues[?] <- anova(mains, interactions)$`Pr(>F)`[2]\n  }\n  \n  # and store\n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        power = sum(pvalues < .001) / length(pvalues)\n      )\n    )\n}\n\nYou can plot the power curve as always with plot(outcomes$sample_size, outcomes$power).\n\n\n\n\n\n\nWe conduct an experiment. Previous research has shown that reading a story in the first person leads to more enjoyment on a 7-point Likert-scale than in the third person. For your thesis, you want to understand that effect better. You believe that the effect will be stronger if there’s lots of action compared to a tame story. In fact, you think that story type can completely knock out the effect (see here): The first-person enjoyment effect will only occur with an action story, but not with a lame one.\nThe linear model for enjoyment, measured on a 7-point Likert-scale, is as follows:\n\\(Enjoyment = \\beta_0 + \\beta_1Perspective + \\beta_2Type + \\beta_3Perspective \\times Type\\)\nYou believe the \\(\\beta\\) are as follows:\n\\[\\begin{align}\n&\\beta_0 = 4.3\\\\\n&\\beta_1 = 0.01\\\\\n&\\beta_2 = -0.03\\\\\n&\\beta_3 = 0.4\n\\end{align}\\]\nFirst draw the interaction to make sure you understand what it’s supposed to look like (either on paper or digitally). \\(Perspective\\) is 0 = 3rd person and 1 = First person; \\(Type\\) is 0 = Lame and 1 = Lots of action. This time, you’d like to decompose the error, meaning rather than adding overall variance, you want to add variance per group. Therefore, you simulate the data with four group means, not with the linear model (you can work the means out with the beta weights and the linear model). You set the SD to 0.6 for all groups, except for the first-person action condition where you expect opinions will be a bit more divided, setting the SD here to 0.9.\nEven though you want to power for the interaction, you’re also interested in main effects you can interpret. Therefore, use type 3 sums of squares with sum-to-zero coding for the factors (afex::aov_car will do that automatically). Start at 30 participants per group and keep going in steps of 5 per group until you reach 95% power (so use a while statement). Because this is exploratory, and you really don’t want to miss an effect, you set your alpha to 0.15. Do 1,000 runs per combo.\nTip: Be careful with factor levels. R will assign levels based on the alphabet. So assigning c(\"3rd person\", \"1st person\") to a factor will make the 1st person the reference category. Verify that you have the correct means by inspecting the raw means per group (i.e., create a large data set and check the means per group are the ones you intended).\n\nset.seed(42)\n\nmeans <- c(4.3, 4.31, 4.27, 4.3+0.01-0.03+0.4)\nsds <- c(rep(0.6, 3), 0.9)\nalpha <- 0.15\ndraws <- 1e3\nn <- 30\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\npower <- 0\n\nwhile (power < .95) {\n  \n  power <- NULL\n  pvalues <- NULL\n  \n  for (i in 1:draws) {\n    \n    d <- \n      data.frame(\n        id = factor(1:c(4*n)),\n        Enjoyment = rnorm(n*4, means, sds),\n        Perspective = factor(rep(c(\"3rd Person\", \"First Person\"), times = n*2)),\n        Action = factor(rep(c(\"Lame\", \"Lots of action\"), times = n, each = 2))\n      )\n    \n    m <- suppressMessages(afex::aov_car(Enjoyment ~ Perspective*Action + Error(id), data = d))\n    \n    pvalues[i] <- m$anova_table$`Pr(>F)`[3]\n  }\n  \n  power <- sum(pvalues < alpha) / length(pvalues)\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        power = power \n      )\n    )\n  \n  n <- n + 5\n}\n\nplot(outcomes$sample_size, outcomes$power)\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nAs always, begin with declaring your variables:\n\nmeans <- ? # you can get those from the betas and the linear model\nsds <- ? # remember that one SD is different\nalpha <- ? # we're less stringent\ndraws <- ? # how many runs?\nn <- ? # starting sample size\n\n# somewhere to store our data\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nNext, we need to initiate a while loop. Within that while statement, we run 1,000 simulations for each sample size. For each, we create a data frame with an id variable, an Enjoyment variable for our outcome, as well as two variables that indicate group membership (Perspective and Action). The structure of that data frame will depend on how you create the scores for Enjoyment (if you’re completely lost, have a look at the tip for the next exercise). To be certain you’re doing things correctly, create your data set with a massive sample and verify it’s doing what it’s supposed to do (so inspect means per group, for example with aggregate(Enjoyment ~ Perspective + Action, data = d, FUN = mean)).\n\n# initialize power \n\n# start loop\nwhile (power < .95) {\n  \n  # create objects to store power and p-values\n  power <- NULL\n  pvalues <- NULL\n  \n  for (i in 1:draws) {\n    \n    # make the score on way or the other (you could also create 4 distinct groups and then merge them)\n    d <- \n      data.frame(\n        id = ?,\n        Enjoyment = ?,\n        Perspective = ?,\n        Action = ?\n      )\n    \n    # do the model\n    m <- afex::aov_car(? ~ ?*? + Error(?), data = ?)\n    \n    pvalues[?] <- m$anova_table$`Pr(>F)`[?]\n  }\n  \n  power <- ?\n  \n  outcomes <- \n    ?\n  \n  # don't forget to go up a step in the sample size\n}\n\nYou can plot the power curve as always with plot(outcomes$sample_size, outcomes$power).\n\n\n\n\n\n\nGPower will give you the same sample size for a power analysis for an interaction as for a t-test if the effect size of the interaction is the same as in the t-test. Let’s check that. The effect size is the same if the effect is completely reversed (see here once more). Let’s say our first experiment produced a difference between control and treatment of 15 points. The mean and SD of the control were 100 and 15. For treatment: 115 and 15. That’s a massive effect size of one standard deviation. Now would a complete reversal look like? As in: A second factor completely reverse the effect of the first factor. Like this, using the plotting function from above:\n\ntwobytwo(means = c(100, 115, 115, 100))\n\n\n\n\nFirst, calculate power for the original effect (Circles above in the graph) in GPower (two-tailed). Alpha is 0.05 and power should be 95%. The groups should have the same size. Now simulate the above interaction and calculate power for the interaction effect. Compare your estimate to the GPower estimate you just got. Go about it as you think is best. (Tip: Turn the solution into a function; you’ll need it again for the next exercises.)\n\nset.seed(42)\n\ninteraction_power <- function(\n    means = c(100, 115, 115, 100),\n    sd = 15,\n    sizes = 1:20,\n    draws = 1e3\n) {\n  outcomes <- \n    data.frame(\n      sample_size = NULL,\n      power = NULL\n    )\n  \n  for (n in sizes) {\n    \n    pvalues <- NULL\n    \n    for (i in 1:draws) {\n      \n      d <- \n        data.frame(\n          id = factor(1:c(4*n)),\n          scores = rnorm(n*4, means, sd),\n          factor1 = factor(rep(c(\"level1\", \"level2\"), times = n*2)),\n          factor2 = factor(rep(c(\"level1\", \"level2\"), times = n, each = 2))\n        )\n      \n      mains <- lm(scores ~ factor1 + factor2, d)\n      interactions <- lm(scores ~ factor1 + factor2 + factor1:factor2, d)\n      \n      pvalues[i] <- anova(mains, interactions)$`Pr(>F)`[2]\n    }\n    \n    outcomes <- \n      rbind(\n        outcomes,\n        data.frame(\n          sample_size = n,\n          power = sum(pvalues < 0.05) / length(pvalues)\n        )\n      )\n  }\n  \n  return(outcomes)\n}\n\nresults <- interaction_power(draws = 200)\n\nplot(results$sample_size, results$power)\nabline(h = 0.95)\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nEssentially, you can take the code from the previous exercise and put it into a function. The biggest challenge will be to make sure the groups have the “correct” means. In other words, the data frame should have the right structure. Let’s go to our means. Say we put our means into a vector: c(100, 115, 115, 100). Now when we call rnorm(n, our_means), it will go one case for 100, one for 115, one for 115, one for 100, and then repeat (remember vectorization). That means our factors in the data frame should have the following structure:\n\n\n  id factor1 factor2 means\n1  1  level1  level1   100\n2  2  level2  level1   115\n3  3  level1  level2   115\n4  4  level2  level2   100\n\n\n\n\n\n\n\n\nWhat if an effect is attenuated? Say the original effect was between two groups with means of 4.4 and 4.9 and SDs of 0.6 and 0.7, respectively. Calculate Cohen’s d, then put that into GPower for an independent t-test (two-tailed) with an alpha of 0.05. Note down the sample size needed for 95% power. Next, we introduce a second factor that attenuates the original effect, such that the effect is half as large when we consider the second condition. The graph below (hopefully) shows what I mean:\n\n\n\n\n\nThat means for the two groups under the new condition, we need to half the effect size. First, half the effect size that you put into GPower and calculate the sample size again. Note down the number you get. Then get to simulation: For the new (half-sized) two groups, half the difference between them. The SDs can be the same as for the first two groups. Calculate power for the interaction effect and compare the sample size needed for 95% to the two estimates you got from GPower. To save yourself time, use the function you wrote for the previous exercise. Also, use the GPower estimates as a ballpark figure to set your sample sizes in the simulation. What does adding an interaction add to your sample size? Compare to the case here. Tip: You’ll need to calculate the standardized effect size first for GPower; then set the means for the new two groups in relation to that standardized effect size.\nSo first you calculate the pooled SD for the original group; with that, you can get Cohen’s \\(d\\) for the original effect. Next, you can use the original means plus half a Cohen’s \\(d\\) (so Cohen’s \\(d\\) times half a pooled SD).\n\nset.seed(42)\n\nm1 <- 4.4\nm2 <- 4.9\nsd1 <- 0.6\nsd2 <- 0.7\npooled_sd <- sqrt((sd1**2 + sd2**2)/2)\nd <- (m2-m1)/pooled_sd\n\nmeans <- c(m1, m2, m1, m1 + d/2*pooled_sd)\n\nresults <- interaction_power(\n  means = means,\n  sd = rep(c(sd1, sd2), 2),\n  sizes = seq(100, 800, 50),\n  draws = 1e3\n)\n\nplot(results$sample_size, results$power)\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nLet’s first declare our variables:\n\nm1 <- 4.4\nm2 <- 4.9\nsd1 <- 0.6\nsd2 <- 0.7\npooled_sd <- ?\nd <- (m2-m1)/pooled_sd\n\nNow we know the means for the first two groups: c(4.4, 4.9). For the next two groups under the new condition, the first mean can stay the same: 4.4. All we need is a mean that’s half the size of the previous difference in pooled SD units: c(m1, m2, m1, m1 + d/2*pooled_sd).\nNow you can easily feed these means to the function you wrote above.\n\n\n\n\n\n\nSo far, we’ve only considered the overall interaction effect. But a significant interaction can mean many different patterns, which is why we always follow up with simple effects. Take the attenuation from the previous exercise: There are 6 comparisons that could be of interest to us, one for each possible contrast. Say for our hypothesis of attenuation to hold, we’re interested in two simple effects: The comparison of the two levels under the old condition and the comparison of the two levels under the new condition.\nTherefore, repeat the above simulation, but this time also extract the p-values of these two posthoc simple effects. You can do that with pairs(emmeans::emmeans(interactions, \"factor1\", by = \"factor2\")). Save the power of the overall interaction term and the two simple effects and plot them. You can use the following code, assuming your data frame is called results, has a variable with the sample size sample_size, the type of power type, and power. You’ll need to adjust your function above or start from scratch. What’s the power of the simple effects compared to the overall interaction effect?\n\nlibrary(ggplot2)\n\nggplot(results, aes(x = sample_size, y = power, color = type, group = type)) + geom_point() + theme_bw()\n\n\nset.seed(42)\n\ninteraction_power <- function(\n    means = c(4.4, 4.9, 4.4, 4.65),\n    sd = rep(c(sd1, sd2), 2),\n    sizes = seq(50, 600, 50),\n    draws = 1e3\n) {\n  outcomes <- \n    data.frame(\n      sample_size = NULL,\n      type = NULL,\n      power = NULL\n    )\n  \n  for (n in sizes) {\n    \n    pvalues <- NULL\n    pvalues_simple1 <- NULL\n    pvalues_simple2 <- NULL\n    \n    for (i in 1:draws) {\n      \n      d <- \n        data.frame(\n          id = factor(1:c(4*n)),\n          scores = rnorm(n*4, means, sd),\n          factor1 = factor(rep(c(\"level1\", \"level2\"), times = n*2)),\n          factor2 = factor(rep(c(\"level1\", \"level2\"), times = n, each = 2))\n        )\n      \n      mains <- lm(scores ~ factor1 + factor2, d)\n      interactions <- lm(scores ~ factor1 + factor2 + factor1:factor2, d)\n      \n      ps <- data.frame(pairs(emmeans::emmeans(interactions, \"factor1\", by = \"factor2\")))\n      \n      pvalues[i] <- anova(mains, interactions)$`Pr(>F)`[2]\n      pvalues_simple1[i] <- ps$p.value[1]\n      pvalues_simple2[i] <- ps$p.value[2]\n    }\n    \n    outcomes <- \n      rbind(\n        outcomes,\n        data.frame(\n          sample_size = rep(n, 3),\n          type = factor(c(\"interaction\", \"simple full\", \"simple half\")),\n          power = c(\n            sum(pvalues < 0.05) / length(pvalues),\n            sum(pvalues_simple1 < 0.05) / length(pvalues_simple1),\n            sum(pvalues_simple2 < 0.05) / length(pvalues_simple2)\n          )\n        )\n      )\n  }\n  \n  return(outcomes)\n}\n\nresults <- interaction_power(draws = 500)\n\nlibrary(ggplot2)\n\nggplot(results, aes(x = sample_size, y = power, color = type, group = type)) + geom_point() + theme_bw()\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nWe can take the function code from above and update it. Essentially, all we need it two more numbers per simulation: the p-value for the first contrast and the p-value for the second contrast. Out storage data frame looks as follow:\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    type = NULL,\n    power = NULL\n  )\n\nNext, we need to access the p-values from the posthoc comparisons. So it’s best we store them with data.frame(pairs(emmeans::emmeans(interactions, \"factor1\", by = \"factor2\"))). Once we have that stored in an object, we can access the two p-values with the dollar sign and indexing ($p.value[1]). All that’s left to do is to save them all into our outcomes data frame:\n\noutcomes <- \n  rbind(\n    outcomes,\n    data.frame(\n      sample_size = rep(n, 3),\n      type = factor(c(\"interaction\", \"simple full\", \"simple half\")),\n      power = c(\n        sum(pvalues < 0.05) / length(pvalues),\n        sum(? < 0.05) / length(?),\n        sum(? < 0.05) / length(?)\n      )\n    )\n  )\n\n\n\n\n\n\n\nYou want to know what leads people to change their opinion about an issue. Previous research shows that the amount of arguments leads to stronger persuasiveness of a message. That effect will be knocked out, you believe, if the message comes from a source with low credibility. Therefore, you expect something like this:\n\n\n\n\n\nYou go for a repeated measures design, where people read four stories, one in each condition, and report how persuasive they find the story to be. (For the sake of argument, let’s ignore order or carry-over effects). You measure the outcome on a 100-point scale. Without knowing any better, you assume the no arguments, low credibility condition will fall on the middle of the scale. That makes it easy, because only the several arguments, high credibility condition will differ from the other three. You want to build in a reasonable amount of uncertainty, so you choose an SD of 20–this way, most of your values will be within 10 and 90 on the rating scale (50-+2*SD). As for the effect: Your SESOI is 10 points–anything below that is too small to care about in your opinion.\nRun a simulation where you draw from a multivariate normal distribution. The correlation between measures should be fairly low, 0.3. Power for the interaction effect (remember sum-to-zero contrasts). Start at 40 people and go up in steps of 5 until you reach 150. You’ll need to do some data transformations to get the data in the right (= long) format.\n\nlibrary(MASS)\n\nset.seed(42)\nsesoi <- 10\nmeans <- c(none_low = 50, several_low = 50, none_high = 50, several_high = 50 + sesoi)\nsd <- 20\ncorrelation <- 0.3\nruns <- 500\nsizes <- seq(20, 150, 10)\n\ncovariance <- correlation * sd * sd\nsigma <- matrix(\n  c(\n    sd**2, covariance, covariance, covariance,\n    covariance, sd**2, covariance, covariance,\n    covariance, covariance, sd**2, covariance,\n    covariance, covariance, covariance, sd**2\n  ),\n  ncol = 4\n)\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nfor (n in sizes) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n      \n      d <- mvrnorm(\n        n,\n        means,\n        sigma\n      )\n      \n      d <- as.data.frame(d)\n      \n      d$id <- factor(1:n)\n      \n      d <- pivot_longer(\n        d,\n        cols = -id,\n        values_to = \"scores\",\n        names_to = \"condition\"\n      ) %>% \n        separate(condition, into = c(\"Arguments\", \"Credibility\"), sep = \"_\") %>% \n        mutate(\n          across(c(\"Arguments\", \"Credibility\"), as.factor),\n        )\n      \n      m <- summary(afex::aov_car(scores ~ Arguments*Credibility + Error(id/Arguments*Credibility), d))\n      \n      pvalues[i] <- as.numeric(unlist(m)[26])\n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        power = sum(pvalues < 0.05) / length(pvalues)\n      )\n    )\n}\n\n\nplot(outcomes$sample_size, outcomes$power)\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nI guess if this were a drinking game, you’d have to drink now: You have all the tools needed for this one. Let’s start with the stuff we know and declare our variables, define our variance-covariance matrix (which is easy because the SD is stable), and a place to store our results.\n\nlibrary(MASS)\n\nsesoi <- ?\nmeans <- c(none_low = ?, several_low = ?, none_high = ?, several_high = ? + sesoi)\nsd <- ?\ncorrelation <- ?\nruns <- ?\nsizes <- seq(?, ?, ?)\n\n# get the covariance\ncovariance <- correlation * sd * sd\n\n# put it all into our matrix\nsigma <- matrix(\n  c(\n    sd**2, ?, ?, ?,\n    ?, sd**2, ?, ?,\n    ?, ?, ?, ?,\n    ?, ?, ?, ?\n  ),\n  ncol = ?\n)\n\n# same as always\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nThe next step is getting the data into the right format. Ultimately, we want them to be in tidy (or long format), with one row per observation. With the below command, we can get the data into long format, but they won’t be ideal. Look at the condition variable.\n\npivot_longer(\n  d,\n  cols = -id,\n  values_to = \"scores\",\n  names_to = \"condition\"\n)\n\n\n\n# A tibble: 6 × 3\n  id    condition    scores\n  <fct> <chr>         <dbl>\n1 1     none_low       80.8\n2 1     several_low    29.4\n3 1     none_high      46.9\n4 1     several_high   53.5\n5 2     none_low       59.1\n6 2     several_low    48.5\n\n\nThey’ll need to somehow look like this:\n\n\n# A tibble: 6 × 4\n  id    Arguments Credibility scores\n  <fct> <fct>     <fct>        <dbl>\n1 1     none      low           80.8\n2 1     several   low           29.4\n3 1     none      high          46.9\n4 1     several   high          53.5\n5 2     none      low           59.1\n6 2     several   low           48.5\n\n\nYou can create those two variables with an ifelse statement (or have a look at tidyr::separate). Once you have the data there, you just need to run the ANOVA and extract the p-value like we always do.\n\n\n\n\n\n\nYou have a natural experiment coming up because you know that some users in the UK get a new streaming service before users in France. You want to know whether access to another streaming service increases satisfaction with people’s media diet. From the waiting list for that streaming service, you want to sample comparable users in the UK and France, ask them how satisfied they are with their media variety, wait for a month to give the UK users time to try out the new streaming service whereas the French users will be the control group. In other words, you have a mixed design with a pre-post measure within, but condition aka country (access to streaming) between.\nYou need to run a power analysis. For your measure of satisfaction, you use a 7-point Likert-scale. You assume that France at the pre-measure will score above the midpoint, given how many providers are already out there, say 3.9. You also expect no change, maybe even a slight decline seeing how the UK is getting access, so you set the post-measure to 3.8. For the UK, you expect a somewhat higher satisfaction already at pre-measure because they usually get stuff directly after the US, say 4.3. Importantly, you believe access to this new streaming service will increase satisfaction by 0.4 points–that’s the mimimum amount of change on that measure that predicts more users in the future. As for the SDs: You don’t see any reason why variation would increase from pre- to post-measure. However, you do believe there’s a bit more variation in France than in the UK based on previous research with large samples. You set the SD for France to 1.2, but for the UK to 0.9. As for correlations between pre- and post-measures: You expect France to be more consistent (0.6) than the UK (0.4).\nSimulate power. You’ll need to do a variance-covariance matrix for correlated measures for each country (the within part) and then combine the two data frames (the between part). Base your power analysis on the interaction effect, not the simple effects. To run the ANOVA, you’ll need to properly nest the error, meaning Error(id/Time) in the aov_car command. You set your alpha to 0.01 and want to stop at 90% power. Start at 100 people per country and go up in steps of 20.\n\nlibrary(MASS)\n\nset.seed(42)\n\nmeans_france <- c(pre = 3.8, post = 3.9)\nsd_france <- 1.2\ncor_france <- 0.6\ncov_france <- cor_france * sd_france * sd_france\nmeans_uk <- c(pre = 4.3, post = 4.7)\nsd_uk <- 0.9\ncor_uk <- 0.4\ncov_uk <- cor_uk * sd_uk * sd_uk\nalpha <- 0.01\nn <- 100\nruns <- 500\n\nsigma_france <- \n  matrix(\n    c(\n      sd_france**2, cov_france,\n      cov_france, sd_france**2\n    ),\n    ncol = 2\n  )\n\nsigma_uk <- \n  matrix(\n    c(\n      sd_uk**2, cov_uk,\n      cov_uk, sd_uk**2\n    ),\n    ncol = 2\n  )\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\npower <- 0\n\nwhile (power < 0.90) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    \n    france <- mvrnorm(\n      n,\n      means_france,\n      sigma_france\n    )\n    \n    france <- as.data.frame(france)\n    france$country <- factor(\"france\")\n    france$id <- 1:n\n    \n    uk <- mvrnorm(\n      n,\n      means_uk,\n      sigma_uk\n    )\n    \n    uk <- as.data.frame(uk)\n    uk$country <- factor(\"uk\")\n    uk$id <- (n+1):(2*n)\n    \n    d <- rbind(france, uk)\n    \n    d <- d %>% \n      pivot_longer(\n        cols = c(pre, post),\n        names_to = \"time\",\n        values_to = \"satisfaction\"\n        \n      )\n    \n    m <- suppressMessages(summary(afex::aov_car(satisfaction ~ country*time + Error(id/time), d), type = 3))\n      \n    pvalues[i] <- as.numeric(unlist(m)[26])\n  }\n  \n  power <- sum(pvalues < alpha) / length(pvalues)\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        power = power\n      )\n    )\n  \n  n <- n + 20\n}\n\nplot(outcomes$sample_size, outcomes$power)\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nI’m going to keep tips to a minimum here. Once you have everything declared, the rest should fall into place:\n\nlibrary(MASS)\n\nmeans_france <- c(pre = ?, post = ?)\nsd_france <- 1?\ncor_france <- ? # correlation between measures for france\ncov_france <- ? # covariance for france\n\nmeans_uk <- c(pre = ?, post = ?)\nsd_uk <- ?\ncor_uk <- ?\ncov_uk <- ?\n\nalpha <- ?\nn <- ?\nruns <- ?\n\nsigma_france <- \n  matrix(\n    ?\n  )\n\nsigma_uk <- \n  matrix(\n    ?\n  )"
  },
  {
    "objectID": "content/09-continuous-predictors/09-exercise.html",
    "href": "content/09-continuous-predictors/09-exercise.html",
    "title": "Exercise VI",
    "section": "",
    "text": "You made it: last set of exercises. Nothing new here; just an extension of the skills you already have at this point. You’ll simulate power for continuous predictors and interactions.\n\n\nYou’re interested in the effects of three predictors on an outcome. When all predictors are 0, the outcome (y) should be around 16. The first predictor, x1, causes a 1-point increase in y; the second predictor, x2 causes a 0.3-point increase in y; the third predictor, x3, causes a 1.4 increase in y. All predictors range from 0 to 7; for our purposes, we can assume that they’re uniformly distributed. The error term has a mean of 0 and an SD of 10.\nSimulate power with 500 runs. You could either power for the entire model (so for the p-value of the F-test for the full lm model) or for each individual predictor. Find out which has the most power: Store power for the full model and each individual predictor from the model. Start with 50 participants and go up in steps of 10 until you reach 200. Plot the power curves for the different power types. You can use the code below:\n\nlibrary(ggplot2)\n\nggplot(outcomes, aes(x = sample_size, y = power, color = type)) + geom_line() + theme_bw()\n\n\nset.seed(42)\n\nb0 <- 16\nb1 <- 1\nb2 <- 0.3\nb3 <- 1.4\nsizes <- seq(50, 200, 10)\nruns <- 500\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    type = NULL,\n    power = NULL\n  )\n\nfor (n in sizes) {\n  \n  pvalues_all <- NULL\n  pvalues_x1 <- NULL\n  pvalues_x2 <- NULL\n  pvalues_x3 <- NULL\n  \n  for (i in 1:runs) {\n    \n    error <- rnorm(n, 0, 10)\n    \n    x1 <- runif(n, 0, 7)\n    x2 <- runif(n, 0, 7)\n    x3 <- runif(n, 0, 7)\n    \n    d <- \n      data.frame(\n        x1 = x1,\n        x2 = x2,\n        x3 = x3,\n        y = b0 + b1*x1 + b2*x2 + b3*x3 + error\n      )\n    \n    m <- summary(lm(y ~ x1 + x2 + x3, d))\n    \n    pvalues_all[i] <- broom::glance(m)$p.value\n    pvalues_x1[i] <- broom::tidy(m)$p.value[2]\n    pvalues_x2[i] <- broom::tidy(m)$p.value[3]\n    pvalues_x3[i] <- broom::tidy(m)$p.value[4]\n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = rep(n, 4),\n        type = factor(c(\"all\", \"x1\", \"x2\", \"x3\")),\n        power = c(\n          sum(pvalues_all < 0.05) / length(pvalues_all),\n          sum(pvalues_x1 < 0.05) / length(pvalues_x1),\n          sum(pvalues_x2 < 0.05) / length(pvalues_x2),\n          sum(pvalues_x3 < 0.05) / length(pvalues_x3)\n        )\n      )\n    )\n}\n\nlibrary(ggplot2)\nggplot(outcomes, aes(x = sample_size, y = power, color = type)) + geom_line() + theme_bw()\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nThe only new part is that you’re relying on a linear model for creating scores and that you’re storing more than one p-value from the model. Let’s declare our variables first.\n\nb0 <- ? # outcome when all predictors are at 0\nb1 <- ? # independent effect of x1\nb2 <- ? # indenepdent effect of x2\nb3 <- ? # independent effect of x3\nsizes <- ? # the sample sizes we're iterating over\nruns <- ? # number of simulations\n\n# somewhere to store our results (type indicates which type of power estimate it is)\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    type = NULL,\n    power = NULL\n  )\n\nNow we just need to fill those parameters into the linear model, put the linear model into a loop that iterates over the sample sizes, and store the results from each run.\n\n# iterate over the sample sizes\nfor (? in ?) {\n  \n  # somewhere to store each type of p-value\n  pvalues_all <- NULL\n  pvalues_x1 <- NULL\n  pvalues_x2 <- NULL\n  pvalues_x3 <- NULL\n  \n  for (? in 1:?) {\n    \n    error <- ?\n    \n    x1 <- ? # uniform predictor\n    x2 <- ? # uniform predictor\n    x3 <- ? # uniform predictor\n    \n    # put predictors into a data frame and create the outcome score\n    d <- \n      data.frame(\n        x1 = ?,\n        x2 = ?,\n        x3 = ?,\n        y = ? + error\n      )\n    \n    m <- summary(lm(y ~ x1 + x2 + x3, d))\n    \n    pvalues_all[?] <- broom::glance(m)$p.value\n    pvalues_x1[?] <- ?\n    pvalues_x2[?] <- ?\n    pvalues_x3[?] <- ?\n  }\n  \n  # store results somewhere\n  outcomes <- \n    ?\n}\n\nYou’ll notice that manually creating one “container” per type of p-value isn’t great coding. It gets the job done, but in your own work, you should consider a programmatic solution.\n\n\n\n\n\n\nCreate a correlation between two variables of 0.2. Use a correlation matrix. How many participants do you need for 95% power with an alpha of 0.01? Go about this as you think is best. Verify your estimate with GPower.\n\nlibrary(MASS)\n\nsigma <- \n  matrix(\n    c(1, 0.2, 0.2, 1),\n    ncol = 2\n  )\nalpha <- 0.01\npower <- 0\nn <- 100\nruns <- 1e3\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nwhile (power < 0.95) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    \n    d <- \n      mvrnorm(\n        n,\n        c(0,0),\n        sigma\n      )\n    \n    d <- as.data.frame(d)\n    \n    colnames(d) <- c(\"x1\", \"x2\")\n    \n    pvalues[i] <- cor.test(d$x1, d$x2)$p.value\n  }\n  \n  power <- sum(pvalues < alpha) / length(pvalues)\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        power = power\n      )\n    )\n  \n  n <- n + 10\n}\n\nplot(outcomes$sample_size, outcomes$power)\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nYou’re creating correlated scores, so it’s probably most straight forward to draw from a multivariate distribution. For that, we need a correlation matrix (aka a variance-covariance matrix with 0s and \\(r\\)s). So how will we get that matrix? Well, all we need, really, is the correlation because we already know the variance is 1:\n\nsigma <- \n  matrix(\n    c(1, 0.2, 0.2, 1),\n    ncol = 2\n  )\n\nThen we can get a sample as always:\n\nd <- \n  mvrnorm(\n    n,\n    c(0,0), # our means are 0 because the \"variables\" are standardized\n    sigma\n  )\n\nNow you can put that into a for loop or while statement, and voila. For getting the p-value for the correlation, look at ?cor.test and inspect the object.\n\n\n\n\n\n\nIn the correlation matrix, you specifically say how variables are related, but also when they aren’t related (i.e., when you assign zero for a correlation). For the data generating process, this is important, because it specifies the causal structure. Going simply by \\(R^2\\) or significance will often be misleading. For example, what if you have a third variable that creates a spurious effect? You can simulate that as well. Say you measure how much ice cream a person consumes in a year and expect that eating a lot of sweet, sweet ice cream makes people crave savory foods, which you measure as the portions of fries consumed in a beer garden. However, there truly is no effect of ice cream on eating fries; it’s just that both are influenced by the number of sun hours: More sun hours will lead people to eat more ice cream but also to spend more time in beer gardens and, inevitably, eat fries there.\nSimulate that: Use a sample size of 10,000. First, create a sun hours variable (rnorm will be enough). Then, have sun hours cause ice cream eating with an effect size of your choosing (don’t forget some error) . Next, also have sun hours cause fries eating. Now run two regression models:\n\none where you predict fries with ice cream alone\none where you predict fries with ice cream and sun hours\n\nWhich model gives you the correct causal effect of ice cream eating? But which one has a lower p-value for the ice cream variables? Do you see how it makes little sense to just go for significance and how you must consider the causal process when simulating data? If this tickled your interest, have a look here and here.\n\nn <- 1e4\n\nsun <- rnorm(n)\nice_cream <- 0.4*sun + rnorm(n)\nfries <- 0.5*sun + rnorm(n)\n\nsummary(lm(fries ~ ice_cream))\n\n\nCall:\nlm(formula = fries ~ ice_cream)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.8340 -0.7412 -0.0039  0.7345  4.3659 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 0.009393   0.011036   0.851    0.395    \nice_cream   0.175206   0.010235  17.118   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.104 on 9998 degrees of freedom\nMultiple R-squared:  0.02847,   Adjusted R-squared:  0.02838 \nF-statistic:   293 on 1 and 9998 DF,  p-value: < 2.2e-16\n\nsummary(lm(fries ~ ice_cream + sun))\n\n\nCall:\nlm(formula = fries ~ ice_cream + sun)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.6994 -0.6739 -0.0083  0.6723  3.6741 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  0.004700   0.010029   0.469    0.639    \nice_cream   -0.003296   0.010080  -0.327    0.744    \nsun          0.497472   0.010829  45.937   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.003 on 9997 degrees of freedom\nMultiple R-squared:  0.1978,    Adjusted R-squared:  0.1976 \nF-statistic:  1233 on 2 and 9997 DF,  p-value: < 2.2e-16\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nNo need to overthink this. Just follow the instructions closely. This is more of a conceptual exercise:\n\nn <- ? # sample size\n\nsun <- rnorm(n) # yup, that's all it takes here; we don't care about the actual values\nice_cream <- ?*sun + rnorm(n) # just put in a number; that's it\nfries <- ? # same here\n\nsummary(lm(fries ~ ice_cream))\nsummary(lm(fries ~ ice_cream + sun))\n\n\n\n\n\n\n\nYour colleague comes to you and tells you that they predicted satisfaction with a film with people’s enjoyment: The higher the enjoyment, the more they reported to be satisfied with the film. However, they only had 20 people in their sample. Run a sensitivity analysis and check for what \\(r\\) 20 people give you 90% power, even if you allow a more liberal alpha of 0.10. Increase \\(r\\) in steps of 0.01 and go the full distance, checking all effect sizes from 0 to 1. Do 500 runs per step. Verify with GPower.\n\nn <- 20\nalpha <- 0.10\neffects <- seq(0, 1, 0.01)\nruns <- 1e3\n\noutcomes <- \n  data.frame(\n    effect_size = NULL,\n    power = NULL\n  )\n\npvalues <- NULL\n\nfor (anr in effects) {\n  \n  for (i in 1:runs) {\n    sigma <- matrix(\n        c(1, anr, anr, 1),\n        ncol = 2\n      )\n      \n      d <- mvrnorm(\n        n,\n        c(0,0),\n        sigma\n      )\n      \n      d <- as.data.frame(d)\n      colnames(d) <- c(\"enjoyment\", \"satisfaction\")\n      \n      pvalues[i] <- cor.test(d$enjoyment, d$satisfaction)$p.value\n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        effect_size = anr,\n        power = sum(pvalues < alpha) / length(pvalues)\n      )\n    )\n}\n\nplot(outcomes$effect_size, outcomes$power)\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nThink back about sensitivity analysis. Here, we don’t iterate over sample sizes; we iterate over effect sizes. Because the effect size is on the standardized scale, we’ll use a correlation matrix to generate the data. The \\(r\\) will go into that correlation matrix, so we’ll need to create a correlation matrix for each iteration of effect size, then get a sample, run a test, and calculate power.\nLet’s start with declaring our variables:\n\nn <- ? # sample size\nalpha <- ? # the alpha\neffects <- seq(?, ?, 0.01) # create all rs between 0 and 1\nruns <- ? # how many runs\n\n# somewhere to store the outcome\noutcomes <- \n  data.frame(\n    effect_size = NULL,\n    power = NULL\n  )\n\nAlright, now let’s iterate over effect sizes:\n\nfor (? in ?) { # for an r in our vector of effect sizes\n  \n  for (i in 1:runs) {\n    \n    sigma <- matrix(\n        c(1, ?, ?, 1), # here, we need the correlation (aka effect size) per run\n        ncol = 2\n      )\n      \n    # create a data frame\n      d <- mvrnorm(\n        ?,\n        ?,\n        sigma\n      )\n      \n    # run cor.test and store p-value\n  }\n  \n  outcomes <- \n    ?\n}\n\n\n\n\n\n\n\nYou design a follow-up study where you measure enjoyment and satisfaction with films, but this time you want to determine a SESOI and do a power analysis before-hand. You measure both variables on a 5-point index Likert-scale (by index I mean it’s the average of several items). You know enjoyment usually scores above the midpoint; say a mean of 3.9 sounds realistic. The SD will be narrow: 0.5. For satisfaction, you expect a score below the midpoint of the scale, at 2.1, but with a larger SD of 1. Your smallest effect size of interest is 0.7 points because a previous analysis shows that this is the point where satisfaction translates to higher well-being for the day.\nYou want to be strict, so you set your alpha to 0.005, but at the same time, you don’t mind missing a true effect just as much, which is why you set your power goal to 85%. Run the power analysis (1000 runs per combo). Start at 50 people and go up in steps of 1 until you reach your desired power level (translation: use a while statement). Tip: Use the raw effect and SDs to get \\(r\\), which you then use in a variance-covariance matrix to simulate a data set for a given sample size. Verify with GPower.\n\ngoal <- 0.85\nalpha <- 0.005\nsesoi <- 0.7\nn <- 50\nmeans <- c(enjoyment = 3.9, satisfaction = 2.1)\nsd_enjoy <- 0.5\nsd_sat <- 1\nr <- sesoi * sd_enjoy/sd_sat\ncovariance <- r * sd_enjoy * sd_sat\nruns <- 1e3\n\nsigma <- \n  matrix(\n    c(\n      sd_enjoy**2, covariance,\n      covariance, sd_sat**2\n    ),\n    ncol = 2\n  )\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\npower <- 0\n\nwhile (power < goal) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    \n    d <- as.data.frame(\n      mvrnorm(\n        n,\n        means,\n        sigma\n      )\n    )\n    \n    pvalues[i] <- cor.test(d$enjoyment, d$satisfaction)$p.value\n  }\n  \n  power <- sum(pvalues < alpha) / length(pvalues)\n  \n  outcomes <- rbind(\n    outcomes,\n    data.frame(\n      sample_size = n,\n      power = power\n    )\n  )\n  \n  n <- n + 1\n}\n\nplot(outcomes$sample_size, outcomes$power)\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nAs always, declaring our variables is a good starting point:\n\ngoal <- 0.85 # our goal for power\nalpha <- ? # our stringent alpha\nsesoi <- ? # what's the smallest effect size we care about?\nn <- ? # what's our starting sample size?\nmeans <- c(enjoyment = ?, satisfaction = ?)\nsd_enjoy <- ? # SD of enjoyment\nsd_sat <- ? # SD of satisfaction\nr <- ? # what's our sesoi on the standardized scale? use the SDs of the two variables\ncovariance <- ? # calculate the covariance with the correlation (from the previous line) and multiply by the two SDs\nruns <- 1e3\n\nOnce we have those variables, it’s a standard while statement that we’ve also done in the previous exercises.\n\npower <- 0 # starting value for power\n\nwhile (power < ?) { # keep going as long as power is below our goal (wink, wink)\n  \n  # somewhere to store our p-values\n  \n  # then we do our 1,000 simulations\n  for (i in 1:?) {\n    \n    # create data frame with mvrnorm\n    d <- ?\n    \n    # get your p-value from the correlation\n    ? <- ?\n  }\n  \n  # calculate power so that the while statement can check whether we reached our goal\n  power <- ?\n  \n  # store it all somewhere\n  \n  # update something so that the power increases with each iteration\n}\n\n\n\n\n\n\n\nA colleague has found that films with higher ratings on IMDB bring in more money at the box office. However, you think this effect is mostly due to genre: for comedies, there is an effect of quality on success, but for action films this doesn’t matter. In other words, you predict an interaction, such that the positive effect of quality (i.e., IMDB rating) is only present in one condition (genre: comedies), but not in the other condition.\nYou want to test that hypothesis and start your power simulation. Specifically, you want to power for the interaction effect. IMDB ratings range from 0-10. Success is measured in million dollar steps. For genre, action films will be your baseline. You’ll also center the rating so that 0 represents an average rating. Overall, when a film is an action flick and has an average rating, you expect it to bring in 20 million. You don’t expect a main effect of quality because the quality effect will depend on genre. You do expect, however, a main effect of genre, such that, at average quality, comedies bring in 5 million more than action flicks. Crucially, you expect an interaction effect: For comedies, each 1-point increase in quality will generate 2 million extra at the box office.\nAs for error: You expect a normally distributed error with a mean of 0 and an SD of 15. Simulate the data and see how many films you need to have enough power to detect the interaction effect with 95% power with an alpha of 0.05. Start at 10 and go up in steps of 10 to a maximum of 250. Do 1,000 runs per combo.\nLike before, when you work with the linear model, it will help write out the model. So maybe start with the formula below:\n\\[\\begin{align}\nmillions = \\beta_0 + \\beta_1 Quality + \\beta_2 Genre + \\beta_3 Quality \\times Genre + \\epsilon\n\\end{align}\\]\nFirst start by filling in the betas and using them to simulate a massive data set (say 10,000 cases) before you start simulating. Plot the data set with:\n\nggplot(d, aes(x = quality, y = success, color = genre)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\") +\n  theme_bw()\n\nDoes the plot look like what we specified? Make sure you code the factor correctly: genre should have action films as the baseline; otherwise, the meaning of \\(\\beta_2\\) and \\(\\beta_3\\) changes.\n\nb0 <- 50\nb1 <- 0\nb2 <- 5\nb3 <- 2\nsizes <- seq(10, 300, 10)\nruns <- 1e3\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nfor (n in sizes) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    \n    error <- rnorm(n, 0, 15)\n    genre <-  rep(0:1, n/2)\n    quality <-  scale(runif(n, 0, 10), center = TRUE, scale = FALSE)\n    \n    d <- \n      data.frame(\n        genre = genre,\n        quality = quality,\n        success = b0 + b1*quality + b2*genre + b3*quality*genre + error\n      )\n    \n    # success can't be less than 0\n    d$success <- ifelse(d$success < 0, 0, d$success)\n    \n    m <- summary(lm(success ~ quality*genre, d))\n    \n    pvalues[i] <- broom::tidy(m)$p.value[4]\n    \n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        power = sum(pvalues < 0.05) / length(pvalues)\n      )\n    )\n}\n\nwith(outcomes, plot(sample_size, power))\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nWe need to fill in the betas for the linear model formula, so let’s define them:\n\nb0 <- ? # action flick at average quality\nb1 <- ? # main effect quality: going 1 up on quality for action films \nb2 <- ? # main effect genre: going from action to comedy at average quality\nb3 <- ? # the extra information when being a comedy and increasing in quality\nsizes <- seq(?, ?, ?)\nruns <- 1e3\n\nAfter that, we can simply run a loop. Just remember to create “fresh” error per run and not define it as a constant with the other parameters above.\n\nfor (? in ?) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:?) {\n    \n    error <- ?\n    genre <-  rep(0:1, ?)\n    quality <-  runif(?, ?, ?)\n    # don't forget to center\n    \n    d <- \n      data.frame(\n        genre = ?,\n        quality = ?,\n        success = b0 + b1*? + b2*? + b3*?*? + ?\n      )\n    \n    # success can't be less than 0\n    \n    # get p-value for interaction\n    \n  }\n}\n\n\n\n\n\n\n\nSimulate the above again, but this time increase the main effects of both quality and genre to 10 million. Leave the interaction effect at 2. What do you think – will this have an effect on power? If so, how and why?\n\nb0 <- 50\nb1 <- 10\nb2 <- 10\nb3 <- 2\nsizes <- seq(10, 250, 10)\nruns <- 1e3\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nfor (n in sizes) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    \n    error <- rnorm(n, 0, 15)\n    genre <-  rep(0:1, n/2)\n    quality <-  scale(runif(n, 0, 10), center = TRUE, scale = FALSE)\n    \n    d <- \n      data.frame(\n        genre = genre,\n        quality = quality,\n        success = b0 + b1*quality + b2*genre + b3*quality*genre + error\n      )\n    \n    # success can't be less than 0\n    d$success <- ifelse(d$success < 0, 0, d$success)\n    \n    m <- summary(lm(success ~ quality*genre, d))\n    \n    pvalues[i] <- broom::tidy(m)$p.value[4]\n    \n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        power = sum(pvalues < 0.05) / length(pvalues)\n      )\n    )\n}\n\nwith(outcomes, plot(sample_size, power))\n\n\n\n\n\n\n\nYou want to know how much listening to music leads to feelings of being relaxed. You expect a linear effect of the number of songs someone listens to on them feeling relaxed. After all, if listening to music relaxes you, then the more music, the more relaxed you get.\nHowever, you suspect that whether this effect occurs will strongly depend on how much people like these songs. Can’t relax if you listen to lots of crap (notice how I didn’t mention an artist here to be nice).\nYou measure feelings of relaxation on a Likert-type index variable with a range of 1-7. The number of songs is measured as a count with a maximum of 30 songs–you counted songs over a 1h period. After that period, you also asked how much people enjoyed the songs they were listening to on a scale from 0-100.\nSimulate number of songs (use sample) and liking (uniform). Transform the number of songs so that 1 up on the variable means listening to 5 more songs. As for liking, you want to go in steps of 10. Center both. These transformations make it easier to get an intuition about the effect sizes.\nAt average listening and liking, you think relaxation should be somewhat below the midpoint of the scale: 3.1. You don’t expect main effects of either predictor: Liking music shouldn’t relax you unless you listen to some of it, and listening to music alone shouldn’t do much unless you like the music.\nHowever, you expect a fairly sizable interaction effect, such that the combination of liking and listening will increase relaxation by 0.2 points. Remember the transformations: We say that listening to 5 songs (which is going up 1 on the listening variable) will increase relaxation 0.2 when liking also goes up by 10 points (which is going up 1 on the liking variable).\nFor error, you expect a normally distributed error with mean 0 and an SD of 2. Simulate power for the interaction effect, starting at 10 and ending at a sample size of 100. Go in steps of 5.Do 1,000 runs per combo. Also use interactions::interaction_plot to plot a random model to check how the data look like.\n\nb0 <- 3.1\nb1 <- 0\nb2 <- 0\nb3 <- 0.2\nsizes <- seq(10, 100, 5)\nruns <- 1e3\n\noutcomes <- \n  data.frame(\n    sample_size = NULL,\n    power = NULL\n  )\n\nfor (n in sizes) {\n  \n  pvalues <- NULL\n  \n  for (i in 1:runs) {\n    \n    error <- rnorm(n, 0, 2)\n    listening <- scale(sample(1:30, n, replace = TRUE) / 5, scale = FALSE)\n    liking <- scale(runif(n, 0, 100) / 10, scale = FALSE)\n    \n    d <- \n      data.frame(\n        listening = listening,\n        liking = liking,\n        relaxation = b0 + b1*listening + b2*liking + b3*listening*liking + error\n      )\n    \n    #trim the outcome\n    d$relaxation <- ifelse(d$relaxation < 0, 0, d$relaxation)\n    d$relaxation <- ifelse(d$relaxation > 7, 7, d$relaxation)\n    \n    m <- summary(lm(relaxation ~ listening*liking, d))\n    pvalues[i] <- broom::tidy(m)$p.value[4]\n  }\n  \n  outcomes <- \n    rbind(\n      outcomes,\n      data.frame(\n        sample_size = n,\n        power = sum(pvalues < 0.05) / length(pvalues)\n      )\n    )\n  \n}\n\nplot(outcomes$sample_size, outcomes$power)\n\n\n\ninteractions::interact_plot(lm(relaxation ~ listening*liking, d), pred = \"listening\", modx = \"liking\")\n\n\n\n\n\n\n\n\n\n\nExpand to get a tip\n\n\n\n\n\nThe actual work here is thinking through correctly defining and transforming the variables. So let’s declare first, then think about the transformations.\n\nb0 <- ? # relaxation when both liking and number of songs variables are at average\nb1 <- ? # main effect listening: going up 1 on number of songs variable while liking variable is at average\nb2 <- ? # main effect liking: going up 1 on liking variable whilst number of songs variable is at average\nb3 <- ? # the extra information when we listen to 5 songs (= going 1 up on number of songs variable) and like them 10 points more (= going up 1 on liking variable)\nsizes <- seq(?, ?, ?)\nruns <- 1e3\n\nOkay, that still doesn’t solve the transformation problem. How do we make the variables? Well, there’s 3 steps each for the listening and the liking variable:\n\nGet the raw variable (with sample or runif)\nTransform it such that 1 point on the variable now means something different\nCenter this transformed variable\n\n\n# gets us a uniform distribution of songs\nlistening <- sample(1:30, ?, replace = ?)\n\n# easy as that: divide by 5 to change a 1-unite increase on the variable\nlistening <- listening / 5\n\n# center\nlistening <- scale(?)\n\nThe process for liking is similar. Once you have that figured out, the rest of the steps are as known: For each simulation, create those variables and use them in the linear model to generate a relaxation score; obtain p-values; calculate power."
  },
  {
    "objectID": "content/04-effect-sizes/slides-part1/index.html#let-the-people-speak",
    "href": "content/04-effect-sizes/slides-part1/index.html#let-the-people-speak",
    "title": "Effect sizes",
    "section": "Let the people speak",
    "text": "Let the people speak"
  },
  {
    "objectID": "content/04-effect-sizes/slides-part2/index.html#logic",
    "href": "content/04-effect-sizes/slides-part2/index.html#logic",
    "title": "Interlude: Correlated measures",
    "section": "Logic",
    "text": "Logic\nAt this point, we’ve worked with for loops and went from a minimum to a maximum. If that maximum is large, that can take quite some time. You can also consider the while function to stop when you’ve reached the point you want to be at.\n Source"
  },
  {
    "objectID": "content/08-interactions/slides/index.html",
    "href": "content/08-interactions/slides/index.html",
    "title": "Interactions",
    "section": "",
    "text": "Takeaways\n\nUnderstand what an interaction is from the perspective of the linear model\nMake yourself think in more detail about the form of interactions\nBe able to translate that detail to generating data\n\n\n\nLet’s get simulating"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#simulating-this-process-is-easy",
    "href": "content/09-continuous-predictors/slides/index.html#simulating-this-process-is-easy",
    "title": "Continuous predictors",
    "section": "Simulating this process is easy",
    "text": "Simulating this process is easy\nWe get our age, put it into the formula, and we have our outcome.\n\nage <- rnorm(100, 50, 20)\ndisagreeableness <- 0 + 1 * age\n\nsummary(lm(disagreeableness ~ age))\n\n\nCall:\nlm(formula = disagreeableness ~ age)\n\nResiduals:\n       Min         1Q     Median         3Q        Max \n-1.287e-13  3.740e-16  9.290e-16  1.765e-15  1.018e-14 \n\nCoefficients:\n              Estimate Std. Error    t value Pr(>|t|)    \n(Intercept) -2.274e-14  3.426e-15 -6.637e+00 1.78e-09 ***\nage          1.000e+00  6.370e-17  1.570e+16  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.323e-14 on 98 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:      1 \nF-statistic: 2.465e+32 on 1 and 98 DF,  p-value: < 2.2e-16"
  },
  {
    "objectID": "content/09-continuous-predictors/slides/index.html#a-tangent",
    "href": "content/09-continuous-predictors/slides/index.html#a-tangent",
    "title": "Continuous predictors",
    "section": "A tangent",
    "text": "A tangent\n\n\nCausality is hard. Best to stay away from multiple predictors unless we’re prepared to defend our causal model.\n\nWhen both the predictor and control variable are unreliable measures of the same construct, the true predictive effect of the construct gets partitioned into two coefficients, neither of which capture the full causal effect. (Wysocki, Lawson, and Rhemtulla 2022)"
  },
  {
    "objectID": "index.html#location",
    "href": "index.html#location",
    "title": "Welcome",
    "section": "Location",
    "text": "Location\nSeminarraum 02-701, Georg-Forster-Gebäude"
  }
]